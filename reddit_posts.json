[{"Title": "PSA: This is not r/Programming. Quick Clarification on the guidelines", "Score": 566, "URL": "https://www.reddit.com/r/compsci/comments/c15nbn/psa_this_is_not_rprogramming_quick_clarification/", "CreatedAt": 1560655615.0, "Full Content": "As there's been recently quite the number of rule-breaking posts slipping by, I felt clarifying on a handful of key points would help out a bit (especially as most people use New.Reddit/Mobile, where the FAQ/sidebar isn't visible)\n\n&#x200B;\n\nFirst thing is first, this is ***not a programming specific subreddit***! If the post is a better fit for r/Programming or r/LearnProgramming, that's exactly where it's supposed to be posted in. Unless it involves some aspects of AI/CS, it's relatively better off somewhere else.\n\n&#x200B;\n\nr/ProgrammerHumor: Have a meme or joke relating to CS/Programming that you'd like to share with others? Head over to r/ProgrammerHumor, please.\n\n&#x200B;\n\nr/AskComputerScience: Have a ***genuine*** question in relation to CS that isn't directly asking for homework/assignment help nor someone to do it for you? Head over to r/AskComputerScience.\n\n&#x200B;\n\nr/CsMajors: Have a question in relation to CS academia (**such as \"Should I take CS70 or CS61A?\" \"Should I go to X or X uni, which has a better CS program?\")**, head over to r/csMajors.\n\n&#x200B;\n\nr/CsCareerQuestions: Have a question in regards to jobs/career in the CS job market? Head on over to to r/cscareerquestions. (or r/careerguidance if it's slightly too broad for it)\n\n&#x200B;\n\nr/SuggestALaptop: Just getting into the field or starting uni and don't know what laptop you should buy for programming? Head over to r/SuggestALaptop \n\n&#x200B;\n\nr/CompSci: Have a post that you'd like to share with the community and have a civil discussion that is in relation to the field of computer science (that doesn't break any of the rules), r/CompSci is the right place for you.  \n\n\n&#x200B;\n\nAnd *finally*, **this community will** ***not*** **do your assignments for you.** Asking questions directly relating to your homework or hell, copying and pasting the entire question into the post, will not be allowed.\n\nI'll be working on the redesign since it's been relatively untouched, and that's what most of the traffic these days see. That's about it, if you have any questions, feel free to ask them here!", "CntComments": 65, "Comments": [{"Comment": "Thank you. :-)", "Score": 85, "Replies": [{"Reply": "Moderating here has been relatively easy because of you guys constantly reporting all the content that doesn't belong here, so I'm thankful as well <3", "Reply Score": 49}]}, {"Comment": "So what does belong here", "Score": 46, "Replies": [{"Reply": "Well, here are a few of the things i've submitted here in recent times:\n\n* \"[Using Category Theory in Modeling Generics in OOP](https://old.reddit.com/r/compsci/comments/c008tk/using_category_theory_in_modeling_generics_in_oop/)\"\n\n* \"['Magic: The Gathering' is Turing Complete](https://old.reddit.com/r/compsci/comments/bggxj4/magic_the_gathering_is_turing_complete_abstract/)\"\n\n* \"[An introduction to regular expressions and parsing, in the context of a LaTeX find-and-replace problem](https://old.reddit.com/r/compsci/comments/bfmi9r/on_the_math_programming_blog_an_introduction_to/)\"", "Reply Score": 92}, {"Reply": "Anything that computer scientists are likely to find interesting and intellectually stimulating.\n\nTo elaborate:\n\n* Homework questions are typically not intellectually stimulating, and are often very repetitive.\n  * It would be fine on the other hand to post about a general computer science problem that is in and of itself noteworthy in some way.\n* Same for questions about what hardware, laptop, etc. to use.\n* Career questions again tend to be pretty repetitive and are not intellectually stimulating. We might allow some of these in exceptional cases if they're notable and well-written.\n* Programming is a topic within computer science, but not everything that would be on-topic in /r/programming is necessarily on topic here. It again comes down to being intellectually stimulating. Topics concerning *programming languages* are more frequently on topic.", "Reply Score": 5}]}, {"Comment": "r/AskProgramming and r/AskProgrammers also exist.\n\nwhile we won't do homework assignments there either, we'll often help with specific problems if you post what you tried and what issue you're running into.\n\nthe communities are pretty small but a few of us tend to be pretty active.", "Score": 7, "Replies": []}, {"Comment": "Thanks for the links to other subreddits. Great communities!", "Score": 6, "Replies": []}, {"Comment": "You should also link the right subreddit to ask about which laptop to pick.", "Score": 4, "Replies": [{"Reply": "Ah right, I usually send those posts over to r/SuggestALaptop in r/CompSci and r/ComputerScience. I'll be sure to list it in a few", "Reply Score": 3}]}, {"Comment": "Thanks so much for this", "Score": 11, "Replies": []}, {"Comment": "Thank you for your contributions to making the subreddit better! :-)", "Score": 2, "Replies": []}, {"Comment": "Just came across this sub, this is actually quite helpful for someone who's about to graduate soon. Thanks. (Im a weeb as well)", "Score": 2, "Replies": []}, {"Comment": "> * i thanks bud but still, fire and forget", "Score": 1, "Replies": []}, {"Comment": "CS61A and CS70 go bears", "Score": 1, "Replies": []}, {"Comment": "[deleted]", "Score": 1, "Replies": [{"Reply": "There\u2019s no specific CS subreddit for HS students, but r/csMajors does take those questions from my time browsing there, prospective students after all.", "Reply Score": 12}, {"Reply": "Many subs of this kind are more like: Here is a problem/hypothesis/theory/solution -> discuss! \n\nStack [exchanges](https://cs.stackexchange.com) are more oriented towards: Here is my problem/question -> solution.", "Reply Score": 6}]}, {"Comment": "How about discussions about research / thesis studies?", "Score": 1, "Replies": []}, {"Comment": "Will you please do my homework for me", "Score": 1, "Replies": [{"Reply": "What year are you? LOL", "Reply Score": 1}]}, {"Comment": "Awesome Content! Here is a quote for you:\n Given enough eyeballs, all bugs are shallow. (Linus Torwalds)", "Score": 1, "Replies": []}, {"Comment": "Awesome Content! Here is a quote for you:\n The most damaging phrase in the language is: We've always done it this way. (Grace Hopper)", "Score": 1, "Replies": []}, {"Comment": "Awesome Content! Here is a quote for you:\n Any program is only as good as it is useful. (Linus Torvalds)", "Score": 1, "Replies": []}, {"Comment": "If this subreddit is dedicated to discussions and content that computer scientists might find interesting/important, then why does content related to open questions in math/computer-science keep getting deleted for no reason?", "Score": 1, "Replies": []}, {"Comment": "Would a post about a new database idea/theory be ok here?", "Score": 1, "Replies": []}, {"Comment": "Moderating here has been relatively easy because of you guys constantly reporting all the content that doesn't belong here, so I'm thankful as well <3", "Score": 49, "Replies": [{"Reply": "You might want to take a more pro-active stance. If I were to really flag everything *I* consider offtopic, I would flag 3/4 of the submissions. \nThe current state is still that most content you see when you browse the subreddit are uninteresting carreer/teaching/programming questions.\n\nYou should really invest in automod or something like that. to clean it up preemptively.", "Reply Score": 22}]}, {"Comment": "Well, here are a few of the things i've submitted here in recent times:\n\n* \"[Using Category Theory in Modeling Generics in OOP](https://old.reddit.com/r/compsci/comments/c008tk/using_category_theory_in_modeling_generics_in_oop/)\"\n\n* \"['Magic: The Gathering' is Turing Complete](https://old.reddit.com/r/compsci/comments/bggxj4/magic_the_gathering_is_turing_complete_abstract/)\"\n\n* \"[An introduction to regular expressions and parsing, in the context of a LaTeX find-and-replace problem](https://old.reddit.com/r/compsci/comments/bfmi9r/on_the_math_programming_blog_an_introduction_to/)\"", "Score": 92, "Replies": [{"Reply": "These are excellent examples of the kind of content that we like to see in /r/compsci. Thank you for contributing!", "Reply Score": 8}, {"Reply": "So ... ?", "Reply Score": 3}, {"Reply": "Excellent posts", "Reply Score": 2}, {"Reply": "Good reply!", "Reply Score": 2}]}, {"Comment": "Anything that computer scientists are likely to find interesting and intellectually stimulating.\n\nTo elaborate:\n\n* Homework questions are typically not intellectually stimulating, and are often very repetitive.\n  * It would be fine on the other hand to post about a general computer science problem that is in and of itself noteworthy in some way.\n* Same for questions about what hardware, laptop, etc. to use.\n* Career questions again tend to be pretty repetitive and are not intellectually stimulating. We might allow some of these in exceptional cases if they're notable and well-written.\n* Programming is a topic within computer science, but not everything that would be on-topic in /r/programming is necessarily on topic here. It again comes down to being intellectually stimulating. Topics concerning *programming languages* are more frequently on topic.", "Score": 5, "Replies": [{"Reply": "I wish you guys didn't have a blanket autospam on all wordpress blogs. I often write things that match your scope and that used to be well-liked on this sub before the autospam.", "Reply Score": 3}]}, {"Comment": "Ah right, I usually send those posts over to r/SuggestALaptop in r/CompSci and r/ComputerScience. I'll be sure to list it in a few", "Score": 3, "Replies": []}, {"Comment": "There\u2019s no specific CS subreddit for HS students, but r/csMajors does take those questions from my time browsing there, prospective students after all.", "Score": 12, "Replies": [{"Reply": "Thank you!", "Reply Score": 2}]}, {"Comment": "Many subs of this kind are more like: Here is a problem/hypothesis/theory/solution -> discuss! \n\nStack [exchanges](https://cs.stackexchange.com) are more oriented towards: Here is my problem/question -> solution.", "Score": 6, "Replies": []}, {"Comment": "What year are you? LOL", "Score": 1, "Replies": []}, {"Comment": "You might want to take a more pro-active stance. If I were to really flag everything *I* consider offtopic, I would flag 3/4 of the submissions. \nThe current state is still that most content you see when you browse the subreddit are uninteresting carreer/teaching/programming questions.\n\nYou should really invest in automod or something like that. to clean it up preemptively.", "Score": 22, "Replies": [{"Reply": "> You should really invest in automod or something like that. to clean it up preemptively.\n\nNot a programmer, I gather? Automated censorship is impossible to get right and very irritating for those humans subjected to it.", "Reply Score": 24}, {"Reply": "Ironically enough, our AutoMod is actually *a bit* too haywire, taking down a high amount of posts that should be allowed, resulting in daily mod mails asking for approvals.\n\nIn reality, it's simply the lack of moderation that lets these slip by. I've recently started break, so I'll have plenty of time to consistently check the queue here, but it doesn't really break the fact that the only active moderators in the past couple of months have been myself alongside the top mod. \n\nKeeping that in mind, the best course of action would just be to add new faces around here on the mod team, I'll set up applications/add a few active mods sometime soon.", "Reply Score": 7}]}, {"Comment": "These are excellent examples of the kind of content that we like to see in /r/compsci. Thank you for contributing!", "Score": 8, "Replies": [{"Reply": "Would the discussion of algorithms in general fit this sub or r/programming better?", "Reply Score": 2}]}, {"Comment": "So ... ?", "Score": 3, "Replies": []}, {"Comment": "Excellent posts", "Score": 2, "Replies": []}, {"Comment": "Good reply!", "Score": 2, "Replies": []}, {"Comment": "I wish you guys didn't have a blanket autospam on all wordpress blogs. I often write things that match your scope and that used to be well-liked on this sub before the autospam.", "Score": 3, "Replies": []}, {"Comment": "Thank you!", "Score": 2, "Replies": []}, {"Comment": "> You should really invest in automod or something like that. to clean it up preemptively.\n\nNot a programmer, I gather? Automated censorship is impossible to get right and very irritating for those humans subjected to it.", "Score": 24, "Replies": []}, {"Comment": "Ironically enough, our AutoMod is actually *a bit* too haywire, taking down a high amount of posts that should be allowed, resulting in daily mod mails asking for approvals.\n\nIn reality, it's simply the lack of moderation that lets these slip by. I've recently started break, so I'll have plenty of time to consistently check the queue here, but it doesn't really break the fact that the only active moderators in the past couple of months have been myself alongside the top mod. \n\nKeeping that in mind, the best course of action would just be to add new faces around here on the mod team, I'll set up applications/add a few active mods sometime soon.", "Score": 7, "Replies": [{"Reply": "Yeah, I'm aware lack of manpower is definitely an issue. How about, on top of recruiting new faces, you purge the inactive mods ? :)", "Reply Score": 2}]}, {"Comment": "Would the discussion of algorithms in general fit this sub or r/programming better?", "Score": 2, "Replies": []}, {"Comment": "Yeah, I'm aware lack of manpower is definitely an issue. How about, on top of recruiting new faces, you purge the inactive mods ? :)", "Score": 2, "Replies": []}]},{"Title": "Invariant check", "Score": 2, "URL": "https://www.reddit.com/r/compsci/comments/1apbxes/invariant_check/", "CreatedAt": 1707774186.0, "Full Content": "Assume N has been defined as a finite even integer >= 2, so the loop must execute at least 1 times.\n\n int f = 1;\n\nfor(int j =2; j<= N; j=j+1)\n\n{\n\n     if(j%2==0)\n     f=f*j;\n\n}\n\nThe range of my LCV: (2 <= N >= j-1) ?\n\nThe only statement I could see that would be invariably true is: \n\nf contains the product of all the even integers from 2 to j-1.\n\nAm I on the right track or am I off ? \nAny input would would be greatly appreciated.", "CntComments": 4, "Comments": [{"Comment": "\\> f contains the product of all the even integers from 2 to j-1.\n\nThis sounds like it is describing the result of *every iteration* after completion of the loop. For an invariant you are describing the results of a *single iteration*. Some thing that:\n\n1. Holds before the iteration starts\n2. Something that is then maintained *in* the loop.\n3. Something that will hold again after the iteration is complete.\n\nAs many of my students do, it sounds like you are trying to describe *the algorithm* implemented by the loop, which is more than you want.", "Score": 3, "Replies": []}, {"Comment": "Your invariant should be an equality between f and j. And as j turns to N, this proves the algorithm correct.", "Score": 2, "Replies": [{"Reply": "So really I\u2019m trying to say that f <= j <=N because even when j = N, it runs through the loop a final time? Then prove that through 1 iteration and then for the termination. Sorry I\u2019ve been staring at this all day and I just want to understand it \ud83d\ude02", "Reply Score": 1}]}, {"Comment": "So really I\u2019m trying to say that f <= j <=N because even when j = N, it runs through the loop a final time? Then prove that through 1 iteration and then for the termination. Sorry I\u2019ve been staring at this all day and I just want to understand it \ud83d\ude02", "Score": 1, "Replies": [{"Reply": "Equality. Proper equals. No less-or. You have to come up with a formula between j and f that's always true at the end of each loop iteration and before the first iteration.\n\nThen by replacing j with N you magically get your post-condition.", "Reply Score": 1}]}, {"Comment": "Equality. Proper equals. No less-or. You have to come up with a formula between j and f that's always true at the end of each loop iteration and before the first iteration.\n\nThen by replacing j with N you magically get your post-condition.", "Score": 1, "Replies": []}]},{"Title": "Balanced Ternary Algebra", "Score": 1, "URL": "https://veniamin-ilmer.github.io/math/balanced-ternary-algebra/", "CreatedAt": 1707646607.0, "Full Content": "", "CntComments": 3, "Comments": [{"Comment": "There is no chance this would ever be used in practice.  The difference between the radix economy of 2 and 3 is so tiny that inventing a radically new type of transistor, rebuilding all the fabs, redesigning every processor architecture, rewriting all code that uses bitwise operations, etc. for about 5% increased efficiency would be insane.\n\nBy comparison, you would get the same increase in efficiency using a ternary tree compared to a binary tree but nobody does it, and that doesn't even involve making new hardware/assembly code.  Just the mental jump of going from a 2-ary to 3-ary tree is not worth it for the small improvement.", "Score": 2, "Replies": [{"Reply": "Although I agree with you, this is still a good academic exercise to discover how such a system would work.", "Reply Score": 2}]}, {"Comment": "Although I agree with you, this is still a good academic exercise to discover how such a system would work.", "Score": 2, "Replies": [{"Reply": "Seems it has already been done though.\n\nhttps://www.researchgate.net/profile/Mansaf-Alam/publication/270816756\\_Balanced-Ternary\\_Logic\\_for\\_Improved\\_and\\_Advanced\\_Computing/links/5502fc130cf2d60c0e64c3f8/Balanced-Ternary-Logic-for-Improved-and-Advanced-Computing.pdf", "Reply Score": 1}]}, {"Comment": "Seems it has already been done though.\n\nhttps://www.researchgate.net/profile/Mansaf-Alam/publication/270816756\\_Balanced-Ternary\\_Logic\\_for\\_Improved\\_and\\_Advanced\\_Computing/links/5502fc130cf2d60c0e64c3f8/Balanced-Ternary-Logic-for-Improved-and-Advanced-Computing.pdf", "Score": 1, "Replies": []}]},{"Title": "The most beautiful computer science paper I have ever read", "Score": 47, "URL": "https://www.reddit.com/r/compsci/comments/1anbb1b/the_most_beautiful_computer_science_paper_i_have/", "CreatedAt": 1707551984.0, "Full Content": "A mathematical algorithm, greek politics and a bunch of priests\u2026this  is what you need for the most beautiful paper I have ever read.\n\nThis week I am bringing you a paper about the Paxos distributed  consensus algorithm. It got its name from an island in the aegean sea,  or more accurate: From a way how the parliament of that island conducted  legislation. The paper is long (33 pages), but beautifully written and  explains the algorithm in a nice way. You got to check it out.\n\n**In order to receive ALL articles, subscribe to the email based newsletter:** [**https://www.weeklycspaper.com/**](https://www.weeklycspaper.com/)**. Here on Reddit I only share some of the articles.** \n\nHere is the first paragraph:  \n\n\n>**1.1 The Island of Paxos**  \nEarly in this millennium,  the Aegean island of Paxos was a thriving mercantile center. Wealth led  to political sophistication, and the Paxons replaced their ancient  theocracy with a parliamentary form of government. But trade came before  civic duty, and no one in Paxos was willingto devote his life to  Parliament. The Paxon Parliament had to function even though legislators  continually wandered in and out  \nof the parliamentary Chamber. The  problem of governing with a part-time parliament bears a remarkable  correspondence to the problem faced by today\u2019s fault-tolerant  distributed systems, where legislators correspond to processes and  leaving the Chamber corresponds to failing. The Paxons\u2019 solution may  therefore be of some interest to computer scientists. I present here a  short history of the Paxos Parliament\u2019s protocol, followed by an even  shorter discussion of its relevance for distributed systems. Paxon  civilization was destroyed by a foreign invasion, and archeologists have  just recently begun to unearth its history. Our knowledge of the Paxon  Parliament is therefore fragmentary. Although the basic protocols are  known, we are ignorant of many details. Where such details are of  interest, I will take the liberty of speculating on what the Paxons  might have done  \n \u201cThe Part-Time Parliament\u201d \u2013 Leslie Lamport\n\n**Abstract:**\n\nRecent archaeological discoveries on the island of Paxos reveal that  the parliament functioned despite the peripatetic propensity of its  part-time legislators. The legislators maintained consistent copies of  the parliamentary record, despite their frequent forays from the chamber  and the forgetfulness of their messengers. The Paxon parliament\u2019s  protocol provides a new way of implementing the state-machine approach  to the design of distributed systems\n\n**Download Link:** \n\n[http://lamport.azurewebsites.net/pubs/lamport-paxos.pdf](http://lamport.azurewebsites.net/pubs/lamport-paxos.pdf)\n\n**Additional Links:**\n\n* None this time", "CntComments": 11, "Comments": [{"Comment": "Bold attempt to spice up a paper by inventing a nice story around it. Something most of us can't afford. But I really like it!", "Score": 19, "Replies": []}, {"Comment": "[deleted]", "Score": 28, "Replies": []}, {"Comment": "So this algorithm provides a solution to CAP theorem related to distributed systems ?", "Score": 5, "Replies": [{"Reply": "No.  In a partition, Paxos guarantees consistency while maximizing availability.  Paxos and Raft are the two best known distributed consensus algorithms, and Leslie Lamport is probably the most prominent computer scientist focusing on distributed algorithms, arguably the founder of the field.", "Reply Score": 22}, {"Reply": "No.  Any time something claims to have 'solved' CAP, the answer is always no.  It's not that we're waiting for someone really clever to come along to solve it, it's literally not possible to solve it.\n\nGilbert & Lynch's 2002 paper on it is pretty short, but honestly I remember it taking me a few readings before I got it.  It's a little ambiguous and confusing in places, and could use some simple illustrations.  It's worth a look though.", "Reply Score": 5}, {"Reply": "I hear the running time of Paxos is theoretically unbounded but practically, most iterations will halt.", "Reply Score": 1}]}, {"Comment": "`Time, Clocks, and the Ordering of Events in a Distributed System` is still at the top, for me..", "Score": 6, "Replies": []}, {"Comment": "Yeah his vector clocks paper is fire too", "Score": 3, "Replies": []}, {"Comment": "Leslie Lamport is not just known as a researcher on distributed systems, he also created LaTeX and a very readable book that explained it. He is very good at explaining things and here, the use of an anecdote as an analogy is perfect.", "Score": 3, "Replies": []}, {"Comment": "The original Paxos paper is unreadable. Paxos made simple is 100x better", "Score": 1, "Replies": []}, {"Comment": "Sounds an interesting topic, maybe some day soon I'll have time to read the full paper.  In the meantime I'm hoping someone can summarise it.", "Score": 1, "Replies": []}, {"Comment": "No.  In a partition, Paxos guarantees consistency while maximizing availability.  Paxos and Raft are the two best known distributed consensus algorithms, and Leslie Lamport is probably the most prominent computer scientist focusing on distributed algorithms, arguably the founder of the field.", "Score": 22, "Replies": [{"Reply": "He worked for Digital which invented VMS Clusters back in 83, networks of well connected machines which may or may not be available at any time. To go with this they produced a distributed lock manager that was excellent and fundamental in other products like their RdB clustered database management system. Lamport's work on distributed systems was important for this.", "Reply Score": 2}]}, {"Comment": "No.  Any time something claims to have 'solved' CAP, the answer is always no.  It's not that we're waiting for someone really clever to come along to solve it, it's literally not possible to solve it.\n\nGilbert & Lynch's 2002 paper on it is pretty short, but honestly I remember it taking me a few readings before I got it.  It's a little ambiguous and confusing in places, and could use some simple illustrations.  It's worth a look though.", "Score": 5, "Replies": []}, {"Comment": "I hear the running time of Paxos is theoretically unbounded but practically, most iterations will halt.", "Score": 1, "Replies": []}, {"Comment": "He worked for Digital which invented VMS Clusters back in 83, networks of well connected machines which may or may not be available at any time. To go with this they produced a distributed lock manager that was excellent and fundamental in other products like their RdB clustered database management system. Lamport's work on distributed systems was important for this.", "Score": 2, "Replies": []}]},{"Title": "optical computing", "Score": 8, "URL": "https://www.reddit.com/r/compsci/comments/1an4agl/optical_computing/", "CreatedAt": 1707527981.0, "Full Content": "does this field actually exist? is this field the future? how does this work? how does it differ from quantum computing? how to get started as an undergrad?", "CntComments": 8, "Comments": [{"Comment": "1. Yes. \n2.  Probably not. \n3. [Relies upon the physical properties of light.](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C44&q=optical+computing&btnG=#d=gs_qabs&t=1707530688695&u=%23p%3D6tXjE5Y6GmMJ) \n4. Quantum computing relies upon quantum properties of physics. \n5. Apply to any REU or UR2PhD program that supports optical computing.", "Score": 16, "Replies": []}, {"Comment": "Photonic computing is very good at very specific things.  It is able to do complex computations with very low energy levels because light doesn't produce waste heat.\n\nUnfortunately you still have to create the light beams and that takes energy.  And they can't store data for long periods (measured in nanoseconds) very well, and they can't do branching very well.\n\nIt's probably not the future, but it is cool.", "Score": 8, "Replies": []}, {"Comment": "[This video](https://youtu.be/Mdh2pLwsK8Y?si=gTIfTU80Gfh162ya) on optical computing with plasma may suit your fancy", "Score": 2, "Replies": []}, {"Comment": "Personally I don't like where optical computing is right now. As an undergrad I dreamed of a computer which utilizes the \"octave bandwidth\" of optical technology. You know, those old radio guys often had like 1% bandwidth. Good for audio, bad for computing.\n\nSo like in USB you could try to avoid long rows of 111 and 000 and also balance 1 vs 0 . In CMOS you try to tweak logic to avoid changes from 0 to 1. Maybe the opposite is possible? I guess that we would need like two octaves of bandwidth. A lot of materials offer exceptionally low absorption around the visible range. Just optical fibers are quite thick. I imagine that -- as seen today -- most volume will be used up by fibers. The gates are tiny non-linear crystals squeezed between metal cones. Big crystals don't offer a whole octave, but they show how the input vectors add to form an output vector so that you can separate the result. With low bandwidth you could a spectrometer .. aren't those all interference effects? How do we get 3 directions? I guess we need an interferometer with 3 arms and 3 crystals. Polarization can also be used, but crystals are not good at using two parallel polarizations and then output an orthogonal one. Also I really need to find a way to grow tiny calcite crystals. Maybe one could combine them with Fresnel multilayer 45\u00b0 mirrors .. to reduce the number of layers and get them smaller? Quasi-phase matching is still my favorite. A wide band can be achieved by varying the vector based on frequency, but I don't see how to miniaturize this.\n\nI really don't want to go to old AM Radio modulation technology for this.", "Score": 1, "Replies": [{"Reply": "At the present, much of what you call optical computing effort is aimed at photon equivalents of the electron. Its fashionable as the benefits of low power requirements, potential speed gains and a large knowledge base of coding are very attractive. Metasurfaces represent a new methodology of manipulating the electromagnetic spectrum generally in a continuous, analogue type of process such as holograms. However, design of these surfaces heavily involves the use of Maxwell's equations so it gets bracketed into a very narrow field. There is also a possibility that new types of metasurface will be discovered, probably through an anomaly with design software or manufacturing error, that results in a paradigm shift away from standard physics. As in 1887 Heinrich Hertz, for some unknown reason, exposed a black light(UV) on his spark gap experiment with historical consequences. Personally, I would split my grad learning two thirds electronic one third electromagnetic. This way if a major advance takes place in photonic computing you will have a good base knowledge.", "Reply Score": 3}]}, {"Comment": "At the present, much of what you call optical computing effort is aimed at photon equivalents of the electron. Its fashionable as the benefits of low power requirements, potential speed gains and a large knowledge base of coding are very attractive. Metasurfaces represent a new methodology of manipulating the electromagnetic spectrum generally in a continuous, analogue type of process such as holograms. However, design of these surfaces heavily involves the use of Maxwell's equations so it gets bracketed into a very narrow field. There is also a possibility that new types of metasurface will be discovered, probably through an anomaly with design software or manufacturing error, that results in a paradigm shift away from standard physics. As in 1887 Heinrich Hertz, for some unknown reason, exposed a black light(UV) on his spark gap experiment with historical consequences. Personally, I would split my grad learning two thirds electronic one third electromagnetic. This way if a major advance takes place in photonic computing you will have a good base knowledge.", "Score": 3, "Replies": [{"Reply": "Metasurfaces .. huh? I just mean, so much money and time went into this research to find applications. But not a lot is found. Meanwhile logic gates find their applications almost on their own. So photonics sit down and just create universally working logic gates. Right now the industry is like this SQUID quantum annealing computer, where tons of people try to find applications, but fail. Just give me reliably working quantum gates and applications come by themselves. To this day I have not seen a systematic way to reduce dephasing. But there is also no proof against it. My last hope is maybe a computer who uses quantum gates to actively fight noise from the outside and also to calibrate internal Hamilton operators. I mean, the natural Hamilton Operators don't look like in computing. Lots of entries with values different from 0 or 1. At least quantum computers are trying. Significant advancement for optical computers came from modern slaves working crystal grow ovens with always changing mixture of elements. It is like high T super conductor research.", "Reply Score": 1}]}, {"Comment": "Metasurfaces .. huh? I just mean, so much money and time went into this research to find applications. But not a lot is found. Meanwhile logic gates find their applications almost on their own. So photonics sit down and just create universally working logic gates. Right now the industry is like this SQUID quantum annealing computer, where tons of people try to find applications, but fail. Just give me reliably working quantum gates and applications come by themselves. To this day I have not seen a systematic way to reduce dephasing. But there is also no proof against it. My last hope is maybe a computer who uses quantum gates to actively fight noise from the outside and also to calibrate internal Hamilton operators. I mean, the natural Hamilton Operators don't look like in computing. Lots of entries with values different from 0 or 1. At least quantum computers are trying. Significant advancement for optical computers came from modern slaves working crystal grow ovens with always changing mixture of elements. It is like high T super conductor research.", "Score": 1, "Replies": [{"Reply": "&#x200B;\n\nCurrent exa scale computers using logic gates have an unhealthy electricity appetite in excess of tens of megawatts. It will have to flat line at some point.", "Reply Score": 1}]}, {"Comment": "&#x200B;\n\nCurrent exa scale computers using logic gates have an unhealthy electricity appetite in excess of tens of megawatts. It will have to flat line at some point.", "Score": 1, "Replies": [{"Reply": "I mean the gates will always be pretty large. I guess three photon absorption should better not bridge the band gap. So frequencies would be red to NIR. So  while I still hope for the 100 THz bandwidth, I only expect simple circuits in stationary applications. Routers?", "Reply Score": 1}]}, {"Comment": "I mean the gates will always be pretty large. I guess three photon absorption should better not bridge the band gap. So frequencies would be red to NIR. So  while I still hope for the 100 THz bandwidth, I only expect simple circuits in stationary applications. Routers?", "Score": 1, "Replies": []}]},{"Title": "Inject memory safety check into executable binaries to get rid of MMUs?", "Score": 0, "URL": "https://www.reddit.com/r/compsci/comments/1aneidu/inject_memory_safety_check_into_executable/", "CreatedAt": 1707565364.0, "Full Content": "Let's say there's a binary with content:\n\n    void main() {/*...*/}\n    int swap(int *a, int *b) {\n        int tmp = &a;\n        &a = &b;\n        &b = tmp;\n    }\n\nIf we insert some memory safety check before every pointer operation into the binary, we'll have:\n\n    void main() {/*...*/}\n    int swap(int *a, int *b) {\n        validate_ptr(a);\n        validate_ptr(b);\n        int tmp = &a;\n        &a = &b;\n        &b = tmp;\n    }\n    \n    inline void validate_ptr(void *p) {\n        if (STACK_BOTTOM <= p && p < STACK_TOP) {\n            return;    \n        }\n        if (is_allocted_on_heap(p)) {\n            return;\n        }\n        exit(ILLEGAL_MEM_ACCESS);\n    }\n\nIt's 100% safe to execute this binary under real mode because it will never access illegal addresses.\n\nNow, if our OS has a \"compiler\" that \"compiles\" every executable binaries into such \"safe binaries\" before executing them, we can get rid of the MMU and let everything run under the same address space. This can reduce a lot of overhead.\n\nMy question is: is there any research or project that explored similar ideas? [Language-based system](https://en.m.wikipedia.org/wiki/Language-based_system) is the closest I can find but I'd like to see some binary-level approach. Rewrite everything in a particular managed language seems a very impractical idea to me.", "CntComments": 8, "Comments": [{"Comment": "This would slow down the code really dramatically, that's the reason it isn't done.  The MMU is in hardware so it is basically free, it's not a lot of overhead.  This method is much worse because it is in software.\n\n[AddressSanitizer](https://github.com/google/sanitizers/wiki/AddressSanitizer) does something like this in GCC, but more useful because it prevents illegal accesses within the stack itself, but it is rarely used because it is a 2x overhead on all programs, and that is too expensive apparently.", "Score": 9, "Replies": []}, {"Comment": "Think you are looking for Software-based Fault Isolation (SFI) [1]. Modern implementations of these ideas include Google\u2019s Native Client and WebAssembly.\n\n[1] Robert Wahbe, Steven Lucco, Thomas E. Anderson, and Susan L. Graham. 1993. Efficient software-based fault isolation. In Proceedings of the fourteenth ACM symposium on Operating systems principles (SOSP '93).", "Score": 3, "Replies": [{"Reply": "Thanks a lot. Will definitely check them out.", "Reply Score": 1}]}, {"Comment": "Managed languages are compiled into some binary language similar to (usually higher level) machine code. In the case of for instance Microsoft IL, this can then be validated when loading, in linear time (or constant overhead, if you will). This was a point in the design.\n\nNow, Microsoft don\u2019t actually do this in practice, and, AFAIK, never fully have :p", "Score": 2, "Replies": []}, {"Comment": "This seems like stack canaries without any of the safe guards.", "Score": 2, "Replies": []}, {"Comment": "What makes you think that reduces overhead?", "Score": 2, "Replies": []}, {"Comment": "Thanks a lot. Will definitely check them out.", "Score": 1, "Replies": []}]},{"Title": "Spearman Correlation Explained", "Score": 2, "URL": "https://www.reddit.com/r/compsci/comments/1amkc67/spearman_correlation_explained/", "CreatedAt": 1707470845.0, "Full Content": "Hi there,\n\nI've created a video [here](https://youtu.be/cHIJjZFfMYo) where I explain how the Spearman correlation works and what it tries to measure.\n\nI hope it may be of use to some of you out there. Feedback is more than welcomed! :)", "CntComments": 0, "Comments": []},{"Title": "Why are the values of bit numbers of different lengths different?", "Score": 48, "URL": "https://i.redd.it/jhynpe0roehc1.jpeg", "CreatedAt": 1707417468.0, "Full Content": "Why does 0.01 give a value of 1/4 and 0.010 gives a value of 2/8? I understand that they are the same numbers in the end, but I don't understand why their values are written differently.\n\nI calculated the decimal places like this:\n01 = 0 * 2^-1 + 1 * 2^-2 = 0 + 1/4 = 1/4\n\n010 = 0 * 2^-1 + 1 * 2^-2 + 0 * 2^-3 = 0 + 1/4 + 0 = 1/4\n\nWhere did the 2/8 come from?", "CntComments": 15, "Comments": [{"Comment": "If you take all the digits after the decimal and just interpret them as a binary integer, and then divide that by 2 to the power of the number of digits then you get 2/8.  I.e., in binary:\n\n0.01 = 0b01 / 0b100\n0.010 = 0b010 / 0b1000\n\nWhich is where they get the 1/4 and 2/8 from.  But the way you did it is perfectly correct.  It would have been much better to show an example like:\n\n0.011 = 0b011 / 0b1000\n\nTo show that this is 3/8.  It\u2019s exactly the same thing as in decimal where you move the decimal left or right being the same as multiplying by a power of 10; moving a binary decimal point left or right is like multiplying by a power of 2.\n\nIf you use a fixed, constant number of binary digits for the fractional part of the number then you are using what is called a \u201cfixed point\u201d representation, and then you can just store everything using an integer and an implicit divisor of 2 to the appropriate power.", "Score": 17, "Replies": [{"Reply": "thx a lot", "Reply Score": 2}]}, {"Comment": "> Where did the 2/8 come from?\n\nIt came from whoever made that chart trying to make a confusing analogy.\n\n> Why are the values of bit numbers of different lengths different?\n\nAs you noted: They aren't. Base-2 0.01 and 0.010 are the same value, in the same way that 1/4 and 2/8 are the same value.", "Score": 37, "Replies": [{"Reply": ">\u00a0It came from whoever made that chart trying to make a confusing analogy.\n\nDo take into account that you\u2019re looking at this chart without any context.\u00a0", "Reply Score": 21}, {"Reply": "Doesn't seem confusing to me - it's no different than saying 0.1 is \"1 tenth\" and 0.10 is \"10 hundredths\".", "Reply Score": 11}]}, {"Comment": "I think they're trying (perhaps awkwardly, hard to tell without more context) to explain binary place values. In decimal, you might describe 0.1 as one-tenth, and 0.10 as ten-hundredths. They represent the same value, but thinking in hundredths may be more convenient if you're about to add 0.02 and get twelve-hundredths. The same works in binary, except that your place values are factors of two instead of ten, giving you halves, fourths, eighths, and so on.", "Score": 7, "Replies": []}, {"Comment": "To extend the analogie to decimal:\n\n0.1 = 1/10\n\n0.10 = 10/100\n\nIt's a bit of a weird trick as supposed to giving the decimal places fixed values but it leads to the same result", "Score": 3, "Replies": []}, {"Comment": "Without Context I'm Making assumptions:\n\nThe denominator is 2^number_of_decimal_places\n\nThe numerator is the decimal places converted to binary.\n\nThat works, even if it doesn't auto reduce..\n\nHowever auto-reducing will always happen if you strip all the zeroes of the ~~left~~ right hand side.  :P", "Score": 2, "Replies": []}, {"Comment": "At the end of the day 2/8 is still 1/4\n\nIt\u2019s probably to help you understand decimals and fractions better. Remember when you learned fractions in elementary school? You learned about 4/8 before you learned 4/8 = 1/2. \n\nSo .010 is 2 parts out of 8. While .01 is 1 part out of 4. Both equal the same, just different ways to represent it.", "Score": 1, "Replies": []}, {"Comment": "thx a lot", "Score": 2, "Replies": []}, {"Comment": ">\u00a0It came from whoever made that chart trying to make a confusing analogy.\n\nDo take into account that you\u2019re looking at this chart without any context.\u00a0", "Score": 21, "Replies": [{"Reply": "realistically charts should be able to stand by themselves without extensive context required to understand them.\n\nthese days, now thay you don't have to pay per figure in journals, people slather articles in useless charts, and dont really give much consideration to how to make a punchy and concise figure or chart.\n\nthese authors make lecture notes and textbooks, and the sloppy figures trickle down.", "Reply Score": 6}]}, {"Comment": "Doesn't seem confusing to me - it's no different than saying 0.1 is \"1 tenth\" and 0.10 is \"10 hundredths\".", "Score": 11, "Replies": [{"Reply": "Does it matter if *you* don't find it confusing, when OP *did*?", "Reply Score": -5}]}, {"Comment": "realistically charts should be able to stand by themselves without extensive context required to understand them.\n\nthese days, now thay you don't have to pay per figure in journals, people slather articles in useless charts, and dont really give much consideration to how to make a punchy and concise figure or chart.\n\nthese authors make lecture notes and textbooks, and the sloppy figures trickle down.", "Score": 6, "Replies": [{"Reply": "This is a table of numbers. Why does it need to speak for itself without context? It's probably from a book, the author had complete control of the context. You're saying they should act as if the page is blank except for the table?", "Reply Score": 8}]}, {"Comment": "Does it matter if *you* don't find it confusing, when OP *did*?", "Score": -5, "Replies": [{"Reply": "I had hoped that rephrasing it in that way might alleviate the confusion.", "Reply Score": 9}]}, {"Comment": "This is a table of numbers. Why does it need to speak for itself without context? It's probably from a book, the author had complete control of the context. You're saying they should act as if the page is blank except for the table?", "Score": 8, "Replies": []}, {"Comment": "I had hoped that rephrasing it in that way might alleviate the confusion.", "Score": 9, "Replies": []}]},{"Title": "Achieving Consensus in Go: A Raft Implementation", "Score": 1, "URL": "https://github.com/jmsadair/raft", "CreatedAt": 1707263106.0, "Full Content": "https://github.com/jmsadair/raft", "CntComments": 0, "Comments": []},{"Title": "Could resistance-based memory somehow make its way into actual products?", "Score": 5, "URL": "https://www.reddit.com/r/compsci/comments/1aiykbk/could_resistancebased_memory_somehow_make_its_way/", "CreatedAt": 1707080782.0, "Full Content": "To give more context:\n\nI need to do a little 5 minutes presentation in a course for learning how to present, not Informatics.\n\nNow  I choose the topic analog In-Memory Computing for deep-learning models,  because just prior I saw a video from veritassium talking about analog  computing.\n\nI started to actually read the papers I found to the topic and the current situation seems pretty dire to me.\n\n[http://knowen-production.s3.amazonaws.com/uploads/attachment/file/5270/10.1038\\_s41565-020-0655-z.pdf](http://knowen-production.s3.amazonaws.com/uploads/attachment/file/5270/10.1038_s41565-020-0655-z.pdf) this paper from 2020 talks about it and especially the different types of resistance-based memory.\n\nRRAM  lacks a lot of research till you could even think about realizing an  actual piece of usuable hardware. MRAM and PCM don't seem to be that far  either.\n\nNow my main idea was that  you could be able to implement solutions with resistance-based memory  into real-time systems. Like a camera analyzing input in real-time with a  deep learning modell.\n\nBut it  seems that the only real application would be only for training the  model, not running it locally, with lower consumption and a big  deep-learning model in a small device?\n\nThe  markting guys from veritassium talked about using it to analyze body  movements \"like for the metaverse\", but how realistic is that?\n\nTo  me its really hard to put the piece together. One article in 2008 says  its impossible, Wikipedia meanwhile says IBM already has a PCM In-Memory  Computing chip, but refers to a paper from a institute. ChatGPT says  its already commercially used (?), meanwhile no products show up when I  search for any actual product.\n\nIf anybody has actual knowledge about the field, just give your input.", "CntComments": 4, "Comments": [{"Comment": "Resistive memory is possible, but it would need new breakthroughs. The existing technologies all suffer from some combination of low write endurance, impractically large footprint, or manufacturing difficulties.\n\nMost of these technologies are leftover from solid-state storage research in the early 2000s. But flash memory won that war, and interest in resistive memory evaporated. It's picked back up again recently because people realized how well-suited it is for deep learning. You could implement the weights as variable resistors and do inference in a single clock cycle.\n\nResistive memory may be a ways out, but I'd say that compute-in-memory as an idea is not going away. Moving information around is fundamentally expensive, and modern GPUs are more bottlenecked by memory bandwidth than by compute speed. This is especially relevant for deep learning, since you have to read out your entire 800GB language model for each token.\n\n>But it seems that the only real application would be only for training the model, not running it locally\n\nNo, it could do both. Any device that can do training can also do inference, because the only difference is that training involves a second backwards pass. \n\n>The markting guys from veritassium talked about using it to analyze body movements \"like for the metaverse\", but how realistic is that?\n\nSeems realistic. That's a pretty easy application for deep learning these days. You could probably do a lot more than that, like render the entire VR world using NeRFs.", "Score": 4, "Replies": [{"Reply": "How do you know all this", "Reply Score": 1}]}, {"Comment": "Yeah, flash memory got lots more attention for a while, but what's cool about memristors is that they could be very well-suited to machine learning, which could put us much closer to machines that acutally operate more like the brain, rather than digital simulations of the brain. There is definitely still active research, and this area is ripe for innovation.", "Score": 1, "Replies": []}, {"Comment": "How do you know all this", "Score": 1, "Replies": []}]},{"Title": "Reflective Analogue Processing Could a type of \u201cAnalogue Algorithm\u201d be encoded as geometry on a reflective surface to process data on a light beam?", "Score": 61, "URL": "https://i.redd.it/3jqpejnzg5gc1.png", "CreatedAt": 1706870083.0, "Full Content": "", "CntComments": 20, "Comments": [{"Comment": "[Photonic / Optical computing](https://en.wikipedia.org/wiki/Optical_computing) is a thing though it usually focuses on digital machines. There definitely is also work in the analog domain though. [Microsoft Research for example has recently published some information on a photonic analog computer](https://www.microsoft.com/en-us/research/blog/unlocking-the-future-of-computing-the-analog-iterative-machines-lightning-fast-approach-to-optimization/). You can also find plenty of other research on the topic online ([for example this](https://spj.science.org/doi/10.34133/adi.0002))", "Score": 14, "Replies": [{"Reply": "Thank you for the reply and links. Most optical computing seems very serial/linear at the moment, differentiation or edge detection . My question relates to continuous parallel computing. For example:compression or encryption of an optical image to a smaller processed image to reduce transmission bandwidth and increase security. Of course this would require an inverse reflective surface.", "Reply Score": 3}]}, {"Comment": "In principle I don't see why not. Multiple reflective surfaces (or even optic fiber) could transmit light between components that transform it just like data through an algorithm.   \n[Optical transistors](https://en.wikipedia.org/wiki/Optical_transistor) exist and I'd imagine they can be declined to a more analog approach. The question lies in what applications/advantages it would bring.", "Score": 7, "Replies": []}, {"Comment": "Laserdisc\u2026.", "Score": 4, "Replies": [{"Reply": "Serial and digital.", "Reply Score": 1}]}, {"Comment": "Nice acronym it has", "Score": 2, "Replies": [{"Reply": "Thanks. Add \"integrated device\" and we get what sort of computing it might represent.", "Reply Score": 2}]}, {"Comment": "anything to get me our of this serial world", "Score": 2, "Replies": []}, {"Comment": "So a fancy barcode? DVD? Blu-ray? Laserdisc? Minecraft.", "Score": 1, "Replies": [{"Reply": "More like processing a complete image in one clock/instruction cycle. But in analogue its a time slice.", "Reply Score": 2}]}, {"Comment": "Thank you for the reply and links. Most optical computing seems very serial/linear at the moment, differentiation or edge detection . My question relates to continuous parallel computing. For example:compression or encryption of an optical image to a smaller processed image to reduce transmission bandwidth and increase security. Of course this would require an inverse reflective surface.", "Score": 3, "Replies": [{"Reply": ">Most optical computing seems very serial/linear at the moment\n\nYou're right, and there's a reason for this. You can't do any nonlinear operations with just light; simple reflection or interference only allows linear operations like addition or multiplication. \n\nThis isn't enough to do complex computation. You need a process that behaves differently depending on the value of the signal. You really only need the tiniest spark of nonlinearity to build a computer (neural networks get it just by cutting off values below zero), but you do need it.\n\nThere are a bunch of candidate materials that have nonlinear interactions with light, but they all have issues - weak effects, hard to build on a chip, etc. So for now, most photonics research devices (including the microsoft one linked) have used light sensors and electronic circuits as their nonlinearity.", "Reply Score": 3}]}, {"Comment": "Serial and digital.", "Score": 1, "Replies": [{"Reply": "Laserdisc has analog video. (audio is digital though)", "Reply Score": 1}]}, {"Comment": "Thanks. Add \"integrated device\" and we get what sort of computing it might represent.", "Score": 2, "Replies": [{"Reply": "IDRAP \ud83d\udc4d", "Reply Score": 2}]}, {"Comment": "More like processing a complete image in one clock/instruction cycle. But in analogue its a time slice.", "Score": 2, "Replies": []}, {"Comment": ">Most optical computing seems very serial/linear at the moment\n\nYou're right, and there's a reason for this. You can't do any nonlinear operations with just light; simple reflection or interference only allows linear operations like addition or multiplication. \n\nThis isn't enough to do complex computation. You need a process that behaves differently depending on the value of the signal. You really only need the tiniest spark of nonlinearity to build a computer (neural networks get it just by cutting off values below zero), but you do need it.\n\nThere are a bunch of candidate materials that have nonlinear interactions with light, but they all have issues - weak effects, hard to build on a chip, etc. So for now, most photonics research devices (including the microsoft one linked) have used light sensors and electronic circuits as their nonlinearity.", "Score": 3, "Replies": [{"Reply": "Yes that seems quite reasonable if you consider that the topologies of current metasurfaces are very simple. You could say a 2 bit cpu only had access to four instructions and therefore could only do simple computations such as move, and, or,, not.", "Reply Score": 2}]}, {"Comment": "Laserdisc has analog video. (audio is digital though)", "Score": 1, "Replies": []}, {"Comment": "IDRAP \ud83d\udc4d", "Score": 2, "Replies": [{"Reply": "RAPID ?", "Reply Score": 3}]}, {"Comment": "Yes that seems quite reasonable if you consider that the topologies of current metasurfaces are very simple. You could say a 2 bit cpu only had access to four instructions and therefore could only do simple computations such as move, and, or,, not.", "Score": 2, "Replies": [{"Reply": "That CPU actually has all it needs to do universal computation; you can build any other computation by stacking those operations. Some real computers have been even simpler - the [Apollo guidance computer could only do NOR.](https://www.righto.com/2019/09/a-computer-built-from-nor-gates-inside.html)\n\nBut stacking linear operations doesn't get you anywhere. Any number of linear operations can be expressed as a single linear operation, so you can't build nonlinear computations out of them. This is the big problem with photonics right now.", "Reply Score": 3}]}, {"Comment": "RAPID ?", "Score": 3, "Replies": [{"Reply": "Nah", "Reply Score": 2}]}, {"Comment": "That CPU actually has all it needs to do universal computation; you can build any other computation by stacking those operations. Some real computers have been even simpler - the [Apollo guidance computer could only do NOR.](https://www.righto.com/2019/09/a-computer-built-from-nor-gates-inside.html)\n\nBut stacking linear operations doesn't get you anywhere. Any number of linear operations can be expressed as a single linear operation, so you can't build nonlinear computations out of them. This is the big problem with photonics right now.", "Score": 3, "Replies": [{"Reply": "Thanks for your input. Stacking or serial I think originates from our thinking that electrons are one dimensional, they travel in one direction down a wire. I agree that some photonics treats a light beam travelling along a waveguide as a photon electron analogy. However light can be a two dimensional. There are metasurface holograms so I wonder if I am really asking how to get 2D data onto a light beam so a lattice topology can process it as parallel and analogue. Prime cellular automata offer an unlimited dataset of lattice based topologies, as in the example image.", "Reply Score": 1}]}, {"Comment": "Nah", "Score": 2, "Replies": []}, {"Comment": "Thanks for your input. Stacking or serial I think originates from our thinking that electrons are one dimensional, they travel in one direction down a wire. I agree that some photonics treats a light beam travelling along a waveguide as a photon electron analogy. However light can be a two dimensional. There are metasurface holograms so I wonder if I am really asking how to get 2D data onto a light beam so a lattice topology can process it as parallel and analogue. Prime cellular automata offer an unlimited dataset of lattice based topologies, as in the example image.", "Score": 1, "Replies": []}]},{"Title": "Whom do you think will win Turing Award this year?", "Score": 4, "URL": "https://www.reddit.com/r/compsci/comments/1ah6ab7/whom_do_you_think_will_win_turing_award_this_year/", "CreatedAt": 1706887622.0, "Full Content": "Since the announcement of the award is approaching, whom do you think will win Turing Award?", "CntComments": 14, "Comments": [{"Comment": "You mean \"who\"?", "Score": 27, "Replies": [{"Reply": "Whomst", "Reply Score": 10}]}, {"Comment": "I think that finally Elon Musk will pass the Turing Test at long last.", "Score": 19, "Replies": [{"Reply": "He has to wait for Zukerborg", "Reply Score": 3}, {"Reply": "He's been like every chat bot that's been turned loose on twitter in the past; ends up as a rightwing racist but job.", "Reply Score": 4}]}, {"Comment": "Schmidhuber", "Score": 6, "Replies": []}, {"Comment": "I am somewhat surprised that Christos Papadimitriou hasn't won the price yet...but I doubt it will change this (or the next) year(s)", "Score": 3, "Replies": [{"Reply": "I don't know much about his research (TCS), as its beyond me, but the book he co-authored on combinatorial optimization is a gem. Not mentioning any of his other classics as I haven't read/studied them.\n\nEven if he never gets awarded the Turing prize he'll remain a great scientist for me.", "Reply Score": 1}]}, {"Comment": "A fairly old white guy.", "Score": 23, "Replies": []}, {"Comment": "Will van der Aalst", "Score": 2, "Replies": []}, {"Comment": "ChatGPT", "Score": 0, "Replies": []}, {"Comment": "Some kind of grammar helper AI.", "Score": -13, "Replies": []}, {"Comment": "Whomst", "Score": 10, "Replies": [{"Reply": "Whomst?\u00a0 Themst.", "Reply Score": 5}]}, {"Comment": "He has to wait for Zukerborg", "Score": 3, "Replies": []}, {"Comment": "He's been like every chat bot that's been turned loose on twitter in the past; ends up as a rightwing racist but job.", "Score": 4, "Replies": []}, {"Comment": "I don't know much about his research (TCS), as its beyond me, but the book he co-authored on combinatorial optimization is a gem. Not mentioning any of his other classics as I haven't read/studied them.\n\nEven if he never gets awarded the Turing prize he'll remain a great scientist for me.", "Score": 1, "Replies": []}, {"Comment": "Whomst?\u00a0 Themst.", "Score": 5, "Replies": []}]},{"Title": "Incremental Database Computations | Feldera", "Score": 1, "URL": "https://www.feldera.com/blog/incremental-database-computations/", "CreatedAt": 1706897894.0, "Full Content": "", "CntComments": 1, "Comments": []},{"Title": "Bottom-up vs Top-down CS Education", "Score": 34, "URL": "https://www.reddit.com/r/compsci/comments/1afge1t/bottomup_vs_topdown_cs_education/", "CreatedAt": 1706702923.0, "Full Content": "Bottom-up:\n\n  \\- Mathematics --> CS theories --> Programming/Frameworks etc.\n\nTop-down:\n\n  \\- Programming/Frameworks etc. --> CS theories --> Mathematics  \n\n\nObviously everyone learns differently, but personally for you, which one do you think is the best path to learn CS, and why?", "CntComments": 26, "Comments": [{"Comment": "Why not learning those concurrently?  \nE. G. you start learning 3 subjects - Programming, CS Theory and Math with some predesigned paths to all 3:\n\nMath:\n\nDiscrete math -> Calculus 1 -> Linear Algebra -> Calculus 2 -> Diff. Eqs -> Probability Theory\n\n&#x200B;\n\nProgramming:\n\nIntroduction to programming -> Introduction to algorithms -> OOP -> Then desired language and framework\n\n&#x200B;\n\nAnd CS Theory:\n\nComputer Architecture -> Operating Systems -> Database Management Systems -> Compiler Construction -> Networks -> etc.\n\n&#x200B;\n\nSo, you will learn at all 3 frontiers at the same time. This is an approach which is used in universities (at least in my country). You just need to carefully set the sequence considering prerequisites for different courses", "Score": 26, "Replies": [{"Reply": "Diff eq over calc 3? Idk I've heard both can be useful. \nAt most of the colleges I've seen discrete 1&2 is required but that could just be the unis I've looked at.", "Reply Score": 3}, {"Reply": "I'd cut out diff eq and tack an \"elective\" slot on the end that could include diff eq, logical foundations of math, number theory, abstract algebra, a higher level logic course (constructive, linear, etc), or diff eq, depending on the person's specific area of interest.\n\nAlso, that CS Theory section should probably be renamed to CS Applications. I'd expect a CS Theory section to include: \n\nTheory of Computation -> Analysis of Algorithms -> a PL course that involves formal semantics and/or reasoning about program correctness -> electives,\n\nwith electives being Type Theory, Cryptography theory, Quantum Computing Theory,  More analysis of Algorithms (randomized, probabilistic, approximation, algorithmic game theory/econ), Theory of Computation beyond basic TMs, modern PL theory, etc depending on interest.\n\nI'd sneak Concurrent Programming somewhere into the programming section as well, ideally before you hit specific frameworks.", "Reply Score": 2}]}, {"Comment": "Middle-Out", "Score": 13, "Replies": []}, {"Comment": "It depends on your perspective.\n\nIf you want to be a researcher, a bottom-up approach is necessary. \n\nIf you want to be a web developer, a top-down approach without a lot of basis is also good.\n\nMy personal choice is to be the bottom-up guy. I think every person should start from somewhere in the middle before climbing up to the higher level concepts. The way I learned from people that are smarter than me is the difference between a electrician at a construction site versus the construction engineer.\n\nIn my vision, we should pioneer thingies with the knowledge of it's basis. After all, everything is built on top of it. Lest it will collapse at some point.", "Score": 25, "Replies": []}, {"Comment": "For employability, top down is faster, because you can provide value with small fixes, doc-PRs etc while already being employed and learn on the job. Obviously, this runs at the risk of you stagnating because you don't have time to learn, or get tasks that overwhelm you so you drop out.\n\nOn the other hand, it's also very rewarding to see a tangible outcome, which happens faster when you learn top down.\n\nBottom up takes a lot of time, time people outside degrees often do not have.\n\n&#x200B;\n\nThere is the application domain/career changer way too:\n\n\\- Mathematics -> Programming/Frameworks -> CS Theory\n\nIts what I am doing, coming from Physics through data and ML now to SWE and learning CS fundamentals on the job.\n\n&#x200B;\n\nMy take, if you are at the beginning: bottom up with a degree has the best job opportunities. Second is top down as a self learner which gets you employed the fastest.", "Score": 8, "Replies": []}, {"Comment": "I think that I'm in the all-at-omce camp.\nSome math can be really helpfull to understand programming languages and frameworks. But you can already program a lot without knowing what happens the-scenes.", "Score": 3, "Replies": []}, {"Comment": "Bottom up by far", "Score": 3, "Replies": []}, {"Comment": "I'm still studying, but I'd say a solid math basis can really help with understanding computer science principles. (Strong calculus basis) It is the foundation way of thinking that help. Kind of \"providing a proof\".\n\nThe rest I think is best to learn concurrently since it allows to connect concepts together, provided you do so consciously.", "Score": 3, "Replies": []}, {"Comment": "Neither of these is a complete CS education, and this is coming from someone who is a theorist by education.\n\nI am old-school.  I *hated* EE classes, but I appreciate the fact that I learned how to build the basic components, then build those into a basic processor, then program that processor using machine code, then turn that into assembly, then turn that into low-level language and build modern language features like OOP on top of that.\n\nDoes every developer need to know all of that?  No.  But to call yourself a Computer Scientist, I think that's the minimum.", "Score": 2, "Replies": []}, {"Comment": "neither of those makes any sense to me.", "Score": 3, "Replies": []}, {"Comment": "I'm in the top-down camp. Start with programming and dig deeper when necessary. e.g. when wanting to add a machine learning component, I dive into the CS theory about machine learning and where necessary I dive deeper into the relevant mathematics. In case of machine learning I needed to brush up on linear algebra, while for neural networks I needed to learn more on using differential equations.", "Score": 1, "Replies": []}, {"Comment": "Top down is easier. It will be filled with alot of eureka moments where something clicks.", "Score": 0, "Replies": [{"Reply": "The way I learn, I do much better learning in the context of an application. I do enjoy the eureka moments and de-mystification. Bottom-up, it's too easy for me to check out.", "Reply Score": 1}]}, {"Comment": "What do the arrows mean?", "Score": 1, "Replies": []}, {"Comment": "Everyone has a left and a right brain.\n\nThat said everyone is much more of one.\n\nI'm right brained and thrive on top down.\n\nMy friend is left brained and is bottom up.\n\nFind friends who are able to think differently.\n\nIt takes all kinds :D ! - <that said top down is best ;D>", "Score": 1, "Replies": []}, {"Comment": "I would've quit CS if I had to do a bottom up approach. Now that I enjoy the major and am pretty deep into it, I enjoy getting into the weeds and the CS fundamentals, but if instead of my intro to CS I had to do Computer Architecture or some shit I would've never continued. You need to balance motivation with learning, bottom-up would be the best if you had infinite willpower and don't mind doing hard shit but many students don't.", "Score": 1, "Replies": []}, {"Comment": "Diff eq over calc 3? Idk I've heard both can be useful. \nAt most of the colleges I've seen discrete 1&2 is required but that could just be the unis I've looked at.", "Score": 3, "Replies": [{"Reply": "Those paths were just for an example. In my country - some universities have 4 semesters of Calculus (or Analysis - in my country we almost never study calculus without theory, so most of the UNIs are teaching Real Analysis right away on the first year of UNI), some have 2 or 3 (The Best Mathematical university has 3 semesters of RA with full coverage of 4-semester cousres and then Functional and Complex Analysis).  \nI my University I studied 2 semesters of Calculus with little theory (We did proofs, but not as rigorously as students of Math Degrees, but more rigorously then MIT Calculus I and II on OCW) and we studied everything: sequences, series, derivatives, integrals, 2- and 3- integrals, curve- and surface- integration, parametric integrals, functional sequences and series, Fourier series, Fourier Transform and Integral  \n\n\nAnd then next semester we studied differential equations with almost no proofs. \n\nI was CS student btw, and our programm looked like:\n\n&#x200B;\n\n1st semester:\n\n\\- Programming I - OOP on Eiffel\n\n\\- Computer Architecture\n\n\\- Analytical Geometry - Study of lines, planes, curves, surfaces and affine and linear transformations\n\n&#x200B;\n\n2nd semester:\n\n\\- Programming II - programming on C\n\n\\- Algorithms and Data Structures\n\n\\- Discrete Mathematics\n\n\\- Linear Algebra\n\n&#x200B;\n\n3'd Semester:\n\n\\- Differential Equations\n\n\\- Theoretical Computer Science - Computability, Complexity and Automata\n\n\\- Database Management Systems I - we studied basics of using RDBMS\n\n&#x200B;\n\n&#x200B;\n\n4'th Semester:\n\n\\- Probability Theory - Discrete and Continious Probability\n\n\\- Computer Networks\n\n\\- Object-Oriented Analysis and Design - Design Patterns Course\n\n\\- Database Management Systems II - we studied internal construction of RDBMS and usage on Non-Relational DBMS\n\n\\- Artifficial Intelegence - we studied different basic AI algorithms (like DFS, BFS, A\\*, Backtracking), Prolog Language, Evolutional Algorithms\n\n&#x200B;\n\n5'th Semester:\n\n\\-  Machine Learning\n\n\\- Introduction to CI/CD\n\n\\- Compiler Construction I - Practical study of creating a compiler\n\n\\- Intro to functional programming\n\n\\- Information Theory - but instead we had 12 lectures on discrete probability theory (because our professor was stupid jerk who didn't know the subject)\n\n  \n6'th Semester:\n\n\\- Compiler Construction II - study of Type Theory\n\n\\- Information Theory\n\n\\- Distributed Systems\n\n&#x200B;\n\n&#x200B;\n\nWe also had other courses, but they are not relevant here, like we had course on Control Theory for preparation to Robotics track, Physics courses - Mechanics and Electrical engineering, Project Management and other unrelated stuff", "Reply Score": 1}, {"Reply": "Anyone reading this: take calc 3 and not DE. DE is a pain in the ass exercise in memorization with almost NO explanation of how/why the shit you\u2019re doing works.", "Reply Score": 1}]}, {"Comment": "I'd cut out diff eq and tack an \"elective\" slot on the end that could include diff eq, logical foundations of math, number theory, abstract algebra, a higher level logic course (constructive, linear, etc), or diff eq, depending on the person's specific area of interest.\n\nAlso, that CS Theory section should probably be renamed to CS Applications. I'd expect a CS Theory section to include: \n\nTheory of Computation -> Analysis of Algorithms -> a PL course that involves formal semantics and/or reasoning about program correctness -> electives,\n\nwith electives being Type Theory, Cryptography theory, Quantum Computing Theory,  More analysis of Algorithms (randomized, probabilistic, approximation, algorithmic game theory/econ), Theory of Computation beyond basic TMs, modern PL theory, etc depending on interest.\n\nI'd sneak Concurrent Programming somewhere into the programming section as well, ideally before you hit specific frameworks.", "Score": 2, "Replies": [{"Reply": "Yeah, agree. Especially with concurrent programming.  \nAlso, in my computation course we learned Turing Machines, Lambda calculus and Primitive recursive functions - so we covered all 3 computability models. Unfortunately, because of this, we didn't have time to cover complexity theory", "Reply Score": 1}]}, {"Comment": "The way I learn, I do much better learning in the context of an application. I do enjoy the eureka moments and de-mystification. Bottom-up, it's too easy for me to check out.", "Score": 1, "Replies": [{"Reply": "That's interesting I could never do it that way. I like to see the applied science and work backwards. It's more motivating for me hitting those eureka moments. It's bad for research but it's agile for engineering I think.", "Reply Score": 0}]}, {"Comment": "Those paths were just for an example. In my country - some universities have 4 semesters of Calculus (or Analysis - in my country we almost never study calculus without theory, so most of the UNIs are teaching Real Analysis right away on the first year of UNI), some have 2 or 3 (The Best Mathematical university has 3 semesters of RA with full coverage of 4-semester cousres and then Functional and Complex Analysis).  \nI my University I studied 2 semesters of Calculus with little theory (We did proofs, but not as rigorously as students of Math Degrees, but more rigorously then MIT Calculus I and II on OCW) and we studied everything: sequences, series, derivatives, integrals, 2- and 3- integrals, curve- and surface- integration, parametric integrals, functional sequences and series, Fourier series, Fourier Transform and Integral  \n\n\nAnd then next semester we studied differential equations with almost no proofs. \n\nI was CS student btw, and our programm looked like:\n\n&#x200B;\n\n1st semester:\n\n\\- Programming I - OOP on Eiffel\n\n\\- Computer Architecture\n\n\\- Analytical Geometry - Study of lines, planes, curves, surfaces and affine and linear transformations\n\n&#x200B;\n\n2nd semester:\n\n\\- Programming II - programming on C\n\n\\- Algorithms and Data Structures\n\n\\- Discrete Mathematics\n\n\\- Linear Algebra\n\n&#x200B;\n\n3'd Semester:\n\n\\- Differential Equations\n\n\\- Theoretical Computer Science - Computability, Complexity and Automata\n\n\\- Database Management Systems I - we studied basics of using RDBMS\n\n&#x200B;\n\n&#x200B;\n\n4'th Semester:\n\n\\- Probability Theory - Discrete and Continious Probability\n\n\\- Computer Networks\n\n\\- Object-Oriented Analysis and Design - Design Patterns Course\n\n\\- Database Management Systems II - we studied internal construction of RDBMS and usage on Non-Relational DBMS\n\n\\- Artifficial Intelegence - we studied different basic AI algorithms (like DFS, BFS, A\\*, Backtracking), Prolog Language, Evolutional Algorithms\n\n&#x200B;\n\n5'th Semester:\n\n\\-  Machine Learning\n\n\\- Introduction to CI/CD\n\n\\- Compiler Construction I - Practical study of creating a compiler\n\n\\- Intro to functional programming\n\n\\- Information Theory - but instead we had 12 lectures on discrete probability theory (because our professor was stupid jerk who didn't know the subject)\n\n  \n6'th Semester:\n\n\\- Compiler Construction II - study of Type Theory\n\n\\- Information Theory\n\n\\- Distributed Systems\n\n&#x200B;\n\n&#x200B;\n\nWe also had other courses, but they are not relevant here, like we had course on Control Theory for preparation to Robotics track, Physics courses - Mechanics and Electrical engineering, Project Management and other unrelated stuff", "Score": 1, "Replies": []}, {"Comment": "Anyone reading this: take calc 3 and not DE. DE is a pain in the ass exercise in memorization with almost NO explanation of how/why the shit you\u2019re doing works.", "Score": 1, "Replies": [{"Reply": "Yeah, but we needed them for Phys I and II as well as control theory. And we didn't need to have calculus 3, because we covered all calculus topics (and more) in calc I and II, so we didn't have anything to cover in calculus.   \nBut I don't agree that DiffEq is pain in the ass - pretty easy course, but yeah a lot of memorization and stupid busy-work doing the same thing over and over again. But yeah, on DiffEq we covered and implemented some numerical methods of solving DEs, so it was at least an interesting part", "Reply Score": 1}]}, {"Comment": "Yeah, agree. Especially with concurrent programming.  \nAlso, in my computation course we learned Turing Machines, Lambda calculus and Primitive recursive functions - so we covered all 3 computability models. Unfortunately, because of this, we didn't have time to cover complexity theory", "Score": 1, "Replies": []}, {"Comment": "That's interesting I could never do it that way. I like to see the applied science and work backwards. It's more motivating for me hitting those eureka moments. It's bad for research but it's agile for engineering I think.", "Score": 0, "Replies": []}, {"Comment": "Yeah, but we needed them for Phys I and II as well as control theory. And we didn't need to have calculus 3, because we covered all calculus topics (and more) in calc I and II, so we didn't have anything to cover in calculus.   \nBut I don't agree that DiffEq is pain in the ass - pretty easy course, but yeah a lot of memorization and stupid busy-work doing the same thing over and over again. But yeah, on DiffEq we covered and implemented some numerical methods of solving DEs, so it was at least an interesting part", "Score": 1, "Replies": [{"Reply": "I agree that calc 3 is far easier than calc 2 in that it just extends to more dimensions. But you have to have that extra-dimensional cherry popped. Gotta really bake into your head that 3+n dimensions is surprisingly intuitive. Once you see that, you can\u2019t unsee it. DE? Fucking alchemy, Man.", "Reply Score": 1}]}, {"Comment": "I agree that calc 3 is far easier than calc 2 in that it just extends to more dimensions. But you have to have that extra-dimensional cherry popped. Gotta really bake into your head that 3+n dimensions is surprisingly intuitive. Once you see that, you can\u2019t unsee it. DE? Fucking alchemy, Man.", "Score": 1, "Replies": []}]},{"Title": "Time series segmentation paper reading list repository", "Score": 2, "URL": "https://github.com/lzz19980125/awesome-time-series-segmentation-papers", "CreatedAt": 1706622387.0, "Full Content": "", "CntComments": 0, "Comments": []},{"Title": "Can a simple functional sieve be fast? Optimizing Tromp's algorithm on HVM.", "Score": 18, "URL": "https://gist.github.com/VictorTaelin/a5571afaf5ee565689d2b9a981bd9df8", "CreatedAt": 1706537227.0, "Full Content": "", "CntComments": 1, "Comments": [{"Comment": "HVM (2) has got to be one of the most exciting projects from recent years. Really interesting to see where it goes", "Score": 2, "Replies": []}]},{"Title": "ShellSort example graphic", "Score": 4, "URL": "https://www.reddit.com/r/compsci/comments/1ae6nkr/shellsort_example_graphic/", "CreatedAt": 1706563476.0, "Full Content": "Since I couldn't find any good example graphic for the shellsort algorithm, I made my own.   \nI hope it helps someone in the future.\n\n&#x200B;\n\n[ShellSort example graphic](https://preview.redd.it/l9m6m1jd5gfc1.png?width=1727&format=png&auto=webp&s=976270a487b13a572f28761fd7169e9bdb2c4e04)", "CntComments": 0, "Comments": []},{"Title": "Question about Big-Oh(Log n)", "Score": 8, "URL": "https://www.reddit.com/r/compsci/comments/1adnkyq/question_about_bigohlog_n/", "CreatedAt": 1706505762.0, "Full Content": "Was doing some readings and found out that the binary search algorithm was O(Log n). If my understanding is correct, that\u2019s because every iteration we are cutting the numbers we have to search through in half, so the worst case scenario is log \u2082 N. So in the case that N = 8, then it would take 3 iterations before we can\u2019t divide by 2 any longer? Should we always try to get algorithms to take this amount of time assuming our algorithm isnt O(1)?", "CntComments": 28, "Comments": [{"Comment": "You should always try to solve a problem in the most reasonably efficient way given the nature of the problem.  There are many problems that cannot be solved in logarithmic time,  such as searching an unsorted list, which must be linear (since it's unsorted, the best you could possibly do is to look at every element once, at least at this abstraction level).\n\nSource:  I taught Algorithms at MIT.", "Score": 42, "Replies": [{"Reply": "Can you teach me?\nIm having a hard time understanding asymptotic notation and hownit relates with big-o", "Reply Score": 2}, {"Reply": "Strictly speaking it depends on the scale of problem you need. Searching through an unsorted list is quicker than doing a binary chop through a sorted one - but only if the list is small in size. \n\nLast time I checked this, it was true for lists about 10 long. That was a few years ago, might be true for 100 items now. Computers are getting faster.", "Reply Score": 0}]}, {"Comment": "In general, a problem will need to process all of its input in order to answer a question about it. That is, O(n) is often considered \\*optimal\\* for many problem settings.\n\nIn this case of binary search, you must start with extra information about the input - namely, that the input is already in sorted order. Knowing that extra piece of information, it allows someone to locate an item (or determine an item is not there) in less than linear time (in this case, O(log(n))-time). \n\nYes, you should always try to make algorithms take O(1)-time when possible, but it often isn't. Yes, you should try to get algorithms to run in O(log(n)) time when possible, but it often isn't possible.", "Score": 11, "Replies": []}, {"Comment": "Not necessary. If n is small, the overhead of the \"Log n\" algorithm can become more time consuming than using a simpler but slower algorithm. \n\nAlso the context of the operation is important. Spending 1ms instead of 10ms to sort a list, once at the start of a program, does not matter as much as shaving off a single ms during a real time operation (e.g. rendering a frame in a game).", "Score": 6, "Replies": [{"Reply": "Quoting Rob Pike:\n\n> Fancy algorithms are slow when n is small, and n is usually small. Fancy algorithms have big constants.", "Reply Score": 3}]}, {"Comment": "Note that time complexity is not the only metric we might want to optimise- some algorithms have different space complexities too, and sometimes that can influence your choice.\n\nAlso, time complexity is usually theoretical and abstracts away from/ignores real-world facts about computation (things like cache locality). It also ignores potentially very large constants - which can have practical impacts when working with sufficiently small values of n.", "Score": 5, "Replies": []}, {"Comment": "I mean there's better bounds than O(log (n)) that aren't constant but the point of big-Oh is to describe an asymptotic upper bound as the input size to a problem grows larger.\n\nIt'd be great if we could design algorithms that ran in logarithmic or better time but that's not possible for many kinds of problems (like searching an unsorted list as explained in other comments on this thread).\n\nInterestingly proving lower bounds for certain types of algorithms is very difficult too, and we don't have tight bounds for many algorithms.", "Score": 5, "Replies": []}, {"Comment": "Not every problem can be solved in O(log n). In fact, most problems can be proved to be impossible to solve with an O(log n) algorithm", "Score": 3, "Replies": []}, {"Comment": "A lot of people miss the point of asymptotic analysis. It's less about speed and more about \"scalability\". An algorithm with a lower Big-Oh order isn't necessarily *faster*, it's simply **faster at scale**. That means no matter how slow it may be, when N gets sufficiently large (when you scale up the size of your data, or the number of your users, or whatever N happens to refer to), the algorithm with lower order will always be faster than the one with higher order -- but only at sufficiently high scale.\n\nI used to work at a \"Big Data\" consultancy. One question I liked to ask during interviews was, \"Are you familiar with Big-Oh notation / asymptotic notation? (then assuming the answer was yes) How does it relate to Big Data?\" I was surprised by how many candidates didn't really seem to understand what the N referred to. Big data was one of those situations where Big-Oh notation was truly relevant. \n\nIronically, after *scaling* up the company, the customers with use relevant use cases couldn't really afford us anymore and we were just helping big companies with deep pockets apply inefficient algorithms at small scale. \"This doesn't make sense for your problem.\" \"Doesn't matter, my manager said we need to put the big in our data, so just do it.\" Then at the end of the project, \"Why is it so slow?\"", "Score": 3, "Replies": [{"Reply": "This makes sense to me. I know that at smaller scales, algorithms that are exponential or linear may preform similarly or faster than logarithmic algorithms. \n\nI haven\u2019t worked with big data like you\u2019re describing yet, but I imagine that in a real world scenario, you would figure out how big N might be and write algorithms that would hopefully run faster at scale. Would it be accurate to say that we should figure out how much data we\u2019re handling  before deciding how to write our algorithm?", "Reply Score": 1}]}, {"Comment": "You can't really get algorithms to take a different amount of time.  Big O notation is a tool to teach you how algorithm performance scales with more elements.\n\nIf you need to visit every element, you can't beat O(n).  If you need to operate on pairs of elements, it's O(n^2).  The only way to make things O(log(n)) is to set up a data structure that can be ignored in large parts, but some problems can't be done like that.", "Score": 6, "Replies": []}, {"Comment": "Sure, and how about some lollipops and rainbows while we're at it?", "Score": -8, "Replies": [{"Reply": "If you\u2019re referring to the comment about writing every algorithm as big o log n, it\u2019s purely hypothetical to allow for me to get a better understanding of log n notation. Obviously you would never be able to do that. I\u2019m just trying to understand.\n\nDon\u2019t know why people in this field get so pressed over seemingly small things", "Reply Score": 7}]}, {"Comment": "[deleted]", "Score": -1, "Replies": [{"Reply": "1. Binary search is O(log n), not O(n log n).\n2. A hash lookup does not operate on sorted data. It operates on hashed (unsorted) data.\n3. Hash tables do not have \"true\" / guaranteed constant time lookup. They only have expected constant time, worst case linear time lookup, depending on the number of hash collisions.\n4. Binary search can provide you with a partition of the sorted data into items less than and items greater than (or equal to) the searched element. A hash lookup can't do that.", "Reply Score": 3}]}, {"Comment": "In a real world application, individual choices rarely happen in a vacuum. If the input to your program is unsorted, you'll need to sort it first in order to use binary search.\u00a0 So the overall time complexity of your program would be O(n\\*log(n) + m\\*log(n)), where m is the number of searches you'll be doing on the same data.\u00a0 If you're only going to search for a single value then quit (m=1), then the overall\u00a0runtime is dominated by the sort phase, and becomes O(n\\*log(n)).\u00a0 At that point, you can probably just use linear search and do away with the sorting altogether, for O(n) time.", "Score": 1, "Replies": []}, {"Comment": "There are yet-slower-but-still-growing-without-bound functions; for example, the [disjoint-set data structure](https://en.wikipedia.org/wiki/Disjoint-set_data_structure) runs in O(inverse-[Ackermann](https://en.wikipedia.org/wiki/Ackermann_function)(n)) time.", "Score": 1, "Replies": []}, {"Comment": "As others have said, the constant factors matter. So actually having a variety of algorithms to solve the same problem with constant factor & big-O tradeoff is a good thing. For example, a lot of the big-O optimal algorithms out there use randomization, which is inherently more expensive in terms of constant factor than linear scan due to locality. So randomization matters at scale, but makes no sense for smaller problem sizes. \n\ne.g. Randomized matrix permanent estimator is not chosen over exponential exact algorithm due to its speed, but due to running out of memory. At n=20, say, exponential algorithm requires enormous amounts of memory. While the FPRAS is still very slow, it is more memory efficient.", "Score": 1, "Replies": []}, {"Comment": "Can you teach me?\nIm having a hard time understanding asymptotic notation and hownit relates with big-o", "Score": 2, "Replies": [{"Reply": "Unlike MIT professor, my consulting fee is very cheap (free). Feel free to DM me any questions and I can answer and link you to some good resources. \n\nSource: Was a TA for an algorithms class at a worse school than MIT", "Reply Score": 3}, {"Reply": "I suspect you couldn't afford my consulting fee.  I'm happy to answer specific questions here in my free time, but for actual work, I am expensive.", "Reply Score": 2}]}, {"Comment": "Strictly speaking it depends on the scale of problem you need. Searching through an unsorted list is quicker than doing a binary chop through a sorted one - but only if the list is small in size. \n\nLast time I checked this, it was true for lists about 10 long. That was a few years ago, might be true for 100 items now. Computers are getting faster.", "Score": 0, "Replies": [{"Reply": "Yes, but this is a different abstraction level.", "Reply Score": 10}, {"Reply": "That's the point of big O notation though. You don't care so much about the concrete number of operations required at a certain input size. It is rather how the number of operations grows in relation to the input size.", "Reply Score": 4}, {"Reply": "It really depends on how things are laid out in memory.  If you can set the solution up in such a way that the binary chop guarantees a cache miss, and the scan guarantees only cache hits, then you can scan roughly 200-500 objects before the memory read will come back.  A lot of the performance fuzziness comes from not really having enough control to guarantee things like that.  If your sorted data is in a flat array, and you know the # of items in the array, jumping around to different indexes isn't necessarily slower than reading items contiguously.\n\nLikewise, if your array contains pointers to objects, and the data you are filtering on is something contained within the object, every scan is going to be a memory access anyways, and there will basically be no performance penalty to binary search.", "Reply Score": 3}, {"Reply": "It's not even about searching, its about the whole asymptotic notation concept", "Reply Score": 1}]}, {"Comment": "Quoting Rob Pike:\n\n> Fancy algorithms are slow when n is small, and n is usually small. Fancy algorithms have big constants.", "Score": 3, "Replies": []}, {"Comment": "This makes sense to me. I know that at smaller scales, algorithms that are exponential or linear may preform similarly or faster than logarithmic algorithms. \n\nI haven\u2019t worked with big data like you\u2019re describing yet, but I imagine that in a real world scenario, you would figure out how big N might be and write algorithms that would hopefully run faster at scale. Would it be accurate to say that we should figure out how much data we\u2019re handling  before deciding how to write our algorithm?", "Score": 1, "Replies": [{"Reply": "Advice #1: Don't pre-optimize -- this is a common mistake. Don't spend too much extra time worrying about writing the most efficient code possible. Inefficient code that works is better than no working code. \n\nAdvice #2: Focus on actual bottle necks -- Once you have working code running in something resembling the real world, assuming you have an actual performance problem, focus on fixing the real bottle necks -- run a profiler, see where you're code is spending the most time, think about how you can reduce the time spent in that part of the algorithm (either by adjusting the coefficient -- making each iteration of the code block faster) or by adjusting the order (changing the higher level algorithm so that you call that block less frequently). \n\nAdvice #3: If and when you actually find yourself needing to address a performance issue, see if you can loosen the constraints. A lot of performance issues come from self imposed assumed constraints -- and likewise those order or magnitude type improvements come from recognizing and removing them from the bottle neck. Do you really need to sort the data to solve your problem? Do you even need to look at all of it? Not every problem can be solved this way, but many can. Think of most puzzle type problems -- they're difficult to solve because they rely on our cognitive biases that expect the world to behave a certain way. But if you know your cognitive biases, it's easier to take the blinders off and find the solution to the puzzle -- oh, why do I expect these pieces to go together at a right angle, silly me, if I use a 78 degree angle it will look ugly, but it will work. Those are the kind of self imposed constraints I'm taking about here -- nobody asked you to add them in, you just did it out of habit. \n\nAdvice #4: understand the difference between speed and scalability (or coefficients and order). Most people seem to think that an \"order of magnitude\" improvement means a decimal place. That may be a big improvement, but it's still just a change to a coefficient. Order of magnitude improvements really mean you've changed the way an algorithm scales (and you had a problem with scale in the first place). \n\n```\ny = a*x^2 + b*x + c\n```\n\nIn the above equation, `a`, `b` and `c` are coefficients. X^2 is the order. If X is zero, C is the only coefficient that matters. If X is large, A is likely the coefficient that matters the most. If you know that X will be large, reducing A will have a bigger impact than reducing B or C, but reducing the Big-Oh order from X^2 to X or log(X) or 1 will likely have an even bigger impact. \n\nBig-Oh notation is simply a quick way of communicating this with people who understand the point. A lot of people don't understand the point though, so using shorthand notation to communicate something that nobody in the room understands just makes for a lot of buzzword soup.\n\nAdvice #5: Don't worry about figuring out exactly what point N becomes a problem. Either you have a problem with Big N (usually because you expect it to grow indefinitely), or you don't have a problem with Big N. If you have a problem, it will probably be pretty obvious (once you're in production that is). If you have a performance problem, but it isn't related to a \"big N\", focus on coefficients. If it is related to a \"big N\" then spend a little more time thinking about how you can re-architect things to scale better.\n\nAdvice #6: Big-Oh isn't just for algorithms. The world isn't only made up of code. You're system may work fine for a hundred users, but struggle with a million. Maybe you can fix it with some code adjustments, but maybe the bottlenecks are in the real world. Strategies that work at small scale don't always work at larger scale. When I was at that same Big Data consultancy, I worked with a lot of people who were used to working at small startups. They couldn't understand why some of our large \"enterprise\" sized customers were so... slow. I thought it was funny that my colleagues who specialized in \"Big Data\" and \"Scalability\" didn't seem to realize that the same concepts applied to scaling up a company's size. The strategies that work at small scale (hiring, organization, separation of responsibilities, redundancy, risk mitigation) don't always make sense at large scale. Large companies may have larger coefficients, but they don't hit a wall scaling up when you try to add a few more employees.", "Reply Score": 1}]}, {"Comment": "If you\u2019re referring to the comment about writing every algorithm as big o log n, it\u2019s purely hypothetical to allow for me to get a better understanding of log n notation. Obviously you would never be able to do that. I\u2019m just trying to understand.\n\nDon\u2019t know why people in this field get so pressed over seemingly small things", "Score": 7, "Replies": [{"Reply": "Probably projecting their imposter symdrome, lol.", "Reply Score": 1}]}, {"Comment": "1. Binary search is O(log n), not O(n log n).\n2. A hash lookup does not operate on sorted data. It operates on hashed (unsorted) data.\n3. Hash tables do not have \"true\" / guaranteed constant time lookup. They only have expected constant time, worst case linear time lookup, depending on the number of hash collisions.\n4. Binary search can provide you with a partition of the sorted data into items less than and items greater than (or equal to) the searched element. A hash lookup can't do that.", "Score": 3, "Replies": []}, {"Comment": "Unlike MIT professor, my consulting fee is very cheap (free). Feel free to DM me any questions and I can answer and link you to some good resources. \n\nSource: Was a TA for an algorithms class at a worse school than MIT", "Score": 3, "Replies": [{"Reply": "Just seeing this post, what a hilarious response by that other guy \ud83d\ude02especially for something as basic as searching in a list", "Reply Score": 3}]}, {"Comment": "I suspect you couldn't afford my consulting fee.  I'm happy to answer specific questions here in my free time, but for actual work, I am expensive.", "Score": 2, "Replies": []}, {"Comment": "Yes, but this is a different abstraction level.", "Score": 10, "Replies": []}, {"Comment": "That's the point of big O notation though. You don't care so much about the concrete number of operations required at a certain input size. It is rather how the number of operations grows in relation to the input size.", "Score": 4, "Replies": []}, {"Comment": "It really depends on how things are laid out in memory.  If you can set the solution up in such a way that the binary chop guarantees a cache miss, and the scan guarantees only cache hits, then you can scan roughly 200-500 objects before the memory read will come back.  A lot of the performance fuzziness comes from not really having enough control to guarantee things like that.  If your sorted data is in a flat array, and you know the # of items in the array, jumping around to different indexes isn't necessarily slower than reading items contiguously.\n\nLikewise, if your array contains pointers to objects, and the data you are filtering on is something contained within the object, every scan is going to be a memory access anyways, and there will basically be no performance penalty to binary search.", "Score": 3, "Replies": [{"Reply": "Yes of course it depends. Equally if you have just create your list, chances are it\u2019ll still be in the cache so finding items is likely to be quick. If you\u2019ve just done a search, the second search is probably fast too. \n\nThe point I was trying to make is that if your problem is small, you don\u2019t necessarily need a fast algorithm with a low O. \n\nFollowing on from that, if you can apply a decent algorithm at the big end, and a dumb-but-quick at the small end, you have the best of both worlds. \n\nIt\u2019s why quickset implementations can go faster if you use insertion sort when the section of the list to be sorted is small. Again, when I last tested it, insertion sort was quicker on lists of size 10 or so.  It also mitigates the worse-case for quick sort when the list is almost in order. \n\nAlthough I suspect the size of the objects to be sorted probably makes quite a difference too.\n\nSource: the Oxford tutor of C.A.R. Hoare  , one Patrick Shackleton, who bet him that he hadn\u2019t invented a quicker sort was my mentor in my first job. RIP Pat.", "Reply Score": 1}]}, {"Comment": "It's not even about searching, its about the whole asymptotic notation concept", "Score": 1, "Replies": []}, {"Comment": "Advice #1: Don't pre-optimize -- this is a common mistake. Don't spend too much extra time worrying about writing the most efficient code possible. Inefficient code that works is better than no working code. \n\nAdvice #2: Focus on actual bottle necks -- Once you have working code running in something resembling the real world, assuming you have an actual performance problem, focus on fixing the real bottle necks -- run a profiler, see where you're code is spending the most time, think about how you can reduce the time spent in that part of the algorithm (either by adjusting the coefficient -- making each iteration of the code block faster) or by adjusting the order (changing the higher level algorithm so that you call that block less frequently). \n\nAdvice #3: If and when you actually find yourself needing to address a performance issue, see if you can loosen the constraints. A lot of performance issues come from self imposed assumed constraints -- and likewise those order or magnitude type improvements come from recognizing and removing them from the bottle neck. Do you really need to sort the data to solve your problem? Do you even need to look at all of it? Not every problem can be solved this way, but many can. Think of most puzzle type problems -- they're difficult to solve because they rely on our cognitive biases that expect the world to behave a certain way. But if you know your cognitive biases, it's easier to take the blinders off and find the solution to the puzzle -- oh, why do I expect these pieces to go together at a right angle, silly me, if I use a 78 degree angle it will look ugly, but it will work. Those are the kind of self imposed constraints I'm taking about here -- nobody asked you to add them in, you just did it out of habit. \n\nAdvice #4: understand the difference between speed and scalability (or coefficients and order). Most people seem to think that an \"order of magnitude\" improvement means a decimal place. That may be a big improvement, but it's still just a change to a coefficient. Order of magnitude improvements really mean you've changed the way an algorithm scales (and you had a problem with scale in the first place). \n\n```\ny = a*x^2 + b*x + c\n```\n\nIn the above equation, `a`, `b` and `c` are coefficients. X^2 is the order. If X is zero, C is the only coefficient that matters. If X is large, A is likely the coefficient that matters the most. If you know that X will be large, reducing A will have a bigger impact than reducing B or C, but reducing the Big-Oh order from X^2 to X or log(X) or 1 will likely have an even bigger impact. \n\nBig-Oh notation is simply a quick way of communicating this with people who understand the point. A lot of people don't understand the point though, so using shorthand notation to communicate something that nobody in the room understands just makes for a lot of buzzword soup.\n\nAdvice #5: Don't worry about figuring out exactly what point N becomes a problem. Either you have a problem with Big N (usually because you expect it to grow indefinitely), or you don't have a problem with Big N. If you have a problem, it will probably be pretty obvious (once you're in production that is). If you have a performance problem, but it isn't related to a \"big N\", focus on coefficients. If it is related to a \"big N\" then spend a little more time thinking about how you can re-architect things to scale better.\n\nAdvice #6: Big-Oh isn't just for algorithms. The world isn't only made up of code. You're system may work fine for a hundred users, but struggle with a million. Maybe you can fix it with some code adjustments, but maybe the bottlenecks are in the real world. Strategies that work at small scale don't always work at larger scale. When I was at that same Big Data consultancy, I worked with a lot of people who were used to working at small startups. They couldn't understand why some of our large \"enterprise\" sized customers were so... slow. I thought it was funny that my colleagues who specialized in \"Big Data\" and \"Scalability\" didn't seem to realize that the same concepts applied to scaling up a company's size. The strategies that work at small scale (hiring, organization, separation of responsibilities, redundancy, risk mitigation) don't always make sense at large scale. Large companies may have larger coefficients, but they don't hit a wall scaling up when you try to add a few more employees.", "Score": 1, "Replies": []}, {"Comment": "Probably projecting their imposter symdrome, lol.", "Score": 1, "Replies": []}, {"Comment": "Just seeing this post, what a hilarious response by that other guy \ud83d\ude02especially for something as basic as searching in a list", "Score": 3, "Replies": []}, {"Comment": "Yes of course it depends. Equally if you have just create your list, chances are it\u2019ll still be in the cache so finding items is likely to be quick. If you\u2019ve just done a search, the second search is probably fast too. \n\nThe point I was trying to make is that if your problem is small, you don\u2019t necessarily need a fast algorithm with a low O. \n\nFollowing on from that, if you can apply a decent algorithm at the big end, and a dumb-but-quick at the small end, you have the best of both worlds. \n\nIt\u2019s why quickset implementations can go faster if you use insertion sort when the section of the list to be sorted is small. Again, when I last tested it, insertion sort was quicker on lists of size 10 or so.  It also mitigates the worse-case for quick sort when the list is almost in order. \n\nAlthough I suspect the size of the objects to be sorted probably makes quite a difference too.\n\nSource: the Oxford tutor of C.A.R. Hoare  , one Patrick Shackleton, who bet him that he hadn\u2019t invented a quicker sort was my mentor in my first job. RIP Pat.", "Score": 1, "Replies": []}]},{"Title": "Uncomputable functions via undecidable table?", "Score": 7, "URL": "https://www.reddit.com/r/compsci/comments/1adeoth/uncomputable_functions_via_undecidable_table/", "CreatedAt": 1706479898.0, "Full Content": "Around the 24:30 mark of this [UC Davis lecture from 2011][0], Dan Gusfield sets up a diagonalization argument around a table *T* whose rows are computer programs. See also \u201cG\u00f6del for Goldilocks: A Rigorous, Streamlined Proof of (a variant of) G\u00f6del's First Incompleteness Theorem,\u201d [arXiv:1409.5944 \\[math.LO\\]][1] from 2014.\n\nHe set out to prove the existence of a non-computable function in\n\n*Q* = { *f* | *f* : \u2124+&nbsp;\u2192&nbsp;{0,1} }\n\nHe defines *A* \u2286 *Q* as the set of computable functions: \u201cgiven *any* positive\ninteger *x*, the program finishes in finite time and correctly spits out the value *f*(*x*).\u201d\n\nBack to table *T*. Its rows are computer programs that compute some function in *A* and columns their values at all \u2124+, but *f\u0305* defined as\n\n*f\u0305*(*i*) = 1 - *f\u1d62*(*i*)\n\nappears nowhere in *T*.\n\nCantor\u2019s diagonalization proofs deny the existence of the analogous *T*s for infinite sets or \u211d. However, Gusfield asserts\n\n> (Remember that lists *L* and *T* are only conceptual; we don\u2019t actually build them\u2014we only have to imagine them for the sake of the proof).\n\nImaginability is one thing; existence is another. I\u2019m stuck on the claim that a function is non-computable because it depends on a table that no oracle can furnish. No matter what *T* we get, we can construct its *f\u0305*.\n\nWhat am I failing to understand?\n\n[0]: https://podcasts.apple.com/us/podcast/theory-of-computation-fall-2011/id585297167?i=1000411462836\n[1]: https://arxiv.org/abs/1409.5944", "CntComments": 21, "Comments": [{"Comment": "It's not an implementation of the function, it's a specification. What, to you, is not meaningful about it?", "Score": 5, "Replies": [{"Reply": "To be a row in T, the program has to implement some function in A. This is an undecidable problem.", "Reply Score": 2}]}, {"Comment": "The link doesn't load a video for me, but this seems like a diagonalization argument where the definitions of the three elements (Q, an infinite set of functions, T, an infinite table of functions from Q, and f-bar, a function that belongs to Q) allow the prover to derive the statement that f-bar does not belong to T by showing that f-bar can't be f\\_i for any i, since f\\_i(i) != 1 - f\\_i(i) for any i.\n\nI'm not a good enough theoretician to give you a hook like \"Axiom of Choice\" or anything like that, so I'm not sure I can build the framework for you that shows that objects like T can be defined \"legally\". But, there's nothing forbidden about defining an object for which most questions about that object can't be answered, as long as the definition is not self-contradicting or in contradiction with the other statements.", "Score": 5, "Replies": [{"Reply": "In the lecture, he goes through the familiar enumeration of all strings in increasing order of length and filtering based on which are syntactically valid Python programs. This of course mirrors the formal approach based on Turing machine descriptions.\n\nWhat I don\u2019t follow is the leap from an enumeration of syntactically valid Python programs to an enumeration of such programs that implement some function in A. Choose your favorite weapon: halting problem, A_{TM}, or Rice\u2019s theorem.\n\nThe cited argument ties construction of T to the above enumeration order of programs. For an arbitrary program and input \u2014 any single cell in T \u2014 we can\u2019t guarantee that the program terminates and thus that the cell even has a value.\n\nOutside consulting an oracle, nothing about f' is remarkable. Why isn\u2019t the conclusion that T cannot exist?", "Reply Score": 2}, {"Reply": "I updated the question to include a link to an arXiv paper by the author.", "Reply Score": 2}]}, {"Comment": "For *any* program P the statement \"P halts on all inputs\" has a truth value (of either True or False). Similarly \"P is the nth program, lexicographically, that halts on all inputs\" also has a truth value.\n\nf-bar can be defined on input i as the truth value of 'OR\\_{over all P} (\"P is the ith program that halts on all inputs\" AND \"P outputs 0 on input i\")'\n\nIt's not a decidable function, sure. But why do you say such a definition is not meaningful?", "Score": 2, "Replies": [{"Reply": "I updated my question to use better wording. How does the ordering exist if for any proposed ordering in *T*, we can construct an *f\u0305* to spoil it?", "Reply Score": 1}]}, {"Comment": "To be a row in T, the program has to implement some function in A. This is an undecidable problem.", "Score": 2, "Replies": [{"Reply": "What his proof shows, essentially, is that Q has uncountably infinite cardinality. Since A has countably infinite cardinality, that means Q - A (the set of uncomputable functions) is non-empty. In fact, it shows that [almost all](https://en.m.wikipedia.org/wiki/Almost_all) functions in Q are uncomputable.\n\nEDIT: To put it another way, he's proving that f-bar (a valid element of Q) indeed cannot be in the table (and thus cannot be in A).", "Reply Score": 4}]}, {"Comment": "In the lecture, he goes through the familiar enumeration of all strings in increasing order of length and filtering based on which are syntactically valid Python programs. This of course mirrors the formal approach based on Turing machine descriptions.\n\nWhat I don\u2019t follow is the leap from an enumeration of syntactically valid Python programs to an enumeration of such programs that implement some function in A. Choose your favorite weapon: halting problem, A_{TM}, or Rice\u2019s theorem.\n\nThe cited argument ties construction of T to the above enumeration order of programs. For an arbitrary program and input \u2014 any single cell in T \u2014 we can\u2019t guarantee that the program terminates and thus that the cell even has a value.\n\nOutside consulting an oracle, nothing about f' is remarkable. Why isn\u2019t the conclusion that T cannot exist?", "Score": 2, "Replies": [{"Reply": "He isn't enumerating programs that implement some particular function in A. He's enumerating programs that implement *any* function in A.", "Reply Score": 3}]}, {"Comment": "I updated the question to include a link to an arXiv paper by the author.", "Score": 2, "Replies": []}, {"Comment": "I updated my question to use better wording. How does the ordering exist if for any proposed ordering in *T*, we can construct an *f\u0305* to spoil it?", "Score": 1, "Replies": [{"Reply": "It's proven by contradiction that f-bar doesn't belong to table T. The ordering of programs that compute functions in A exists regardless of the definition of any function in Q.\n\nf-bar doesn't spoil the ordering.\n\nUsing the arxiv paper you posted, where does your understanding no longer align with the author's argument on page 3 under the heading \"An ordering Exists\"? Can you point out a sentence or phrase that you think is faulty before the \"Function f-bar\" heading?", "Reply Score": 3}]}, {"Comment": "What his proof shows, essentially, is that Q has uncountably infinite cardinality. Since A has countably infinite cardinality, that means Q - A (the set of uncomputable functions) is non-empty. In fact, it shows that [almost all](https://en.m.wikipedia.org/wiki/Almost_all) functions in Q are uncomputable.\n\nEDIT: To put it another way, he's proving that f-bar (a valid element of Q) indeed cannot be in the table (and thus cannot be in A).", "Score": 4, "Replies": [{"Reply": "Thank you for your responses. I edited my question to take your thoughts into account. The key issue is\n\n> I\u2019m stuck on the claim that a function is non-computable because it depends on a table that no oracle can furnish.\n\nI have no problem with saying *T* doesn\u2019t exist, but the ordering in *T* is central to the argument that *f\u0305* is non-computable.", "Reply Score": 2}]}, {"Comment": "He isn't enumerating programs that implement some particular function in A. He's enumerating programs that implement *any* function in A.", "Score": 3, "Replies": [{"Reply": "I understand that. How does that avoid undecidability?", "Reply Score": 1}]}, {"Comment": "It's proven by contradiction that f-bar doesn't belong to table T. The ordering of programs that compute functions in A exists regardless of the definition of any function in Q.\n\nf-bar doesn't spoil the ordering.\n\nUsing the arxiv paper you posted, where does your understanding no longer align with the author's argument on page 3 under the heading \"An ordering Exists\"? Can you point out a sentence or phrase that you think is faulty before the \"Function f-bar\" heading?", "Score": 3, "Replies": [{"Reply": "Thanks. I was getting hung up on a habitual but incorrect assumption that *f\u0305* would spoil everything, but it won\u2019t in this case because it is not part of *T* by construction.", "Reply Score": 2}]}, {"Comment": "Thank you for your responses. I edited my question to take your thoughts into account. The key issue is\n\n> I\u2019m stuck on the claim that a function is non-computable because it depends on a table that no oracle can furnish.\n\nI have no problem with saying *T* doesn\u2019t exist, but the ordering in *T* is central to the argument that *f\u0305* is non-computable.", "Score": 2, "Replies": [{"Reply": "You mean the fact that T can be ordered? Well that is true for the set of all finite strings, and computer programs are a subset thereof. To order the infinite set of all finite strings:\n\n* Assign the first number to the empty string.\n* Assign the next numbers to the length-1 strings, in alphabetical order.\n* Assign the next numbers to the length-2 strings, in alphabetical order.\n* And so on.", "Reply Score": 2}]}, {"Comment": "I understand that. How does that avoid undecidability?", "Score": 1, "Replies": [{"Reply": "Uncomputability typically only shows up when you want to assert some property of the program's behavior. In this case, he's only asserting that the program is syntactically valid.\n\nEdit: rereading your original post, I think I see the confusion.\n\nI was thinking that the table was just a list of programs. But he's actually listing programs that always halt.\n\nYou are absolutely correct that this list is undecidable.\n\nHowever, that's fine! The proof only requires the list to exist in principle. Another way of thinking about this is that we're proving that an uncomputable function exists, but we're not actually constructing one.", "Reply Score": 6}]}, {"Comment": "Thanks. I was getting hung up on a habitual but incorrect assumption that *f\u0305* would spoil everything, but it won\u2019t in this case because it is not part of *T* by construction.", "Score": 2, "Replies": [{"Reply": "Awesome, it's a good question and I appreciate your commitment to growing your own understanding.", "Reply Score": 2}]}, {"Comment": "You mean the fact that T can be ordered? Well that is true for the set of all finite strings, and computer programs are a subset thereof. To order the infinite set of all finite strings:\n\n* Assign the first number to the empty string.\n* Assign the next numbers to the length-1 strings, in alphabetical order.\n* Assign the next numbers to the length-2 strings, in alphabetical order.\n* And so on.", "Score": 2, "Replies": [{"Reply": "I have no problem with the ordering of the programs from the enumerator, which is upstream of *T* in the proof.\n\nNo matter what *T* is, its ordering of computer programs that implement some *f*&nbsp;\u2208&nbsp;*A* is incorrect because we can always produce an *f\u0305* that should be present somewhere but is not.\n\nAha! I think you\u2019ve helped me make progress. We do not expect *A* to have a corresponding row for *f\u0305*.", "Reply Score": 1}]}, {"Comment": "Uncomputability typically only shows up when you want to assert some property of the program's behavior. In this case, he's only asserting that the program is syntactically valid.\n\nEdit: rereading your original post, I think I see the confusion.\n\nI was thinking that the table was just a list of programs. But he's actually listing programs that always halt.\n\nYou are absolutely correct that this list is undecidable.\n\nHowever, that's fine! The proof only requires the list to exist in principle. Another way of thinking about this is that we're proving that an uncomputable function exists, but we're not actually constructing one.", "Score": 6, "Replies": [{"Reply": "Diagonalization proofs that infinite sets and the reals are uncountable deny that the analogous lists exist at all. This argument parallels their structure but explicitly depends on the list *T* to exist in order to use its ordering.", "Reply Score": 1}]}, {"Comment": "Awesome, it's a good question and I appreciate your commitment to growing your own understanding.", "Score": 2, "Replies": []}, {"Comment": "I have no problem with the ordering of the programs from the enumerator, which is upstream of *T* in the proof.\n\nNo matter what *T* is, its ordering of computer programs that implement some *f*&nbsp;\u2208&nbsp;*A* is incorrect because we can always produce an *f\u0305* that should be present somewhere but is not.\n\nAha! I think you\u2019ve helped me make progress. We do not expect *A* to have a corresponding row for *f\u0305*.", "Score": 1, "Replies": [{"Reply": "Indeed, it's a proof by contradiction. **If** all functions in Q were computable, then we could put the programs that compute them in some countably infinite table T. But we can construct an f\u0305 \u2208 Q whose program is demonstrably not in the table. Contradiction! Hence, we must conclude that not all functions in Q are computable. (And as I said above, it's actually much worse: almost none of the functions in Q are computable.)", "Reply Score": 2}]}, {"Comment": "Diagonalization proofs that infinite sets and the reals are uncountable deny that the analogous lists exist at all. This argument parallels their structure but explicitly depends on the list *T* to exist in order to use its ordering.", "Score": 1, "Replies": []}, {"Comment": "Indeed, it's a proof by contradiction. **If** all functions in Q were computable, then we could put the programs that compute them in some countably infinite table T. But we can construct an f\u0305 \u2208 Q whose program is demonstrably not in the table. Contradiction! Hence, we must conclude that not all functions in Q are computable. (And as I said above, it's actually much worse: almost none of the functions in Q are computable.)", "Score": 2, "Replies": [{"Reply": "Right, *A* has no corresponding row for *f\u0305* because the author explicitly defined it to be outside *A*, *i.e.*, non-computable. So the existence of an *f\u0305* doesn\u2019t spoil an ordering in *T* for TMs that compute some function in *A*.", "Reply Score": 1}]}, {"Comment": "Right, *A* has no corresponding row for *f\u0305* because the author explicitly defined it to be outside *A*, *i.e.*, non-computable. So the existence of an *f\u0305* doesn\u2019t spoil an ordering in *T* for TMs that compute some function in *A*.", "Score": 1, "Replies": []}]},{"Title": "Deep Reinforcement Learning", "Score": 8, "URL": "https://www.reddit.com/r/compsci/comments/1ad0y93/deep_reinforcement_learning/", "CreatedAt": 1706442408.0, "Full Content": "A Survey Analyzing Generalization in Deep Reinforcement Learning \n\n [https://github.com/EzgiKorkmaz/generalization-reinforcement-learning](https://github.com/EzgiKorkmaz/generalization-reinforcement-learning) ", "CntComments": 0, "Comments": []},{"Title": "Security and Privacy Failures in Popular 2FA Apps -- \"We identified all general purpose Android TOTP apps in the Google Play Store with at least 100k installs that implemented a backup mechanism (n = 22).\"", "Score": 7, "URL": "https://www.usenix.org/conference/usenixsecurity23/presentation/gilsenan", "CreatedAt": 1706438042.0, "Full Content": "", "CntComments": 2, "Comments": [{"Comment": "The submitted link is from \"Interesting Links\" in https://old.reddit.com/r/termux/comments/19573gg/encryption_decryption_android_11_operating_system/ (\"Encryption, Decryption, Android 11 Operating System, Termux, And proot-distro Using Alpine Linux minirootfs: cryptsetup v2.6.1 And LUKS\").", "Score": 1, "Replies": []}]},{"Title": "Does multiplying with zero affect runtime Big O?", "Score": 0, "URL": "https://www.reddit.com/r/compsci/comments/1acu5k1/does_multiplying_with_zero_affect_runtime_big_o/", "CreatedAt": 1706416010.0, "Full Content": "ELI5 very new to this topic. If I'm multiplying two vectors (for regression), and there are n elements in them but in one vector, there are m non-zero elements (m<=n). Would my Big O of this operation be O(m) or O(n)?", "CntComments": 24, "Comments": [{"Comment": "Typically, multiplication of any two numbers is treated as a constant-time operation. That's because most computers use finite-sized numerical data types with specialized circuits that do multiply those numbers all in one go. Confusingly, the asymptotic analysis of algorithms often simultaneously assumes that numbers can be arbitrary large. In practice, unless your computer is somehow infinitely parallelized, you can't actually have arbitrarily large numbers *and* constant-time multiplication.\n\nIn a computer with constant-time multiplication, there's nothing to be gained because multiplying by 0 is no faster than any other multiplication.\n\nIn a computer with arbitrarily large numbers but constant-time addressible memory access, multiplication by operating digit-by-digit becomes an P\\*Q-time operation where P and Q are the number of digits in the two operands (and thus scale by the logarithms of the actual sizes of the operands). In that case, any *one* multiplication by zero saves time insofar as it occurs in constant time (you can just output 0 regardless of the value of the other operand). In the case of your N-element vectors where one has M nonzero elements, you could save some time with this, although the amount you save might vanish into insignificance if the other elements are large enough numbers relative to the number of elements in the vectors.\n\nIn either case, if you anticipate having very large vectors with very few nonzero elements, there might be some tricks you can use to store and manipulate those vectors that could save on time and memory. Those tricks wouldn't generalize well, but they might be effective in the scenarios you're actually dealing with.", "Score": 7, "Replies": []}, {"Comment": "It depends on how the vector is stored. If it is length n and you literally just have some zeroes in it, then that doesn\u2019t help you at all because you have to touch each spot to recognize that it is zero or not so it is still O(n). \n\n\nHowever, if the vector is stored as a list of tuples, (index, value), with the zero values omitted then you can do it in O(m) because each entry is guaranteed to be a nonzero value and there are m of them. Note: this assumes you have already allocated the output vector and it has zeroes in every spot, or that you have a constant time allocating function that zeroes out the memory.", "Score": 3, "Replies": []}, {"Comment": "Multiplying by 0 can be computed in constant time: it's 0. That takes the same amount of time regardless of the other input, because you aren't looking at it. So here you're looking at O(m) runtime, but only if you use an algorithm that's optimized to do this zero checking. (I assume your question is just theoretical, but this is why sparse matrix libraries exist: both from a memory and a computational point of view, it quickly becomes worth it to build in special-cases for this kinda thing.)\n\nEDIT: I realize I'm fudging a bit here. Your question seems to be talking about multiplications? We can operate under the model that computing ab for any two numbers a and b is so much more work than just writing 0 that the extra 0 multiplications don't actually count for runtime purposes. If you're specifically interested in multiplications, unless you define things using sparse matrices, you're going to technically be multiplying O(n) times, but n - m of those will be much cheaper than the others.", "Score": 3, "Replies": [{"Reply": "I think this a bit of a confusing answer. Multiplication, regardless of its with 0 or not, is a O(1) operation.  There is correct answer is O(N) and nothing less. the other operations wouldn't actually be noticeably cheaper\n\nSparse matrix would apply only if they were multiply two vectors to get a matrix but I believe they are implying elementwise.", "Reply Score": 7}]}, {"Comment": "You need to clarify what kind of vector multiplication you're doing: dot product, cross product, tensor product, etc?", "Score": 1, "Replies": [{"Reply": "It looks like OP wants element-wise multiplication.", "Reply Score": 0}]}, {"Comment": "for purposes of big o, o(m) and o(n) are equivalent since big o cares about computation as the size goes towards larger numbers. unless there is a nonlinear relationship between m and n (like m = log(n)) that emerges from the constraint m <= n, the constraint is not relevant for big o.", "Score": 0, "Replies": []}, {"Comment": "It depends on your algorithm and your data structure.\n\nIf your vectors are small, say, 4 elements, you would store them as a 4-element array and just do all the multiplications. A single multiplication is one operation as far as most CPUs are concerned. But since 4 is a constant, this would be O(1) overall.\n\nHowever, you could imagine working with sparse vectors that only store the nonzero elements. In that case, you would probably need to iterate over roughly all the nonzero elements from both vectors. In that case you would probably have O(n + m).", "Score": 1, "Replies": []}, {"Comment": "You generally can\u2019t treat multiplying by zero as special when analyzing an algorithm because testing for zero is generally more expensive than multiplication. In fact, when evaluating the time complexity of algorithms, we usually count branches and ignore arithmetic operations. You could use a sparse vector representation that records indices and values, and considers any omitted indices to have value zero, but then you have to compare indices to align values when calculating things. For very sparse vectors, there is a big advantage to the sparse representation, but for moderate sparsity, the dense representation wins because it only has one branch per iteration, and that branch is very predictable.", "Score": 1, "Replies": []}, {"Comment": "Multiplying a vector of length m with another of length n would give you O(m * n) if I'm not mistaken. And I'm not sure I understand the question, found it confusing.\n\nN.b: I'm thinking you might simply not care about the size difference for big O notation and just express it as O(n^2) (or O(m^2)).\n\nEDIT: oh, I'm realizing OP probably means he's multiply elements at each index of array A with each elements of corresponding index in array B (rather than dot product, etc.). So it'd be O(n)...", "Score": 1, "Replies": []}, {"Comment": "I don't quite understand the question, but it seems like this can only be a theoretical question.\n\n\nString operations (immutable types, which have to be copied to the stack) are really expensive when compared to floating point ops.\n\n\nWe're also measuring desktop pc's at gigaflops. Does it really matter, except as a thought experiment?", "Score": 1, "Replies": []}, {"Comment": "A lot of posts are saying O(n) for roughly the same reason. I've got a concurring opinion: *even if we're just counting multiplications*, the worst case O(n) because in the worst case, m=n. Knowing m ahead of time when m<=n isn't giving us novel information: we could figure that out on-line. Even knowing m << n might not give any benefits. \n\nEven if we want to avoid multiplications by 0, there's a lower level concern. On many machines, a multiplication by 0 (FP or Integer) is going to be much cheaper than doing a conditional jump. While the cycle count may be cheaper, the real killer is the reduction in throughput caused by a branch misprediction flushing the instruction pipeline.\n\nAre there machines or data types where this doesn't hold? Yes. E.g. I've had to program vector operations for a bespoke machine with finite field arithmetic instructions and no pipelining where the cost of a multiplication is more than the cost of a conditional jump. Other answers point out arbitrary-sized integers that need software routines. \n\nNow, if we're given something like m is roughly n/2, AND a cheap way of avoiding the indices with 0 elements ahead of time, then we can start seeing appreciable results. It's still technically O(n) in multiplications, but cutting the arithmetic cost in half isn't something to ignore, especially if you're doing things at lightspeed.", "Score": 1, "Replies": []}, {"Comment": "If (by some unspecific trickery) you _only_ do work on the m non-zeros (i.e. absolutely _no_ work, not _anything_!!) with the rest (n-m) ones: Then yes.\n\nNote that even _determine_ whether one element is or is not zero is \"work\".", "Score": 1, "Replies": []}, {"Comment": "I think this a bit of a confusing answer. Multiplication, regardless of its with 0 or not, is a O(1) operation.  There is correct answer is O(N) and nothing less. the other operations wouldn't actually be noticeably cheaper\n\nSparse matrix would apply only if they were multiply two vectors to get a matrix but I believe they are implying elementwise.", "Score": 7, "Replies": [{"Reply": "Just some further pedanticism:\nMultiplication on fixed-length elements, such as b-bit floats (which I presume is what it is in OP's context), is an O(1) operation essentially by definition. On general integers (or pretty much any variable-length numbers), it is not constant.", "Reply Score": 1}, {"Reply": "A sparse implementation of a dot product can avoid doing the multiplications with 0.\n\nIf I'm multiplying a dense vector by a sparse vector where only elements 1, 2, and 5 are nonzero, then I only need to do those multiplications in order to get the result. As m << n, this does become noticeably faster than standard multiplication. If you imagine the extreme case, where we have 100,000 elements in the other vector and only 1, 2, and 5 nonzero in the sparse one, I'd consider it quite misleading to say that this multiplication would take 100,000 multiplications.", "Reply Score": 1}]}, {"Comment": "It looks like OP wants element-wise multiplication.", "Score": 0, "Replies": [{"Reply": "Then merely knowing that some elements are zero isn't enough to change anything. Iterating through the vector is O(n). Multiplication of two integers, constant time. If one or both of those integers are zero, that's still constant time. You need to iterate through the vectors to get the pairs of numbers that you're multiplying.", "Reply Score": 2}]}, {"Comment": "Just some further pedanticism:\nMultiplication on fixed-length elements, such as b-bit floats (which I presume is what it is in OP's context), is an O(1) operation essentially by definition. On general integers (or pretty much any variable-length numbers), it is not constant.", "Score": 1, "Replies": []}, {"Comment": "A sparse implementation of a dot product can avoid doing the multiplications with 0.\n\nIf I'm multiplying a dense vector by a sparse vector where only elements 1, 2, and 5 are nonzero, then I only need to do those multiplications in order to get the result. As m << n, this does become noticeably faster than standard multiplication. If you imagine the extreme case, where we have 100,000 elements in the other vector and only 1, 2, and 5 nonzero in the sparse one, I'd consider it quite misleading to say that this multiplication would take 100,000 multiplications.", "Score": 1, "Replies": [{"Reply": "Respectfully, What you're saying is wrong and conveys a clear lack of understanding of big O notation.It would take 100,000 checks to verify which elements are/aren't 0, which is the same thing. O(n)", "Reply Score": 0}]}, {"Comment": "Then merely knowing that some elements are zero isn't enough to change anything. Iterating through the vector is O(n). Multiplication of two integers, constant time. If one or both of those integers are zero, that's still constant time. You need to iterate through the vectors to get the pairs of numbers that you're multiplying.", "Score": 2, "Replies": [{"Reply": "In general, yes. If OPs just counting multiplications, then technically they can get away with m multiplications, but on a lot of machines it's not worth upsetting the branch predictor to do so.", "Reply Score": 0}]}, {"Comment": "Respectfully, What you're saying is wrong and conveys a clear lack of understanding of big O notation.It would take 100,000 checks to verify which elements are/aren't 0, which is the same thing. O(n)", "Score": 0, "Replies": [{"Reply": "Sparse vectors store the nonzero elements only, so you don't have to make the checks. It depends on the data structure you use, but in a sparse vector setup this is definitely O(m).", "Reply Score": 1}]}, {"Comment": "In general, yes. If OPs just counting multiplications, then technically they can get away with m multiplications, but on a lot of machines it's not worth upsetting the branch predictor to do so.", "Score": 0, "Replies": [{"Reply": "I don't think this holds up to scrutiny and it wasn't what OP was asking, either. You're saying you can get away with \"not multiplying\" if one of the operands is 0. How do you propose to get a result? You need to multiply. If you want to roll your own logic for this as a special case, you're still doing multiplication.\n\nThe question about whether it's more efficient to use a hardware implementation or roll your own logic doesn't have any bearing on whether you're doing multiplication or not, and I think you can take it on faith that for most computer architectures, the hardware implementation is going to be more efficient than trying to short circuit the multiply operation with logic tests on the operands.\n\nAnd none of that matters for time complexity since the multiply and the logic test would both be constant time operations.", "Reply Score": 0}]}, {"Comment": "Sparse vectors store the nonzero elements only, so you don't have to make the checks. It depends on the data structure you use, but in a sparse vector setup this is definitely O(m).", "Score": 1, "Replies": []}, {"Comment": "I don't think this holds up to scrutiny and it wasn't what OP was asking, either. You're saying you can get away with \"not multiplying\" if one of the operands is 0. How do you propose to get a result? You need to multiply. If you want to roll your own logic for this as a special case, you're still doing multiplication.\n\nThe question about whether it's more efficient to use a hardware implementation or roll your own logic doesn't have any bearing on whether you're doing multiplication or not, and I think you can take it on faith that for most computer architectures, the hardware implementation is going to be more efficient than trying to short circuit the multiply operation with logic tests on the operands.\n\nAnd none of that matters for time complexity since the multiply and the logic test would both be constant time operations.", "Score": 0, "Replies": [{"Reply": "> I don't think this holds up to scrutiny and it wasn't what OP was asking, either.\n\nCounting multiplications in a high performance setting (regression with a large number of elements qualifies) isn't uncommon. Given OP is reasoning about multiplications, it's not unlikely that they care about the number of multiplications.\n\n>You're saying you can get away with \"not multiplying\" if one of the operands is 0. How do you propose to get a result? You need to multiply. If you want to roll your own logic for this as a special case, you're still doing multiplication.\n\nOkay? There's still a difference in cycle counts between different machine instructions. Moreover, software numeric types like arbitrary sized integers might have much higher cost in cycle counts for multiplication than a 0 check.\n\n>The question about whether it's more efficient to use a hardware implementation or roll your own logic doesn't have any bearing on whether you're doing multiplication or not, and I think you can take it on faith that for most computer architectures, the hardware implementation is going to be more efficient than trying to short circuit the multiply operation with logic tests on the operands.\n\nI'm pretty sure I addressed this by saying it's generally not worth it, but I just made a top level comment saying so if it wasn't clear. However, I've worked on machines and with data types where this isn't true.\n\n>And none of that matters for time complexity since the multiply and the logic test would both be constant time operations.\n\nThe problem here is that big O is optimistic. If OP is in a setting where multiplication is expensive, avoiding it may cut the scale factor of n (in cycle counts). Cutting down the scale factor on the highest order term (when said term is actually dominating) by some factor is often appreciable, especially if the algorithm is a subroutine that might be used a non-constant or even non-linear number of times and even more so if the algorithm is compute-bound.", "Reply Score": 0}]}, {"Comment": "> I don't think this holds up to scrutiny and it wasn't what OP was asking, either.\n\nCounting multiplications in a high performance setting (regression with a large number of elements qualifies) isn't uncommon. Given OP is reasoning about multiplications, it's not unlikely that they care about the number of multiplications.\n\n>You're saying you can get away with \"not multiplying\" if one of the operands is 0. How do you propose to get a result? You need to multiply. If you want to roll your own logic for this as a special case, you're still doing multiplication.\n\nOkay? There's still a difference in cycle counts between different machine instructions. Moreover, software numeric types like arbitrary sized integers might have much higher cost in cycle counts for multiplication than a 0 check.\n\n>The question about whether it's more efficient to use a hardware implementation or roll your own logic doesn't have any bearing on whether you're doing multiplication or not, and I think you can take it on faith that for most computer architectures, the hardware implementation is going to be more efficient than trying to short circuit the multiply operation with logic tests on the operands.\n\nI'm pretty sure I addressed this by saying it's generally not worth it, but I just made a top level comment saying so if it wasn't clear. However, I've worked on machines and with data types where this isn't true.\n\n>And none of that matters for time complexity since the multiply and the logic test would both be constant time operations.\n\nThe problem here is that big O is optimistic. If OP is in a setting where multiplication is expensive, avoiding it may cut the scale factor of n (in cycle counts). Cutting down the scale factor on the highest order term (when said term is actually dominating) by some factor is often appreciable, especially if the algorithm is a subroutine that might be used a non-constant or even non-linear number of times and even more so if the algorithm is compute-bound.", "Score": 0, "Replies": [{"Reply": "I have to say that I really disagree with taking the discussion down the rabbit hole of scale factors and how algorithmic time complexity doesn't cover all possible interests. This is a compsci sub, and per OP's problem statement, we're given a very straightforward ELI5 question about time complexity. We're not offered any implementation details about machines or engineering goals. We don't have any context that justifies speaking to any of this.", "Reply Score": 0}]}, {"Comment": "I have to say that I really disagree with taking the discussion down the rabbit hole of scale factors and how algorithmic time complexity doesn't cover all possible interests. This is a compsci sub, and per OP's problem statement, we're given a very straightforward ELI5 question about time complexity. We're not offered any implementation details about machines or engineering goals. We don't have any context that justifies speaking to any of this.", "Score": 0, "Replies": [{"Reply": "You're free to disagree, but the information might be relevant to HPC-minded people.\n\nEdit: and OP did mention regression as the use case, so performance beyond time complexity may be a valid concern.", "Reply Score": 0}]}, {"Comment": "You're free to disagree, but the information might be relevant to HPC-minded people.\n\nEdit: and OP did mention regression as the use case, so performance beyond time complexity may be a valid concern.", "Score": 0, "Replies": []}]},{"Title": "Suggestions to tame complexity in hierarchical state machine?", "Score": 10, "URL": "https://www.reddit.com/r/compsci/comments/1abv6t0/suggestions_to_tame_complexity_in_hierarchical/", "CreatedAt": 1706308884.0, "Full Content": "I'm working with a hierarchical state machine which seems very large to me \u2014 15 leaf nodes, and 5 parent nodes (example of hierarchy included below). The number of states makes it difficult to understand all the interactions, and putting all the state transitions into a single diagram is a nightmare. I suspect that the state machine could be simplified, possibly through:  \n\n\n1. reducing the number of states.\n2. restricting the kinds of transitions that are allowed.\n\n&#x200B;\n\nhttps://preview.redd.it/n7vpf2ja4vec1.png?width=1762&format=png&auto=webp&s=e2e2fb4b426ebafdc36d41ce68348ac4139676b6\n\nI'm hoping for advice on how to do either of these, or more suggestions for how to tame the complexity in the state machine.  \nI've searched for suggestions regarding ideas (1) and (2) a bit already. I expected there would be some heuristic to determine when states should be combined to reduce the number of states, but I haven't found anything about this. I also haven't found definitive agreement on idea (2) \u2014 I think I'd prefer if leaf nodes only only handle transitions to direct siblings, and anything more complex needs to be handled through an ancestor state, like what is described here ([https://stackoverflow.com/questions/50182913/what-are-the-principles-involved-for-an-hierarchical-state-machine-and-how-to-i](https://stackoverflow.com/questions/50182913/what-are-the-principles-involved-for-an-hierarchical-state-machine-and-how-to-i)). But many of the guides describing hierarchical state machines violate this kind of restriction in their examples.  \n ", "CntComments": 9, "Comments": [{"Comment": "Behavior Trees?", "Score": 3, "Replies": [{"Reply": "Thanks for the suggestion! I need to learn more about this, it's my first time hearing of behavior trees", "Reply Score": 1}]}, {"Comment": "Why are you representing your hierarchy as a tree instead of nested blocks, as is the norm? I would think that visualizing your HSM properly would likely help immensely in understanding your state transitions, without even addressing whether there is unnecessary complexity.\n\nRemember, the entire benefit of HSM is that you can take advantage of inherited behavior. If your state machine is correct, and shared behavior is applied at the highest appropriate parent state, then all your complexity is necessary to meet your requirements.\n\nThis is by far not the most complicated HSM I have ever seen. Only you can tell us if the complexity is necessary or not.", "Score": 2, "Replies": [{"Reply": "I find the nested blocks to be more confusing because of the rather deep nesting, but your point is well taken -- I should be representing it with the familiar format.\n\nI think your comment about behavior being applied at the highest appropriate parent state is very apt. The parent states capture very little of the behavior of their children right now, and approx 30 of 40 total connections are between leaf nodes. Thanks for the feedback", "Reply Score": 1}]}, {"Comment": "Maybe model it as an NDFA and convert it to a DFA.", "Score": 1, "Replies": []}, {"Comment": "Thanks for the suggestion! I need to learn more about this, it's my first time hearing of behavior trees", "Score": 1, "Replies": [{"Reply": "Material is pretty scattered, but this implementation has a good general explanation: [https://www.behaviortree.dev/](https://www.behaviortree.dev/)\n\nI was working on a project with a state machine for high level fetch tasks, which in itself contained submachines for lower level joint movements. This quickly turned into hell because we had multiple nesting layers, and each layers had its own fault handling and conditionals.\n\nIt took a while for the dev lead to convince me to move everything to behavior trees, but after I did, I became a believer. I had to stop thinking in terms of states and instead see behaviors as subtrees, black boxes that can be moved, entered, and caught.", "Reply Score": 1}]}, {"Comment": "I find the nested blocks to be more confusing because of the rather deep nesting, but your point is well taken -- I should be representing it with the familiar format.\n\nI think your comment about behavior being applied at the highest appropriate parent state is very apt. The parent states capture very little of the behavior of their children right now, and approx 30 of 40 total connections are between leaf nodes. Thanks for the feedback", "Score": 1, "Replies": [{"Reply": "You're welcome. Another way to think about things that came to mind after re-reading your post, is that if all your transitions are between leaf nodes, then perhaps your parent states aren't modeled ideally.\n\nAre your parent states modeling behaviors? Or are they trying to model something more abstract about your problem? I've found that when things don't make sense to me anymore that maybe I've got the wrong top level abstraction in my state machine. For example, if it's a physical device, usually you want things like \"On\" and \"Off\" at the top, because there are clear signals that transition between those states, and there might be special low-power processing that only occurs in the \"Off\" state.\n\nThere's also benefits to recognizing when a system should be split out into two or more independent, parallel HSMs, if they share no state and only send signals to each other. I've broken out certain input modules into a separate HSM before, when only sequences required their own state independent of the main system behavior.\n\nBut, those are dependent on your problem and abstraction. If this is not a work related problem, please provide some more detail as to what your states represent, and perhaps we can offer more specific suggestions.\n\nGood day!", "Reply Score": 1}]}, {"Comment": "Material is pretty scattered, but this implementation has a good general explanation: [https://www.behaviortree.dev/](https://www.behaviortree.dev/)\n\nI was working on a project with a state machine for high level fetch tasks, which in itself contained submachines for lower level joint movements. This quickly turned into hell because we had multiple nesting layers, and each layers had its own fault handling and conditionals.\n\nIt took a while for the dev lead to convince me to move everything to behavior trees, but after I did, I became a believer. I had to stop thinking in terms of states and instead see behaviors as subtrees, black boxes that can be moved, entered, and caught.", "Score": 1, "Replies": []}, {"Comment": "You're welcome. Another way to think about things that came to mind after re-reading your post, is that if all your transitions are between leaf nodes, then perhaps your parent states aren't modeled ideally.\n\nAre your parent states modeling behaviors? Or are they trying to model something more abstract about your problem? I've found that when things don't make sense to me anymore that maybe I've got the wrong top level abstraction in my state machine. For example, if it's a physical device, usually you want things like \"On\" and \"Off\" at the top, because there are clear signals that transition between those states, and there might be special low-power processing that only occurs in the \"Off\" state.\n\nThere's also benefits to recognizing when a system should be split out into two or more independent, parallel HSMs, if they share no state and only send signals to each other. I've broken out certain input modules into a separate HSM before, when only sequences required their own state independent of the main system behavior.\n\nBut, those are dependent on your problem and abstraction. If this is not a work related problem, please provide some more detail as to what your states represent, and perhaps we can offer more specific suggestions.\n\nGood day!", "Score": 1, "Replies": [{"Reply": "Ya, I think the parent states are probably not very good choices. Unfortunately I'm pretty new to the system, so it's not immediately obvious to me what better choices would be, haha. A parallel HSM is a good suggestion as well, I'll ask my teammates about it. Unfortunately it is related to work, otherwise I would love to share more!\n\nGood day to you too!", "Reply Score": 1}]}, {"Comment": "Ya, I think the parent states are probably not very good choices. Unfortunately I'm pretty new to the system, so it's not immediately obvious to me what better choices would be, haha. A parallel HSM is a good suggestion as well, I'll ask my teammates about it. Unfortunately it is related to work, otherwise I would love to share more!\n\nGood day to you too!", "Score": 1, "Replies": []}]},{"Title": "Towards modern development of cloud applications", "Score": 0, "URL": "https://engineeringatscale.substack.com/p/towards-modern-development-of-cloud", "CreatedAt": 1706291480.0, "Full Content": "", "CntComments": 5, "Comments": [{"Comment": "I'm a little confused how this works in principle and would appreciate further reading, or an explanation.\n\nIf all components are part of the same monolith code base, how are only certain components deployed to separate machines, and how does each component know if something would be a method call or be piped to a different machine via a request?", "Score": 1, "Replies": [{"Reply": "This is handled by the run-time. The lower-level details are abstracted from the developer. \n\nThe decision to keep two components on the same machine is taken based on whether the applications are chatty or have higher network call latency. \n\nThe paper doesn't cover these details but we can reverse engineer these things by reading the code.", "Reply Score": 0}]}, {"Comment": "This is handled by the run-time. The lower-level details are abstracted from the developer. \n\nThe decision to keep two components on the same machine is taken based on whether the applications are chatty or have higher network call latency. \n\nThe paper doesn't cover these details but we can reverse engineer these things by reading the code.", "Score": 0, "Replies": [{"Reply": "So they have some framework that introspects the code and decides whether a component belongs coupled with others on a single machine or on a separate machine?\n\nOr do the developers decide what goes where?\n\nMy experience with deploying code to production is that a given repository of code is deployed to a given machine.\n\nI guess I'm not grokking what run-time means in this. I'd like to more fully understand this concept. Do you have some suggested reading that could maybe bring my background knowledge up to speed?", "Reply Score": 1}]}, {"Comment": "So they have some framework that introspects the code and decides whether a component belongs coupled with others on a single machine or on a separate machine?\n\nOr do the developers decide what goes where?\n\nMy experience with deploying code to production is that a given repository of code is deployed to a given machine.\n\nI guess I'm not grokking what run-time means in this. I'd like to more fully understand this concept. Do you have some suggested reading that could maybe bring my background knowledge up to speed?", "Score": 1, "Replies": [{"Reply": "No, in this case, the developer isn't aware of the lower level details of where the code is getting deployed. The framework proposed by the paper tries to overcome the fundamental problem where we mix how we write code with how we deploy it. \n\nRuntime is nothing but the software that orchestrates the deployment of your code. You can go through the code to understand the concept at a deeper level.", "Reply Score": 1}]}, {"Comment": "No, in this case, the developer isn't aware of the lower level details of where the code is getting deployed. The framework proposed by the paper tries to overcome the fundamental problem where we mix how we write code with how we deploy it. \n\nRuntime is nothing but the software that orchestrates the deployment of your code. You can go through the code to understand the concept at a deeper level.", "Score": 1, "Replies": [{"Reply": "Thank you. This is making more sense now.", "Reply Score": 2}]}, {"Comment": "Thank you. This is making more sense now.", "Score": 2, "Replies": []}]},{"Title": "Understanding Mesh Allocator", "Score": 9, "URL": "https://veera.app/mesh_allocator.html", "CreatedAt": 1706191227.0, "Full Content": "", "CntComments": 2, "Comments": [{"Comment": "I like the idea, but what is the mechanism to update the page table from userspace?\n\nFrom TFP:\n\nMesh updates the process\u2019s page tables via calls to mmap. We exploit the fact that mmap lets the same offset in a file (corresponding to a physical span) be mapped to multiple addresses. Mesh\u2019s arena, rather than being an anonymous mapping, as in traditional malloc implementations, is instead a shared mapping backed by a temporary file. This temporary file is obtained via the memfd_create system call and only exists in memory or on swap.", "Score": 1, "Replies": []}, {"Comment": "I am a bit confused (or ignorant), I thought this was the kernel's job and it already takes care of this?", "Score": 1, "Replies": []}]},{"Title": "Framing Frames: Bypassing Wi-Fi Encryption by Manipulating Transmit Queues", "Score": 8, "URL": "https://www.usenix.org/conference/usenixsecurity23/presentation/schepers", "CreatedAt": 1706180116.0, "Full Content": "", "CntComments": 1, "Comments": [{"Comment": "The submitted link is from \"Interesting Links\" in https://old.reddit.com/r/termux/comments/19573gg/encryption_decryption_android_11_operating_system/ (\"Encryption, Decryption, Android 11 Operating System, Termux, And proot-distro Using Alpine Linux minirootfs: cryptsetup v2.6.1 And LUKS\").", "Score": 1, "Replies": []}]},{"Title": "Random Graph Generation Algorithm", "Score": 10, "URL": "https://www.reddit.com/r/compsci/comments/19edpz6/random_graph_generation_algorithm/", "CreatedAt": 1706090660.0, "Full Content": "While a seemingly simple looking task, when you dive deeper for an optimal algorithm, we discover various intricacies of this task.\n\nTextbook algorithm is fairly simple, if we need a graph with N nodes and M edges, just keep generating random edges and add if not already present till we get M edges, while this may work great for sparse graphs it becomes inefficient really quick as graph becomes denser.\n\nThe problem becomes even harder if we want the graph to be connected, the base step is to generate a random spanning tree amd add remaining edges randomly such that they don't repeat.\n\nThis problem is essentially to sample some edges from complement of a set where the universal set is the set of all possible edges, in a way that they don't repeat, we use Floyd's Sampling algorithm for it, the end result is a mathematically random graph which is optimal in worst case, you can check out the implementation.\n\n[Implementation](https://gist.github.com/theabbie/0be86edab481af581c450a4cfeef6d69)", "CntComments": 1, "Comments": [{"Comment": "Textbook algorithm for random graphs is the LFR graph algorithm (for clustering/community detection at least), have you checked out that at all? What kinds of graphs does your algorithm generate? Are you getting mostly dense/uniform networks, or can you make ones with power law distributed node degree? Just curious, nice work on this!", "Score": 3, "Replies": []}]},{"Title": "Best way to simulate Low-Field MRI from High-Field MRI", "Score": 10, "URL": "https://www.reddit.com/r/compsci/comments/19d23mv/best_way_to_simulate_lowfield_mri_from_highfield/", "CreatedAt": 1705947072.0, "Full Content": "Hi fellow computer scientists,\n\n&#x200B;\n\nI'm trying to trivially simulate Low-Field MRI from High-Field MRI. I'm wondering if any of this options is valid. If so which one is the best?\n\n&#x200B;\n\nA) Let's consider we have a 3D High-Field MRI image:\n\n1. Apply FFT to obtain k-space -> Undersample k-space with mask -> Apply IFFT\n2. Apply FFT to obtain k-space -> Downsample k-space with bicubic interpolation -> Apply IFFT\n3. Apply FFT to obtain k-space -> Center crop k-space -> Apply IFFT\n\n&#x200B;\n\nB) Also, in case of low SNR in Low-Field, I can consider larger voxels during acquisiton. We want the same FOV (is this okay, right?). In such case what will happen to k-space when compared to an acquisition with smaller voxels? Let's consider we have a 3D High-Field MRI image with size 512x512x512:\n\n1. The new k-space, with size 256x256x256, will look like a downsample version of the k-space acquired with smaller voxels. Similar to option 2.\n2. The new k-space, with size 256x256x256, will look like a center cropped version of the k-space acquired with smaller voxels. Similar to option 3.\n\n&#x200B;\n\nThank you :)", "CntComments": 7, "Comments": [{"Comment": "what's the difference between 1 and 3 in practice?\n\nin the end isn't this more physics than computer science?  1+3 are assuming that the lower field has lower resolution with a sharp cutoff (iiuc).  2 could presumably model a less-abrupt cut-off, which might be more correct.  to decide you would need to understand the physics / engineering, no?", "Score": 1, "Replies": [{"Reply": "The difference between 1 and 3 is that in 1 you clear some lines of the k-space but the matrix size is the same. In option 3) by cropping to the center I reduce the matrix size of the k-space.  \n\n\nYeah, it is also in the context of physics, but it does have a lot to do with signal processing which is something we computer scientists deal with :p (That's why I'm working on it haha)", "Reply Score": 1}]}, {"Comment": "The difference between 1 and 3 is that in 1 you clear some lines of the k-space but the matrix size is the same. In option 3) by cropping to the center I reduce the matrix size of the k-space.  \n\n\nYeah, it is also in the context of physics, but it does have a lot to do with signal processing which is something we computer scientists deal with :p (That's why I'm working on it haha)", "Score": 1, "Replies": [{"Reply": "The wizards at r/DSP might be a little more knowledgeable about this one", "Reply Score": 5}]}, {"Comment": "The wizards at r/DSP might be a little more knowledgeable about this one", "Score": 5, "Replies": [{"Reply": "Here's a sneak peek of /r/DSP using the [top posts](https://np.reddit.com/r/DSP/top/?sort=top&t=year) of the year!\n\n\\#1: [TIL most audio recordings contain a background \"mains hum\" from electric power grid oscillations that can be matched with grid readings to date the clip to the second it was recorded.](https://robertheaton.com/enf/) | [10 comments](https://np.reddit.com/r/DSP/comments/11c3cbf/til_most_audio_recordings_contain_a_background/)  \n\\#2: [Coding a basic guitar tuner in C++](https://youtu.be/Ufx_nrxLhq0) | [5 comments](https://np.reddit.com/r/DSP/comments/15aujd9/coding_a_basic_guitar_tuner_in_c/)  \n\\#3: [About to lose access to MATLAB, is Python a realistic replacement for DSP algorithm development?](https://np.reddit.com/r/DSP/comments/12u3oin/about_to_lose_access_to_matlab_is_python_a/)\n\n----\n^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| ^^[Contact](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| ^^[Info](https://np.reddit.com/r/sneakpeekbot/) ^^| ^^[Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/o8wk1r/blacklist_ix/) ^^| ^^[GitHub](https://github.com/ghnr/sneakpeekbot)", "Reply Score": 1}, {"Reply": "Awesome, thank you!! :D", "Reply Score": 1}]}, {"Comment": "Here's a sneak peek of /r/DSP using the [top posts](https://np.reddit.com/r/DSP/top/?sort=top&t=year) of the year!\n\n\\#1: [TIL most audio recordings contain a background \"mains hum\" from electric power grid oscillations that can be matched with grid readings to date the clip to the second it was recorded.](https://robertheaton.com/enf/) | [10 comments](https://np.reddit.com/r/DSP/comments/11c3cbf/til_most_audio_recordings_contain_a_background/)  \n\\#2: [Coding a basic guitar tuner in C++](https://youtu.be/Ufx_nrxLhq0) | [5 comments](https://np.reddit.com/r/DSP/comments/15aujd9/coding_a_basic_guitar_tuner_in_c/)  \n\\#3: [About to lose access to MATLAB, is Python a realistic replacement for DSP algorithm development?](https://np.reddit.com/r/DSP/comments/12u3oin/about_to_lose_access_to_matlab_is_python_a/)\n\n----\n^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| ^^[Contact](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| ^^[Info](https://np.reddit.com/r/sneakpeekbot/) ^^| ^^[Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/o8wk1r/blacklist_ix/) ^^| ^^[GitHub](https://github.com/ghnr/sneakpeekbot)", "Score": 1, "Replies": []}, {"Comment": "Awesome, thank you!! :D", "Score": 1, "Replies": []}]},{"Title": "Extension of fuzzing for Linux disk encryption", "Score": 1, "URL": "https://is.muni.cz/th/z3gxn/?lang=en", "CreatedAt": 1705952670.0, "Full Content": "", "CntComments": 4, "Comments": [{"Comment": "The submitted link is from \"Interesting Links\" in https://old.reddit.com/r/termux/comments/19573gg/encryption_decryption_android_11_operating_system/ (\"Encryption, Decryption, Android 11 Operating System, Termux, And proot-distro Using Alpine Linux minirootfs: cryptsetup v2.6.1 And LUKS\").", "Score": 1, "Replies": []}]},{"Title": "Why was CP/M so much more expensive than MSDOS?", "Score": 18, "URL": "https://www.reddit.com/r/compsci/comments/19bb8rk/why_was_cpm_so_much_more_expensive_than_msdos/", "CreatedAt": 1705755586.0, "Full Content": "Gary Kidall's OS was priced by IBM at $240, while Bill Gates OS was priced at $40. Why did IBM do this? Was it more expensive to make Gary's? Really interesting story", "CntComments": 13, "Comments": [{"Comment": "I think Bill Gates has understood quite early that selling his OS to students at the university for a low price would lead to a ton of talented people getting used to his product, later introducing it at firms they work. \n\nIBM was directly aiming at selling to corporations as it was their business model on all other products. They priced it according to that.\n\nEven in the windows days it was microsofts policy to try to get schools to use windows, ignore private people who pirate it, but go hard after corporations. This lead to a world where people learn windows in school and use it at home, giving employers no real alternative if they do not want to spend money on training them on the new software.\n\nImho, this idea has governed microsofts actions since it was founded. They were not aiming for a maximum revenue per item sold, they were aiming for market dominance. \n\nAnd considering that almost everyone uses windows and 99% could not name a single OS created by IBM, their strategy seems to have worked out.", "Score": 50, "Replies": [{"Reply": "Still the same. Office 365, Teams, OneNote all the Crap is basically given away to schools  and kids from school don't know anything else and are used to it. This is now called and industry standard.", "Reply Score": 6}]}, {"Comment": "in 1980, CP/M was an established industry standard with thousands of apps. and MS-DOS didn't even exist. so, DR asked e.g. $200 per PC for his OS, while MS asked a fixed price for any number of PCs. As a result, IBM set a price of $40 for MS DOS, and $240 for CP/M", "Score": 13, "Replies": [{"Reply": "This. Also, two different business and pricing strategies.\n\nRemember that Microsoft originally sold programming language products to hobbyists and later professional developers. MSDOS was opportunistic, but not without a strategy behind it.\n\nGates\u2019 goal for MS-DOS from the beginning was to have MS-DOS running on every computer. That\u2019s why he ensured his contract with IBM for PC-DOS allowed him to resell essentially the same operating system as MS-DOS.\n\nSince most people choose an operating system based on the key applications they want to run, having most applications written for MS-DOS would reinforce the cycle of buying MS-DOS. Microsoft\u2019s language products facilitated building MS-DOS applications, and those products were also priced lower than alternatives.\n\nMicrosoft consistently introduced products at a lower price to gain market share.\n\nTake SQL Server, which followed a similar trajectory. It initially significantly undercut Oracle on price, then as it became widely adopted (at one point it was the most-deployed commercial (paid) database), the price increased substantially, but still less than Oracle.\n\nThat strategy has proven very successful over the last 50 years \ud83d\ude42", "Reply Score": 7}]}, {"Comment": "Remember os2 warp was around for a minute?", "Score": 5, "Replies": []}, {"Comment": "It was a smart move on Gates side of it.  Plus [https://cult.honeypot.io/reads/lesson-from-gary-kildall/](https://cult.honeypot.io/reads/lesson-from-gary-kildall/)\n\nOriginal ideas and code is hard.  Stealing it was cheaper.", "Score": 4, "Replies": []}, {"Comment": "IBM was a HW company at the time and didn't see the potential to make any real money with SW.", "Score": 1, "Replies": [{"Reply": "Not really. Their heavy iron (Mainframes) had various very expensive software that you generally leased from IBM.\n\nThe problem is that the PCs were a totally different division. It kind of started out of intelligent terminals without much of a capability of their own as the mainframe did the work. Now the PCs could actually run software independently, they weren't totally sure what to do.", "Reply Score": 4}]}, {"Comment": "Gates undercutting Kidall by so much is part of how he got the contract at all.\n\nThey didn't even write much of DOS 1.0, but instead bought QDOS (Quick and Dirty Operating System) and pretty much flipped it to IBM in a matter of weeks for a quick buck.", "Score": 1, "Replies": [{"Reply": "I\u2019ve still have the original code.", "Reply Score": 1}]}, {"Comment": "Because corporations are stupid enough to pay it. That's always been true.", "Score": 1, "Replies": []}, {"Comment": "Still the same. Office 365, Teams, OneNote all the Crap is basically given away to schools  and kids from school don't know anything else and are used to it. This is now called and industry standard.", "Score": 6, "Replies": [{"Reply": "I remember that in the early 2000s there was a non profit project that designed small netbooks with long battery life, anti-glare displays and a free linux, for use in 3rd world countries with limited infrastructure.\n\nThe idea was, that anyone in the west who bought one, would basically pay double and pay for one being shipped to a 3rd world country. \n\nMicrosofts response was that the bill and melinda gates foundation donated PCs to schools in Africa, forcing them to contractually agree to not accept any free linux devices from anyone and to pay the license for all windows computers starting year 2.\n\nBill gates got a tax-cut for his donation, while microsoft got additional licenses sold and got a non-profit competitor out of the market....\n\nThat's how Microsoft has always operated and how they still operate.", "Reply Score": 6}]}, {"Comment": "This. Also, two different business and pricing strategies.\n\nRemember that Microsoft originally sold programming language products to hobbyists and later professional developers. MSDOS was opportunistic, but not without a strategy behind it.\n\nGates\u2019 goal for MS-DOS from the beginning was to have MS-DOS running on every computer. That\u2019s why he ensured his contract with IBM for PC-DOS allowed him to resell essentially the same operating system as MS-DOS.\n\nSince most people choose an operating system based on the key applications they want to run, having most applications written for MS-DOS would reinforce the cycle of buying MS-DOS. Microsoft\u2019s language products facilitated building MS-DOS applications, and those products were also priced lower than alternatives.\n\nMicrosoft consistently introduced products at a lower price to gain market share.\n\nTake SQL Server, which followed a similar trajectory. It initially significantly undercut Oracle on price, then as it became widely adopted (at one point it was the most-deployed commercial (paid) database), the price increased substantially, but still less than Oracle.\n\nThat strategy has proven very successful over the last 50 years \ud83d\ude42", "Score": 7, "Replies": []}, {"Comment": "Not really. Their heavy iron (Mainframes) had various very expensive software that you generally leased from IBM.\n\nThe problem is that the PCs were a totally different division. It kind of started out of intelligent terminals without much of a capability of their own as the mainframe did the work. Now the PCs could actually run software independently, they weren't totally sure what to do.", "Score": 4, "Replies": []}, {"Comment": "I\u2019ve still have the original code.", "Score": 1, "Replies": []}, {"Comment": "I remember that in the early 2000s there was a non profit project that designed small netbooks with long battery life, anti-glare displays and a free linux, for use in 3rd world countries with limited infrastructure.\n\nThe idea was, that anyone in the west who bought one, would basically pay double and pay for one being shipped to a 3rd world country. \n\nMicrosofts response was that the bill and melinda gates foundation donated PCs to schools in Africa, forcing them to contractually agree to not accept any free linux devices from anyone and to pay the license for all windows computers starting year 2.\n\nBill gates got a tax-cut for his donation, while microsoft got additional licenses sold and got a non-profit competitor out of the market....\n\nThat's how Microsoft has always operated and how they still operate.", "Score": 6, "Replies": []}]},{"Title": "Am I The Only One Interested In Committed-choice Concurrent Constraint Logic Programming For The Web?", "Score": 0, "URL": "https://www.reddit.com/r/compsci/comments/19bt5vk/am_i_the_only_one_interested_in_committedchoice/", "CreatedAt": 1705805159.0, "Full Content": "", "CntComments": 5, "Comments": [{"Comment": "Yes, but we're open - Convince Us :D", "Score": 10, "Replies": [{"Reply": "- This style can be referentially transparent (RT).\n\n- But functional programming requires twisting your brain to do common things, whereas by contrast, logic programming allows a style that looks sort of like imperative coding (\"make it so\"), but still with a fairly straightforward RT interpretation.\n\nhttps://www.researchgate.net/publication/221321646_Actors_as_a_Special_Case_of_Concurrent_Constraint_Programming\n\nI don't know whether I can be convincing without prototyping something, but I'm trying to choose between two models for some details about the semantics.\n\n- try to make everything first-class and conceptually simple, at the cost of maybe chronic poor performance\n\n*vs*\n\n- use a model known to at least be executable but that uses two classes of operations and operands and so may be conceptually more complicated than necessary.", "Reply Score": -2}]}, {"Comment": "When you say \u201cfor the web\u201d.. what exactly do you mean? \u00a0I\u2019ve been interested in similar things from a functional perspective, and have thought about how to build something logic based on top of it.\u00a0", "Score": 2, "Replies": [{"Reply": "By \"for the web\", I meant to run in browsers and servers. Websites can send code to the browser, along with text and markup. The code is usually in JavaScript. The code can implement any reactive behavior relating input from the user to displays in front of the user, and interaction with the server side. The model of time and events and concurrency or reactivity offered by the underlying environment is such that any reactive code has to yield control voluntarily every so often, or otherwise the browser appears from the user viewpoint to have hung.", "Reply Score": 1}]}, {"Comment": "- This style can be referentially transparent (RT).\n\n- But functional programming requires twisting your brain to do common things, whereas by contrast, logic programming allows a style that looks sort of like imperative coding (\"make it so\"), but still with a fairly straightforward RT interpretation.\n\nhttps://www.researchgate.net/publication/221321646_Actors_as_a_Special_Case_of_Concurrent_Constraint_Programming\n\nI don't know whether I can be convincing without prototyping something, but I'm trying to choose between two models for some details about the semantics.\n\n- try to make everything first-class and conceptually simple, at the cost of maybe chronic poor performance\n\n*vs*\n\n- use a model known to at least be executable but that uses two classes of operations and operands and so may be conceptually more complicated than necessary.", "Score": -2, "Replies": [{"Reply": "Cool! okay maybe there's two now :D\n\n<Love the captain Picard reference ;)>\n\nReally interesting explanation! thanks dude I'll definitely keep reading into this one! (up till now I assume just first class EVERYHING!)", "Reply Score": -3}]}, {"Comment": "By \"for the web\", I meant to run in browsers and servers. Websites can send code to the browser, along with text and markup. The code is usually in JavaScript. The code can implement any reactive behavior relating input from the user to displays in front of the user, and interaction with the server side. The model of time and events and concurrency or reactivity offered by the underlying environment is such that any reactive code has to yield control voluntarily every so often, or otherwise the browser appears from the user viewpoint to have hung.", "Score": 1, "Replies": []}, {"Comment": "Cool! okay maybe there's two now :D\n\n<Love the captain Picard reference ;)>\n\nReally interesting explanation! thanks dude I'll definitely keep reading into this one! (up till now I assume just first class EVERYHING!)", "Score": -3, "Replies": []}]},{"Title": "Temperature, Top-k and Top-p Explained", "Score": 4, "URL": "https://www.reddit.com/r/compsci/comments/19ahrbw/temperature_topk_and_topp_explained/", "CreatedAt": 1705666462.0, "Full Content": "Hi there,\n\nI've created a video [here](https://youtu.be/-BBulGM6xF0) where I explain how the temperature, top-k and top-p sampling affect the LLM text generation.\n\nI hope it may be of use to some of you out there. Feedback is more than welcomed! :)", "CntComments": 0, "Comments": []},{"Title": "Cognitive Load In Software Development", "Score": 30, "URL": "https://github.com/zakirullin/cognitive-load", "CreatedAt": 1705508318.0, "Full Content": "", "CntComments": 7, "Comments": [{"Comment": "I have tried to explain this to so very many executives. Having a complex set of rule such as code formatting, increases cognitive load, not decreases it. Basically, a well run company removes the barriers to solid days of solving the real problems, not artificial problems set up by people who have lost the plot of why they were hired.\n\nMost people will naturally adhere to the formatting culture around them. Having a bunch of rigidly enforced rules for this just means a few fewer brain cells wasted on this. This can go into ones like \"No early returns.\" when the rule should be, \"No stupid returns.\"\n\nSomeone can't go nuts with entirely unreadable code, but the reality is that programmers have to read code samples all the time which don't adhere to their rules.\n\nI've been to more than one company where code formatting was a pass/fail on a code review, whereas having any unit tests at all was not.\n\nFor example, I avoid deep nesting. It is usually confusing and ugly. But sometimes you just have to pinch your nose and do it. Having a certain level of nesting as a hard and fast rule is dumb.\n\nI have been moving much more back to monoliths. Way way way easier if you have good programmers; but a total disaster if you don't. This is one of the great advantages of microservices. You can give a halfwit a microservice and it limits the damage they can do. Or, just don't hire nitwits. If your executive is out of control on a hiring binge, then this could be hard. I've been with startups with 3 people and they were desperate to bring in interns just to get body count up.\n\nMeetings. I don't think people fully understand the cognitive load imposed by meetings. Let's say there is a 10am meeting. A programmer showing up at 8:30 will see that and do the mental math that they will just be getting in the groove by 9 or 9:30. Then, they will have to stop, do some minor prep for the meeting (as someone probably attached some reading materials), and then go early to the meeting. Thus, this programmer will probably just surf the net and answer some useless emails.\n\nThe meeting might only be 45 min, but there will be some chatting after, and often manager/executive types intercept programmers at this moment for \"just 5 minutes of your time\" which is often used to try to pin them down to when some feature will be ready. A feature which is in a pure research phase at this point and has no possible firm deadline.\n\nNow it is about 11am. The programmer does the mental calculation that again, they will just be getting in the groove as lunch looms, so more surfing and chatting on slack. Thus, a 30-45 minute meeting just killed an entire morning. The same could happen with one more 30-45 minute meeting in the middle of the afternoon.\n\nEven first thing standups are just BS to compensate for terrible designs and terrible planning. If the design is good, then people know what is needed and are doing it. Progress can be monitored through tools like Jira. Standups mostly are for the egos of managers and to prop up very weak team members.\n\nThen you have the endless stream of interruptions such as slack. This is a nightmare tool for poor designs and poor planning; something which comes with typical microservice workflows where all the interfaces/contracts/apis are in eternal flux. Or you have HR continuously wasting people's time with mandatory training, or questions about their ever changing medical plans. I witnessed one company where the VP of engineering shut down the changeover to a new medical plan. It was going to save the company some modest amount of money, but take up a huge amount of employee time. He did the simple math of just salaries wasted filling out forms, and the endless new questions like, \"is this or that covered\" and showed it was no savings at all.\n\nAnother place showed every email of zero importance sent out to all cost $5,000. They implemented an email quota for all with a wonderful trick. You could mark an email as useless. The more emails marked useless resulted in an imposed quota. HR was almost immediately shut out of the email system. Everyone was marking all their emails as stupid. The IT guy said in the first week 100 of their emails were marked as stupid. The person I know at this organization who pushed for this asked why they were spending more time on BBQ-related emails than things like resolving questions where someone was incorrectly paid. \n\nIf I were running a large company of hundreds or thousands, I would have roving patrols looking for wasters of time with a snitch line. They would have the ability to not only reprimand people but order them to stop the interrupting behaviour on pain of being fired. But the key would be the only valid complaints would be listened to from people who were interrupted. Not people trying to shut down some behaviour they didn't like. \n\nI endlessly meet people in companies where tech debt has overcome them and they are no longer creating much value who defend their stupid practices. They see complex code reviews which add no value as somehow solving problems which don't really exist. They see meetings as a way forward out of their tech debt when usually they are mad scrambles to blame others and figure out a way forward. They see change as \"chasing fads\" when it is really the only way they will have any hope.\n\nThe success stories I've seen always ended up throwing out old systems. A classic that I've seen work over and over is to literally throw out the old codebase and start fresh. Ideally, with a language/OS/framework which excluded the former \"senior\" developers. Often this was wholesale. New team, new people, new source code repository, new managers, new OS, new language, and cut them off very much from the old. Often all the institutional knowledge could be pilfered through one or two key developers from the old team. The firefighters who knew all the problems because they were the ones in on weekends solving the latest crisis.\n \nThe next step the company usually took at this point was to attack this new team as hard and as fast as was possible. What I never saw happen was the old team to realize there was an existential crisis which could be solved by getting their heads out of their asses. They only used internal political scheming to do battle.", "Score": 9, "Replies": [{"Reply": "Wow, thanks for such a long and meaningful writing! It seems you've got a lot to share!", "Reply Score": 2}]}, {"Comment": "good tips covered in there", "Score": 5, "Replies": []}, {"Comment": "[deleted]", "Score": -5, "Replies": [{"Reply": "> turns into a gigantic mess eventually anyway.\n\nWhich doesn't mean we should do nothing. It is like that in most of the cases. But not everywhere. If the developers are experienced and aware enough - things end up far better than a gigantic mess", "Reply Score": 8}, {"Reply": "Not entirely, it depends on what and why it's a mess.\n\nMy last job had developers not use the standard c# style guide. (there is only one, no one uses the only other option) and Hungarian notation.\n\nIt was a real wtf moment on that code base.", "Reply Score": 2}, {"Reply": "Coding is easy - software development is hard!\n\nOur whole job is to avoid that gigantic mess :)", "Reply Score": 1}]}, {"Comment": "Wow, thanks for such a long and meaningful writing! It seems you've got a lot to share!", "Score": 2, "Replies": []}, {"Comment": "> turns into a gigantic mess eventually anyway.\n\nWhich doesn't mean we should do nothing. It is like that in most of the cases. But not everywhere. If the developers are experienced and aware enough - things end up far better than a gigantic mess", "Score": 8, "Replies": []}, {"Comment": "Not entirely, it depends on what and why it's a mess.\n\nMy last job had developers not use the standard c# style guide. (there is only one, no one uses the only other option) and Hungarian notation.\n\nIt was a real wtf moment on that code base.", "Score": 2, "Replies": []}, {"Comment": "Coding is easy - software development is hard!\n\nOur whole job is to avoid that gigantic mess :)", "Score": 1, "Replies": []}]},{"Title": "Debating the Transition from Monolith to Microservices: Where's the Line?", "Score": 0, "URL": "https://www.reddit.com/r/compsci/comments/1999672/debating_the_transition_from_monolith_to/", "CreatedAt": 1705529026.0, "Full Content": "I'm diving deep into the complexities of evolving from a monolithic architecture to a microservices model and I'm intrigued by the nuances of this transition. I\u2019d appreciate your insights and personal experiences on this topic.\n\nThe journey from a monolithic application to a microservices architecture is rarely straightforward. It often involves segmenting a large, integrated system into smaller, independent components. But the big question is: when do these changes signify a genuine shift from a monolith to microservices?\n\n**Example for Discussion**:\n\n* Imagine a system where the UI is a React-based Node.js application, and the backend is a separate Django service.\n* Additionally, there are distinct SQL and NoSQL databases, each functioning as separate services.\n* The front end and back end are developed and deployed independently.\n\nWith this setup in mind, here are my queries:\n\n* Does breaking down a monolithic service into distinct components like this mean we're no longer dealing with a monolith, or is it too soon to call it a microservices architecture?\n* Considering key microservices attributes (independent deployment, loosely coupled services, individual data management), what are the definitive signs that we've transitioned to a microservices model?\n* In the spectrum between monolithic and microservices structures, we encounter terms like \"modular monolith,\" \"hybrid architecture,\" and \"distributed monolith.\" How do you differentiate between these in real-world scenarios?\n\nThis transformation seems to be more of a gradual evolution rather than a distinct jump. I\u2019m curious about how others identify this shift in their projects. At what stage would you classify an architecture like the example above as microservices? Are there particular indicators or benchmarks that you look out for?", "CntComments": 6, "Comments": [{"Comment": "I would say that the distinction between monolith and microservice is rather subjective and the more important focus is whether you are able to develop and deploy features in the appropriate size and scope.\n\nFor example, if you are changing the font of a web page and you have to redeploy your entire backend stack, then the system is too monolithic. \n\nIf you are deploying a small feature and it requires you to change the contracts across multiple related microservices, then the system is too separated and need to be combined. \n\nWithout knowing your developmental goals and restrictions, it would be hard to tell whether a system is a \u2018monolith\u2019 or a \u2018microservice\u2019", "Score": 5, "Replies": []}, {"Comment": "It's a simple pros / cons list. You are a company. You have a certain amount of resources. The aim is to use those resources as best you can to maximize delighting your customers.\n\nAny rewrite is a use of resources that is usually forgoing short term / immediate customer value (e.g. the launch of a new feature or product) for long term customer value (e.g. launching more features faster in the future, being able to scale, being more flexible for future needs that cant be anticipated, etc.)\n\nA move from a monolith to a micro service architecture is a company wide architectural change and thus a *massive* re-write. \n\nTherefore it usually happens when the situation is so dire that, in order to survive, one must change their entire paradigm. This is not to be taken likely. This is an effort where a company says - \"I don't care if we don't improve our customer's experience for years, if we don't change the way we build software we'll be out of business\".\n\nIn my opinion. That's the line.\n\nI've done this a few times in my career. A quick example from when I was at Amazon. We moved Amazon from a monolith to micro service architecture. The effort started in 2000, teams were still working on it in 2005. It happened because our e-commerce platform was so big and bulky that we could not spin up servers fast enough to meet Black Friday and Cyber Monday demands. We were literally losing customers while waiting for servers to spin up and DDOSing our other services. That's a good reason to reconsider your architecture.", "Score": 3, "Replies": []}, {"Comment": "You'll always be bound by Conway's law. If you're a single person, you'll end up with Microservices as tightly bound as a monolith.", "Score": 2, "Replies": []}, {"Comment": "Start by dropping the \u201cmicro\u201d, just think about the services you need. Identify them as operational subdomains of your system that are loosely coupled to the rest of the system (i.e. they have few kinds of interactions). You\u2019ll realize you started the transition when you actually move the first of such services to an independent repo/deployment. Then you just need to move the rest. Depending on the size of the system this can take several years.", "Score": 1, "Replies": []}]},{"Title": "Can Rice's theorem be applied to decision problems that might allow \"fuzzy\" answers?", "Score": 6, "URL": "https://www.reddit.com/r/compsci/comments/198sspq/can_rices_theorem_be_applied_to_decision_problems/", "CreatedAt": 1705484286.0, "Full Content": "This question is based on the post [https://www.reddit.com/r/compsci/comments/18m2y94/](https://www.reddit.com/r/compsci/comments/18m2y94/is_the_problem_of_finding_the_output_given_the/?utm_source=share&utm_medium=web2x&context=3).\n\nIn that post, the following question was asked: is there a Turing machine `A`, accepting **any** Turing machine `T` as an input, which is guaranteed to **halt on all inputs**, and outputs the last symbol printed by `T` if `T` halts (and when `T` does not halt, simply print anything)?\n\nPeople mentioned that this is closely related to Rice's theorem.\n\nFor simplicity, let's say `T` can only output 0 and 1. The property that \"the last character printed by `T` is 1\" is undecidable by Rice's theorem. However, `A` is **not** required to **decide** this property. It may freely output anything if `T` does not halt. So **can Rice's theorem really be applied to determine that such machine** `A` **does NOT exist?** Are there any subtleties there?\n\n&#x200B;", "CntComments": 23, "Comments": [{"Comment": "This is pretty easy to see for yourself if you just plug your idea in to how Rice's theorem works. Suppose we have a machine A that outputs the last symbol of any machine T that halts, and outputs an arbitrary symbol if T does not halt.\n\nWe create a machine M that calls T on M, then if T outputs 0, M outputs 1 and halts. Otherwise M outputs 0 and halts. If T(M) outputs 0, then it's wrong because M outputs 1. If T(M) outputs anything else, it's wrong because then M outputs 0. So whatever T outputs, it's wrong.\n\nSaying that Rice's theorem proves you can't do something is the same as saying that if such a machine existed, you could slot it into this type of proof where a machine calls it on its own code and does the opposite of what's indicated by the output.", "Score": 5, "Replies": []}, {"Comment": "Your question itself contains \u201coutputs the last symbol .. if T halts\u201d. Due to \u201cif T halts\u201d, where T is any Turing machine, it is already impossible simply by Turing\u2019s halting theorem (Rice\u2019s theorem is proved actually in a similar manner, but no need to invoke it here).", "Score": 7, "Replies": [{"Reply": "\"If T halts\" does not mean being able to determine whether or not T halts. Read carefully.", "Reply Score": -3}]}, {"Comment": "> Can Rice's theorem be applied to decision problems that might allow \"fuzzy\" answers?\n\nYes.\n\n> is there a Turing machine A, accepting any Turing machine T as an input, which is guaranteed to halt on all inputs, and outputs the last symbol printed by T if T halts (and when T does not halt, simply print anything)?\n\nNo.\n\n> can Rice's theorem really be applied to determine that such machine A does NOT exist? \n\nYes.\n\n> Are there any subtleties there?\n\nNo.\n\nLet me know if you have any followup questions.", "Score": 2, "Replies": [{"Reply": "Explain your answer! I already know the answer; all I am asking for in an explanation.", "Reply Score": 2}, {"Reply": "I don't think you're right about the \"is there a Turing Machine\" part. Despite the unnecessary extension to any TM, we're given that any input T does halt. You could have a two-tape machine that simulates T on one tape and between each step of T, if T wrote a symbol on the last step, replaces the contents of the other tape with said symbol (encoded if necessary).", "Reply Score": 1}]}, {"Comment": "Yes.", "Score": -1, "Replies": [{"Reply": "How?", "Reply Score": 3}]}, {"Comment": ">So can Rice's theorem really be applied to determine that such machine A does NOT exist?\n\nYes\n\n>Are there any subtleties there?\n\nYes. And this is a good question. \ud83d\ude01\n\nLet me start by saying that people will often use \"by Rice's theorem\" to mean \"by applying the technique used to prove Rice's theorem\". I think this is because (1) the formal statement is dense, and (2) the proof is short and very insightful. So it's easier (and often more elucidating) to apply the proof to your problem instead of checking the exact preconditions.\n\nSo back to this particular problem. The subtlety, as you note, is that `A` is \"fuzzy\" on some inputs. Formally, we can represent this by restricting `A` to be a *partial function* that is only defined on the non-fuzzy cases. Rice's Theorem applies to partial functions, so it does still apply in this case.\n\n\\-----\n\nFor an example of a problem that is solvable \"by the proof of Rice's theorem\" but not strictly \"by Rice's Theorem\", consider the following: Given a TM that is guaranteed to run in either time `O(n)` or `O(2^n)`, it's impossible to determine which it is.", "Score": 1, "Replies": []}, {"Comment": "\"If T halts\" does not mean being able to determine whether or not T halts. Read carefully.", "Score": -3, "Replies": [{"Reply": "Do you mean something more like \"when T halts\"? So you already know the T you have halts and you want to get the last symbol it printed before doing so? (follow up questions: for what input? What if it's doesn't print any?)", "Reply Score": 5}]}, {"Comment": "Explain your answer! I already know the answer; all I am asking for in an explanation.", "Score": 2, "Replies": []}, {"Comment": "I don't think you're right about the \"is there a Turing Machine\" part. Despite the unnecessary extension to any TM, we're given that any input T does halt. You could have a two-tape machine that simulates T on one tape and between each step of T, if T wrote a symbol on the last step, replaces the contents of the other tape with said symbol (encoded if necessary).", "Score": 1, "Replies": [{"Reply": "We're given that A halts, not T.", "Reply Score": 1}]}, {"Comment": "How?", "Score": 3, "Replies": [{"Reply": "It's just a direct application of the theorem. The function you're asking about is just an instance of a property to which Rice's theorem applies.", "Reply Score": 2}]}, {"Comment": "Do you mean something more like \"when T halts\"? So you already know the T you have halts and you want to get the last symbol it printed before doing so? (follow up questions: for what input? What if it's doesn't print any?)", "Score": 5, "Replies": [{"Reply": "When T does not halt, I still require the machine A to halt, but otherwise the behaviour is unrestricted.", "Reply Score": 2}]}, {"Comment": "We're given that A halts, not T.", "Score": 1, "Replies": [{"Reply": "Ah, yep. That wasn't clear from the OP, but is in the SO post.", "Reply Score": 1}]}, {"Comment": "It's just a direct application of the theorem. The function you're asking about is just an instance of a property to which Rice's theorem applies.", "Score": 2, "Replies": [{"Reply": "What property do you have in mind? I'm not seeing anything obvious that works here.\n\nIt's not hard to prove that such an A does not exist (https://www.reddit.com/r/compsci/comments/18m2y94/is_the_problem_of_finding_the_output_given_the/ke1m0j6/ gives a proof), and the proof for this is *very* similar to the proof of Rice's theorem. But I don't see how Rice's theorem can be applied directly--am I missing something obvious? The Turing machine A doesn't have to give the same result for every index of a non-halting TM, so it seems to me like the obvious choice of a property is one that *doesn't* depend solely on the language (and can instead depend on the specific index of a non-halting TM), and so Rice's theorem doesn't directly apply. What property did you have in mind that works?", "Reply Score": 2}]}, {"Comment": "When T does not halt, I still require the machine A to halt, but otherwise the behaviour is unrestricted.", "Score": 2, "Replies": [{"Reply": "How does A know when to halt in the general case? For a halting T, A can simply halt after T halts. But what if T only halts after the lifetime of the universe? Or if T doesn't halt? How does A meaningfully distinguish between those two scenarios in such a way that A will halt (presumably before the lifetime of the universe) for non-halting T, but not halt for the extremely long-lived-but-still-halting T until it's done?\n\nThe answer is: A would have to know whether a general input T would halt, which is impossible because halting problem.", "Reply Score": 3}, {"Reply": "Let\u2019s have two such machines you want: one of them called T1 outputs 0, when its parameter doesn\u2019t halt, the other, T2 outputs 1 when its parameter doesn\u2019t halt.\n\n```\ndef halt(t):\n  return T1(t) != T2(t)\n```\n\nHere, you have an algorithm to determine if any program terminates, which is clearly impossible as per the halting theorem.", "Reply Score": 1}]}, {"Comment": "Ah, yep. That wasn't clear from the OP, but is in the SO post.", "Score": 1, "Replies": []}, {"Comment": "What property do you have in mind? I'm not seeing anything obvious that works here.\n\nIt's not hard to prove that such an A does not exist (https://www.reddit.com/r/compsci/comments/18m2y94/is_the_problem_of_finding_the_output_given_the/ke1m0j6/ gives a proof), and the proof for this is *very* similar to the proof of Rice's theorem. But I don't see how Rice's theorem can be applied directly--am I missing something obvious? The Turing machine A doesn't have to give the same result for every index of a non-halting TM, so it seems to me like the obvious choice of a property is one that *doesn't* depend solely on the language (and can instead depend on the specific index of a non-halting TM), and so Rice's theorem doesn't directly apply. What property did you have in mind that works?", "Score": 2, "Replies": [{"Reply": "Rice's Theorem applies to partial functions, so we restrict A to the non-fuzzy inputs.", "Reply Score": 1}]}, {"Comment": "How does A know when to halt in the general case? For a halting T, A can simply halt after T halts. But what if T only halts after the lifetime of the universe? Or if T doesn't halt? How does A meaningfully distinguish between those two scenarios in such a way that A will halt (presumably before the lifetime of the universe) for non-halting T, but not halt for the extremely long-lived-but-still-halting T until it's done?\n\nThe answer is: A would have to know whether a general input T would halt, which is impossible because halting problem.", "Score": 3, "Replies": [{"Reply": "I think that's a good intuition behind the proof given in the link OP gave, but if I'm understanding OP right, their question is specifically whether Rice's theorem can be directly applied to prove it--it sounds like OP isn't confused about the proof that A cannot exist, just about the claim that it follows directly from Rice's theorem.", "Reply Score": 3}]}, {"Comment": "Let\u2019s have two such machines you want: one of them called T1 outputs 0, when its parameter doesn\u2019t halt, the other, T2 outputs 1 when its parameter doesn\u2019t halt.\n\n```\ndef halt(t):\n  return T1(t) != T2(t)\n```\n\nHere, you have an algorithm to determine if any program terminates, which is clearly impossible as per the halting theorem.", "Score": 1, "Replies": [{"Reply": "But I don't think my assumptions imply the existence of such T1, T2.", "Reply Score": 1}]}, {"Comment": "Rice's Theorem applies to partial functions, so we restrict A to the non-fuzzy inputs.", "Score": 1, "Replies": []}, {"Comment": "I think that's a good intuition behind the proof given in the link OP gave, but if I'm understanding OP right, their question is specifically whether Rice's theorem can be directly applied to prove it--it sounds like OP isn't confused about the proof that A cannot exist, just about the claim that it follows directly from Rice's theorem.", "Score": 3, "Replies": []}, {"Comment": "But I don't think my assumptions imply the existence of such T1, T2.", "Score": 1, "Replies": [{"Reply": "It\u2019s literally your machine, I just define the arbitrary value as 0 and 1.", "Reply Score": 1}]}, {"Comment": "It\u2019s literally your machine, I just define the arbitrary value as 0 and 1.", "Score": 1, "Replies": [{"Reply": "I mean that the arbitrary value can be any value **separately** for **each** input. It won't be 0 or 1 for all inputs at the same time.", "Reply Score": 1}]}, {"Comment": "I mean that the arbitrary value can be any value **separately** for **each** input. It won't be 0 or 1 for all inputs at the same time.", "Score": 1, "Replies": []}]},{"Title": "Relationship between symbol (data type) and debug symbol (symbol table)?", "Score": 2, "URL": "https://www.reddit.com/r/compsci/comments/1983fk9/relationship_between_symbol_data_type_and_debug/", "CreatedAt": 1705413355.0, "Full Content": "I'm reading Wiki page on [debug symbols](https://en.wikipedia.org/wiki/Debug_symbol), where it says the following:\n>A debug symbol is a special kind of symbol \n\nReading linked page on [symbols (primitive data type)](https://en.wikipedia.org/wiki/Symbol_(programming)), I think this connection is incorrect. \"Symbol\" in the context of [symbol tables](https://en.wikipedia.org/wiki/Symbol_(programming)) refers to key-value pairs, and debug symbol is just special type of information used for debugging, and does not have anything to do with the primitive data type.\n\nWikipedia page for [symbol (data type)](https://en.wikipedia.org/wiki/Symbol_(programming)) says:\n>Uniqueness is enforced by holding them in a symbol table.\n\nThis refers that these primitive data types are made in some sense \"constant\" by the way they're stored in symbol tables. \n\nPlease correct me if I understood this incorrectly.", "CntComments": 1, "Comments": [{"Comment": "> \"Symbol\" in the context of symbol tables refers to key-value pairs\n\nCorrect. Specifically, it associates things like function and variable names and their corresponding memory addresses. This is typically used by the compiler mid-compilation: if you're compiling multiple files then you may see a reference to a function in a different file you haven't gotten to yet, so you leave in a note \"look up function 'foo' in the symbol table during the linking stage to hook this up\"\n\n> debug symbol is just special type of information used for debugging\n\nCorrect. The compiler does not normally leave in information about variable types or the line numbers of source code that a block of assembly corresponds to. It has that information while compiling, but it's of no use during execution, so it's left out unless you specifically say \"keep all the verbose notes, I want to use them in my debugger\"\n\n> does not have anything to do with the primitive data type\n\nOn the contrary, it's completely intertwined. Where before the symbol table might only have contained \"x is defined at the following memory address,\" debugging symbols add additional context \"and x is an integer defined on line 238 of analysis.c\"", "Score": 5, "Replies": []}]},{"Title": "Crashing As A Kind of Failure vs. Crashing As A Separate Concept From Failure", "Score": 12, "URL": "https://www.reddit.com/r/compsci/comments/197ca4e/crashing_as_a_kind_of_failure_vs_crashing_as_a/", "CreatedAt": 1705334807.0, "Full Content": "Succeed/fail semantics can be supported with a code library or directly in a programming language. In succeed/fail languages and techniques, predicate operators (e. g. comparisons, such as `<=`) do not typically return a Boolean value, but rather, they fail the current execution context if the condition is not met. It's possible to code a conjunction of operations and the conjunction succeeds if and only if all the operations succeed. It's possible to code a list of alternatives and the semantics here is that the computer must try alternatives until encountering one that succeeds.\n\nAll this can be done in imperative programming, pure functional programming, or logic-based programming (for example, committed-choice concurrent-constraint logic programming, like ToonTalk).\n\nI coded a library that supports succeed/fail semantics in an imperative language and the client code can also be imperative. I decided that if the underlying programming system detects a condition that would usually only arise from a programmer error, I would not treat a case like that as failure, but rather, as a separate concept called a \"crash\". I provide a slot where a reference to a callback function can be lodged, so that if a run in the library crashes, the outer code that is invoking the library can recover control and deal with the error. I felt that by providing this concept and hook, I was mimicking Erlang's support for having one process supervise one or more other processes and deal with their possibly crashing.\n\nI now feel that separating crashing from failure was an engineering error, that it leads to requiring more programming effort than should be necessary in building, for example, a web server.\n\nDo you have any strong case to make for separating, conceptually, crashing from failure in a technique (or language) supporting succeed/fail semantics?", "CntComments": 9, "Comments": [{"Comment": "It sounds like you've reinvented exceptions.", "Score": 10, "Replies": []}, {"Comment": "Have you looked into an adjacent topic of research, [Crash-Only Software](https://lwn.net/Articles/191059/)?", "Score": 5, "Replies": [{"Reply": "I had not; thank you for the link. It's an interesting article, but it is not exactly about the semantics of programming languages and language-like techniques, I think.", "Reply Score": 3}, {"Reply": "Another adjacent topic is \"Railway-oriented Programming\". This term arose in connection with F#, a functional-mostly language.", "Reply Score": 1}]}, {"Comment": "There are 'engineering errors' that look like 'failures' and vice versa, in your parlance, and that's the part that gets me. You're adding extra complexity which might be justified in some cases, but adding a headache in others.\n\nConsider a memory bit flip (albeit rare) that corrupts a pointer and causes a crash, for whatever reason. Is that really an engineering error?", "Score": 2, "Replies": [{"Reply": "Computers are engineered with at least some measures to reduce the frequency of computer failure to execute as per the design intent for it. However, to the best of my understanding, there is no way to engineer it so well that such failures can be completely excluded. So no, a bit flip from a cosmic ray or whatever is not an engineering error. It is an accident during operation.", "Reply Score": 0}]}, {"Comment": "I believe this is how I build all my apps, I\u2019m a self taught programmer now 9 years in with 20+ apps out there including client apps. I have always believed that the app should never crash without an easily explainable reason. \n\nMy apps rarely crash because all of the code, all of the functions etc are built to succeed or fail. I would rather catch the bad patterns that led to a crash then have a product in peoples hands that doesn\u2019t crash and also doesn\u2019t work as they expected. \n\nIMO it\u2019s better to just write code that doesn\u2019t crash and the only way to do that is to provide a way it could theoretically crash if a certain value was missing, insuring that you write better code to prevent the code that could crash from running unless the value is actually there.", "Score": 1, "Replies": []}, {"Comment": "I had not; thank you for the link. It's an interesting article, but it is not exactly about the semantics of programming languages and language-like techniques, I think.", "Score": 3, "Replies": []}, {"Comment": "Another adjacent topic is \"Railway-oriented Programming\". This term arose in connection with F#, a functional-mostly language.", "Score": 1, "Replies": []}, {"Comment": "Computers are engineered with at least some measures to reduce the frequency of computer failure to execute as per the design intent for it. However, to the best of my understanding, there is no way to engineer it so well that such failures can be completely excluded. So no, a bit flip from a cosmic ray or whatever is not an engineering error. It is an accident during operation.", "Score": 0, "Replies": [{"Reply": "Yes, but your scheme can\u2019t tell the difference.", "Reply Score": 2}]}, {"Comment": "Yes, but your scheme can\u2019t tell the difference.", "Score": 2, "Replies": [{"Reply": "Both of my schemes assume the computer is error-free. Using software to detect hardware problems would be a whole other area of inquiry, if it's even possible.", "Reply Score": 0}]}, {"Comment": "Both of my schemes assume the computer is error-free. Using software to detect hardware problems would be a whole other area of inquiry, if it's even possible.", "Score": 0, "Replies": []}]},{"Title": "easy to criticize papers for undergrads", "Score": 18, "URL": "https://www.reddit.com/r/compsci/comments/196i70i/easy_to_criticize_papers_for_undergrads/", "CreatedAt": 1705245685.0, "Full Content": " i'm TAing an intro to research class. i want to teach students how to critically review a paper (consider experimental design, results, etc) by having them walk through some examples. do y'all know of any easy to read papers that have some obvious flaws/shortcomings? Particularly papers from areas that feel more approachable like HCI or CS Ethics? Or some fundamental papers in systems/architecture? Or just papers that are easy for students to analyze?\n\nFor context, these students are 2nd semester freshmen with limited research experience. The goal is to teach them how to read papers critically.", "CntComments": 4, "Comments": [{"Comment": "I taught a special topics course on pervasive computing a few years ago and allowed students to pick specifically from a well known author (http://www.cs.cmu.edu/~satya/satyapubs.html). \n\nThis worked out pretty well because they were all in the same domain (almost all) and therefore could be compared.   \n\nAdditionally since this was over a couple of decades in a fast-moving field, by the end of the semester they could see the differences in research focus between the late 90\u2019s and 2020\u2019s.", "Score": 4, "Replies": []}, {"Comment": "Find paper on openreview and let students write their own peer review (summary, strength, weakness)?", "Score": 2, "Replies": [{"Reply": "Great idea! Is there something like OpenReview for non-ML papers? I looked around but couldn't find anything.", "Reply Score": 1}]}, {"Comment": "The maintainer of memcached identified serious concerns in the MemC3 paper.\n\nhttps://memcached.org/blog/paper-review-memc3", "Score": 1, "Replies": []}, {"Comment": "Great idea! Is there something like OpenReview for non-ML papers? I looked around but couldn't find anything.", "Score": 1, "Replies": []}]},{"Title": "KL Divergence Mathematics Explained", "Score": 5, "URL": "https://www.reddit.com/r/compsci/comments/196d6o1/kl_divergence_mathematics_explained/", "CreatedAt": 1705228952.0, "Full Content": "Hi there,\n\nI've created a video [here](https://youtu.be/MXcsW613msA) where I explain the mathematical intuition behind the KL divergence.\n\nI hope it may be of use to some of you out there. Feedback is more than welcomed! :)", "CntComments": 1, "Comments": [{"Comment": "How about a paragraph in your post explaining what kl divergence is and why we might be interested in it", "Score": 16, "Replies": []}]},{"Title": "Why is mkdir slower as we get deeper into the directory hierarchy?", "Score": 20, "URL": "https://www.reddit.com/r/compsci/comments/195kb4a/why_is_mkdir_slower_as_we_get_deeper_into_the/", "CreatedAt": 1705138248.0, "Full Content": "Consider the following program in C:\n\n    #include <stdio.h>\n    #include <unistd.h>\n    #include <sys/stat.h>\n    \n    int main(){\n        size_t const N = 10000000;\n        for(size_t i = 0; i<N; i++){\n            printf(\"%zu\\n\", i);\n            mkdir(\"1\", 0700);\n            chdir(\"1\");\n        }\n        return 0;\n    }\n\nOn my filesystem, the program gets slower and slower as `i` increases, after making tens of thousands of directories.\n\nI do not understand this. Isn't creating a directory just\n\n1. Allocating a little space on disk,\n2. Write a link from the current directory to the allocated space,\n3. Write links for . and ..\n\nNONE of these operations have anything to do with directory hierarchy depth. Basically, we start with the file descriptor of the current directory, and there is no need to traverse the entire hierarchy. The permission info of the current directory should already be cached. So why is it slowing down?\n\n\\[Also posted here: [https://www.reddit.com/r/filesystems/comments/1958e7y/why\\_is\\_mkdir\\_slower\\_as\\_we\\_get\\_deeper\\_into\\_the/](https://www.reddit.com/r/filesystems/comments/1958e7y/why_is_mkdir_slower_as_we_get_deeper_into_the/?utm_source=share&utm_medium=web2x&context=3)\\]", "CntComments": 21, "Comments": [{"Comment": "See if you still get a slowdown if you add an `fsync` or use an in-memory filesystem like `tmpfs`.\n\nSince you aren't doing any `fsync`s in this program, the OS is free to implement this by updating only the in-memory page cache before returning control to your program, and only actually write these changes to disk asynchronously in the background.  At some point you'll exhaust the memory available for this cache and start having to wait on the disk to catch up (specifically: each new write would have to wait for one long-ago enqueued write to complete so that that the dirty page from the old write can be freed and become available to capture the new write).\n\nCan you plot your data?  If this is the cause of the slowdown you're seeing, you should see some initial period of uniformly fast operations followed by a sharp transition to uniformly slower operations.  If instead you're seeing a more gradual change, then it's likely a different mechanism.  Being able to see the curve in graph form helps a lot in figuring these things out.", "Score": 35, "Replies": [{"Reply": "This makes sense. Also you can check if the depth really is a factor by just starting the program 10k deep instead of from the start.", "Reply Score": 3}, {"Reply": "Adding an `fsync` or using an in-memory filesystem like `tmpfs` should help determine if the slowdown is due to the limited memory available for caching and background disk writes. Plotting your data will provide a clearer picture of the trend and help identify the cause of the slowdown.", "Reply Score": 1}]}, {"Comment": "modern OS are multi-threaded. between these two calls, the directory can be moved to a different place, access rights may be changed and so on. So, we need either to watch all changes made by other threads, or avoid caching and go from root dir on each operation, using the full directory path as the only input.\n\nBTW, I wrote an archiver, and the time to open all files processed by an operation may easily become a bottleneck (afair I measured 10k fopens/sec on Windows). When I looked into optimization, I found an internal Windows API that allows to work with inodes directly. But it's not for light-hearted ones :)", "Score": 9, "Replies": [{"Reply": "> modern OS are multi-threaded. between these two calls, the directory can be moved to a different place, access rights may be changed and so on. So, we need either to watch all changes made by other threads, or avoid caching and go from root dir on each operation, using the full directory path as the only input.\n\nAt least on the system the OP is using \u2014 in their other post they revealed it is an APFS filesystem \u2014 access permissions on any of the ancestor directories should not be relevant. `chdir` only needs to consult the permissions for the current working directory in order to look up the name, and the file type and permissions of the entry identified by the name. `mkdir` only needs to consult the permissions for the current working directory and check that the directory doesn't already have an entry for the name.\n\nNow it's entirely possible the OP has something unusual on their system that _does_ make `mkdir` or `chdir` quadratic when they shouldn't be. Or perhaps Apple is doing something stupid with their APFS filesystem. That's for us to guess. I wouldn't say it's anything to do with permissions though, since that's just not how permissions work on it.", "Reply Score": 6}]}, {"Comment": "Have you tried profiling your toy code?", "Score": 2, "Replies": [{"Reply": "Same question on my mind.  Which system calls are happening, how long are they taking, and what are the kernel, drivers or observable hardware state doing in the meantime?", "Reply Score": 1}]}, {"Comment": ">  Isn't creating a directory just\n> Allocating a little space on disk,\n> Write a link from the current directory to the allocated space,\n> Write links for . and ..\n> NONE of these operations have anything to do with directory hierarchy depth\n\nNo / your assumption is bad. APFS is a closed source filesystem. The work involved under the hood in creating a directory is neither documented nor published in this case. It's completely open ended as to what the behavior might be.\n\nI did run this code and reproduced the behavior. I'm taking it face value. I presume there is a quirk of the APFS filesystem that involves some performance cost re operating with deeply nested folders. APFS is a proprietary closed source filesystem, so there's no practical way to dig into this beyond that level. Thus, the research has hit a wall and the case is closed for me.", "Score": 2, "Replies": [{"Reply": "Since this is on macOS\u2026 I wonder if: \n\n(1) indexing for Spotlight searches could be an issue. Try running from a base directory where you\u2019ve disabled spotlight indexing. I always keep source code in such a directory b/c spotlight slows down compilation.\n\n(2) are you running antivirus? You may want to try this again on a machine without antivirus, or with antivirus temporarily disabled.", "Reply Score": 0}]}, {"Comment": "I cannot reproduce, either on a fast SSD or spinning rust. Here is my code:\n\n    #include <inttypes.h>\n    #include <stdio.h>\n    #include <sys/stat.h>\n    #include <time.h>\n    #include <unistd.h>\n    \n    static const int iterations = 100000;\n    \n    int main() {\n      puts(\"iteration,time\");\n      for (int i = 0; i < iterations; i++) {\n        struct timespec start, stop;\n    \n        clock_gettime(CLOCK_MONOTONIC, &start);\n        mkdir(\"1\", 0700);\n        const int e = chdir(\"1\");\n        clock_gettime(CLOCK_MONOTONIC, &stop);\n    \n        if (e) {\n          printf(\"err: %d\\n\", e);\n          return e;\n        }\n    \n        int64_t t = stop.tv_sec - start.tv_sec;\n        t *= 1000000000;\n        t += stop.tv_nsec - start.tv_nsec;\n    \n        printf(\"%d,%ld\\n\", i, t);\n      }\n    }\n\nI then load it up in a spreadsheet, delete outliers, plot it as a scatterplot, and add a trendline. In neither case was there a significant trend.\n\n> Isn't creating a directory just\n>\n> 1. Allocating a little space on disk,\n> 1. Write a link from the current directory to the allocated space,\n> 1. Write links for . and ..\n\nNo.\n\nEach filesystem will do different things. You shouldn't make assumptions about how a filesystem works without reading the documentation and/or code.\n\n`mkdir` won't allocate space inside a directory for the files in that directory. There will just be some pile of free space somewhere that the disk will write new data in.\n\nThere will exist somewhere some data structure that has elements in it; it might have one entry per filesystem entity (a directory, a file, etc) or it might have one data structure for files and another data structure for directories. In both cases there will (almost always) be space allocated for a new entry already. When you run `mkdir`, you add an entry corresponding to the new directory. It will probably add a link from the new directory to the parent directory, and it will add some sort of metadata somewhere so the parent directory can find the new directory.\n\nThe 'links' to `.` and `..` are implicit; they're not actually written to disk.\n\nDon't assume that the filesystem thinks in terms of paths and such; a modern filesystem is more akin to a database than your naive mental model would lead you to believe. If my description about how a filesystem works seems very vague, that is on purpose. A filesystem in whatever manner it pleases. Some developer somewhere though, \"hey, wouldn't it be neat if I made a filesystem that works like <x>\" and then a new filesystem is born, potentially with a completely different method of operation than any other filesystem in existence. On January 7th, 2024, 6 days ago at the time of this writing, Linux 6.7 was released, and included in linux 6.7 is a new filesystem called [bcachefs.](https://bcachefs.org/) How does it work? No idea. Read the docs.", "Score": 1, "Replies": [{"Reply": ">No.  \n>  \n>Each filesystem will do different things. You shouldn't make assumptions about how a filesystem works without reading the documentation and/or code.  \n>  \n>mkdir won't allocate space inside a directory for the files in that directory. There will just be some pile of free space somewhere that the disk will write new data in.  \n>  \n>There will exist somewhere some data structure that has elements in it; it might have one entry per filesystem entity (a directory, a file, etc) or it might have one data structure for files and another data structure for directories. In both cases there will (almost always) be space allocated for a new entry already. When you run mkdir, you add an entry corresponding to the new directory. It will probably add a link from the new directory to the parent directory, and it will add some sort of metadata somewhere so the parent directory can find the new directory.\n\nI am having simplified assumptions of a filesystem, but everything you say here is just repeating my assumption. You add a lot of details, but it still falls within the points I list.", "Reply Score": 0}]}, {"Comment": "The other place you posted it is much more relevant. It\u2019s hardly computer science at this level. But yeah, it\u2019s due to specifics about how the particular file system or other parts of the call is implemented.\u00a0", "Score": -11, "Replies": [{"Reply": "but general ideas on OS/FS implementation are part of CS", "Reply Score": 11}, {"Reply": "How is this not related to computer science? It covers a concept involving programming and operating systems", "Reply Score": 1}]}, {"Comment": "How much of a slowdown are we talking about?\n\nI would point out the obvious int formatting going on with the printf but I don't think that would be hugely noticeable", "Score": 1, "Replies": [{"Reply": "I am sure printf is not the reason. I have tried removing it.", "Reply Score": 2}, {"Reply": "formatting an integer can be done in a few dozen cycles. a system call on its own takes hundreds of cycles. OTOH, console interaction may be pretty slow, since it also requires a system call each time", "Reply Score": 1}]}, {"Comment": "This makes sense. Also you can check if the depth really is a factor by just starting the program 10k deep instead of from the start.", "Score": 3, "Replies": []}, {"Comment": "Adding an `fsync` or using an in-memory filesystem like `tmpfs` should help determine if the slowdown is due to the limited memory available for caching and background disk writes. Plotting your data will provide a clearer picture of the trend and help identify the cause of the slowdown.", "Score": 1, "Replies": []}, {"Comment": "> modern OS are multi-threaded. between these two calls, the directory can be moved to a different place, access rights may be changed and so on. So, we need either to watch all changes made by other threads, or avoid caching and go from root dir on each operation, using the full directory path as the only input.\n\nAt least on the system the OP is using \u2014 in their other post they revealed it is an APFS filesystem \u2014 access permissions on any of the ancestor directories should not be relevant. `chdir` only needs to consult the permissions for the current working directory in order to look up the name, and the file type and permissions of the entry identified by the name. `mkdir` only needs to consult the permissions for the current working directory and check that the directory doesn't already have an entry for the name.\n\nNow it's entirely possible the OP has something unusual on their system that _does_ make `mkdir` or `chdir` quadratic when they shouldn't be. Or perhaps Apple is doing something stupid with their APFS filesystem. That's for us to guess. I wouldn't say it's anything to do with permissions though, since that's just not how permissions work on it.", "Score": 6, "Replies": [{"Reply": "i have no idea about Apple, but on Windows, even mapping between user-level directory names (like C:\\\\) and real names (like \\\\\\\\DISK1\\\\DIR1\\\\DIR2) may change\n\nalso, chdir isn't quadratic, he said it's linear in the length of the path, and afaik it's true for Windows too", "Reply Score": 1}]}, {"Comment": "Same question on my mind.  Which system calls are happening, how long are they taking, and what are the kernel, drivers or observable hardware state doing in the meantime?", "Score": 1, "Replies": []}, {"Comment": "Since this is on macOS\u2026 I wonder if: \n\n(1) indexing for Spotlight searches could be an issue. Try running from a base directory where you\u2019ve disabled spotlight indexing. I always keep source code in such a directory b/c spotlight slows down compilation.\n\n(2) are you running antivirus? You may want to try this again on a machine without antivirus, or with antivirus temporarily disabled.", "Score": 0, "Replies": [{"Reply": "Good ideas, but no change. No antivirus either, (besides XProtect obviously).", "Reply Score": 0}]}, {"Comment": ">No.  \n>  \n>Each filesystem will do different things. You shouldn't make assumptions about how a filesystem works without reading the documentation and/or code.  \n>  \n>mkdir won't allocate space inside a directory for the files in that directory. There will just be some pile of free space somewhere that the disk will write new data in.  \n>  \n>There will exist somewhere some data structure that has elements in it; it might have one entry per filesystem entity (a directory, a file, etc) or it might have one data structure for files and another data structure for directories. In both cases there will (almost always) be space allocated for a new entry already. When you run mkdir, you add an entry corresponding to the new directory. It will probably add a link from the new directory to the parent directory, and it will add some sort of metadata somewhere so the parent directory can find the new directory.\n\nI am having simplified assumptions of a filesystem, but everything you say here is just repeating my assumption. You add a lot of details, but it still falls within the points I list.", "Score": 0, "Replies": []}, {"Comment": "but general ideas on OS/FS implementation are part of CS", "Score": 11, "Replies": [{"Reply": "yeah this is 100% cs.", "Reply Score": 9}]}, {"Comment": "How is this not related to computer science? It covers a concept involving programming and operating systems", "Score": 1, "Replies": []}, {"Comment": "I am sure printf is not the reason. I have tried removing it.", "Score": 2, "Replies": []}, {"Comment": "formatting an integer can be done in a few dozen cycles. a system call on its own takes hundreds of cycles. OTOH, console interaction may be pretty slow, since it also requires a system call each time", "Score": 1, "Replies": []}, {"Comment": "i have no idea about Apple, but on Windows, even mapping between user-level directory names (like C:\\\\) and real names (like \\\\\\\\DISK1\\\\DIR1\\\\DIR2) may change\n\nalso, chdir isn't quadratic, he said it's linear in the length of the path, and afaik it's true for Windows too", "Score": 1, "Replies": [{"Reply": "Op's code #includes unistd.h\n\nMight not be windows filesystem semantics", "Reply Score": 2}]}, {"Comment": "Good ideas, but no change. No antivirus either, (besides XProtect obviously).", "Score": 0, "Replies": []}, {"Comment": "yeah this is 100% cs.", "Score": 9, "Replies": []}, {"Comment": "Op's code #includes unistd.h\n\nMight not be windows filesystem semantics", "Score": 2, "Replies": []}]},{"Title": "How You Can Hide Files Inside Images: The Art of Steganography", "Score": 0, "URL": "https://www.reddit.com/r/compsci/comments/195qspy/how_you_can_hide_files_inside_images_the_art_of/", "CreatedAt": 1705160702.0, "Full Content": "https://medium.com/@jizoskasa/how-you-can-hide-files-inside-images-the-art-of-steganography-e5c2200a5671", "CntComments": 4, "Comments": [{"Comment": "I remember the first time I saw this was in the movie Along Came A Spider. The kids were sending images with hidden messages. I thought it was so cool.", "Score": 3, "Replies": [{"Reply": "I learned everything I needed to know about computer science from Sandra Bullock in The Net.", "Reply Score": 5}]}, {"Comment": "I'm not a steganography expert but what the article describes clearly isn't any form of (secure) steganography and not even _doable_ (for educational purpose) given common image formats like JPEG.", "Score": 1, "Replies": []}, {"Comment": "I learned everything I needed to know about computer science from Sandra Bullock in The Net.", "Score": 5, "Replies": [{"Reply": "You better stop the hack before your photo gets completely pixelated.  If not?  You're basically dead.", "Reply Score": 1}]}, {"Comment": "You better stop the hack before your photo gets completely pixelated.  If not?  You're basically dead.", "Score": 1, "Replies": []}]},{"Title": "Books recommendation of computer history", "Score": 18, "URL": "https://www.reddit.com/r/compsci/comments/194mr7u/books_recommendation_of_computer_history/", "CreatedAt": 1705035489.0, "Full Content": "Hi. I'm here to ask for one or more books that you would recommend about computer history. I need some which explain from a architecture perspective. I hope the book tell about the IBM mainframes, CPUs architectures like Motorola 6800, Intel 8086, etc. The evolution from CISC to RISC, The birth of OS, UNIX, Linux and GNU. Also programming languages historical perspective punch cards, assembly, BASIC, C language, Java, Python, etc. \n\nI appreciate any suggestion and thanks for reading so far. ", "CntComments": 19, "Comments": [{"Comment": "The history of computers is an area I kind of specialize in. Here's some suggestions:\n\n* [Bit by Bit: An Illustrated History of Computers by Stan Augarten](https://www.amazon.com/Bit-Illustrated-History-Computers/dp/0899193021/)\n* [Fire in the Valley: The Birth and Death of the Personal Computer by Michael Swaine](https://www.amazon.com/Fire-Valley-Birth-Personal-Computer/dp/1937785769/)\n* [Hackers: Heroes of the Computer Revolution by Steven Levy](https://www.amazon.com/Hackers-Computer-Revolution-Steven-Levy/dp/1449388396/)\n* [The Man Who Invented the Computer: The Biography of John Atanasoff, Digital Pioneer by Jane Smiley](https://www.amazon.com/Man-Who-Invented-Computer-Biography/dp/0385527136/)\n* [Atanasoff: Forgotten Father of the Computer by Clark R. Mollenhoff](https://www.amazon.com/Atanasoff-Forgotten-Computer-Clark-Mollenhoff/dp/0813800323/)\n* [The First Electronic Computer: The Atanasoff Story by Alice R. Burks](https://www.amazon.com/First-Electronic-Computer-Atanasoff-Story/dp/0472100904/)\n* [The Story of the Computer: A Technical and Business History by Stephen J Marshall](https://www.amazon.com/Story-Computer-Technical-Business-History/dp/1546849076/)\n* [The Dream Machine by M. Mitchell Waldrop](https://www.amazon.com/Dream-Machine-M-Mitchell-Waldrop/dp/1732265119/)\n* [Dr. An Wang: Computer Pioneer by Jim Hargrove](https://www.amazon.com/Dr-Wang-Computer-Distinction-Biographies/dp/0516032909/)\n* [The Friendly Orange Glow: The Untold Story of the Rise of Cyberculture by Brian Dear](https://www.amazon.com/Friendly-Orange-Glow-Untold-Cyberculture/dp/1101973633/)\n* [Electronic Brains: Stories from the Dawn of the Computer Age by Mike Hally](https://www.amazon.com/Electronic-Brains-Stories-Dawn-Computer/dp/0309096308/)\n* [Project Whirlwind: The History of a Pioneer Computer by Kent C Redmond](https://www.amazon.com/Project-Whirlwind-history-pioneer-computer/dp/0932376096/)\n* [From Whirlwind to MITRE: The R&D Story of The SAGE Air Defense Computer by Kent C. Redmond](https://www.amazon.com/Whirlwind-MITRE-Defense-Computer-Computing/dp/0262182017)\n* [Where Wizards Stay Up Late: The Origins Of The Internet by Katie Hafner](https://www.amazon.com/Where-Wizards-Stay-Up-Late/dp/0684832674/)\n* [ENIAC: The Triumphs and Tragedies of the World's First Computer by Scott McCartney](https://www.amazon.com/Eniac-Triumphs-Tragedies-Worlds-Computer/dp/0802713483/)\n* [ENIAC in Action: Making and Remaking the Modern Computer by Thomas Haigh](https://www.amazon.com/ENIAC-Action-Remaking-Computer-Computing/dp/0262535173)\n* [John von Neumann and the Origins of Modern Computing by William Aspray](https://www.amazon.com/Neumann-Origins-Modern-Computing-History/dp/0262011212)\n* [The Computer Prophets by Jerry M. Rosenberg](https://www.amazon.com/Computer-Prophets-Jerry-M-Rosenberg/dp/0026049600/)\n* [Computers: The Life Story of a Technology by Eric G. Swedin](https://www.amazon.com/Computers-Story-Technology-Greenwood-Technographies/dp/0313331499/)\n* [The Making of the Micro: A History of the Computer by Christopher Riche Evans](https://www.amazon.com/Making-Micro-History-Computer/dp/0442222408/)\n* [IBM's Early Computers by Charles J. Bashe](https://www.amazon.com/IBMs-Early-Computers-History-Computing/dp/0262022257)\n* [History of the Internet: A Chronology, 1843 to the Present by Hilary Poole](https://www.amazon.com/History-Internet-Chronology-1843-Present/dp/1576071189/)\n* [Charles Babbage: Pioneer of the Computer by Anthony Hyman](https://www.amazon.com/Charles-Babbage-Computer-Anthony-Hyman/dp/0691083037/)\n* [The Supermen: The Story of Seymour Cray and the Technical Wizards Behind the Supercomputer by Charles J. Murray](https://www.amazon.com/Supermen-Seymour-Technical-Wizards-Supercomputer/dp/0471048852/)\n* [IBM: The Rise and Fall and Reinvention of a Global Icon by James W. Cortada](https://www.amazon.com/gp/product/0262039443)\n* [A History of Modern Computing by Paul E. Ceruzzi](https://www.amazon.com/History-Modern-Computing/dp/0262532034/)", "Score": 20, "Replies": [{"Reply": "My bank account now hates you with a passion.\n\nThanks!", "Reply Score": 3}, {"Reply": "Well. that's a list for sure. Thank you", "Reply Score": 4}, {"Reply": "[The First Computers\u2014History and Architectures](https://www.amazon.com/First-Computers-History-Architectures-History-Computing/dp/0262681374)\n\n[Racing the Beam: The Atari Video Computer System](https://en.wikipedia.org/wiki/Racing_the_Beam)", "Reply Score": 2}, {"Reply": "i'd like to add these as well\n\n* [Cellular: An Economic and Business History of the International Mobile-Phone Industry](https://www.amazon.com/gp/product/B09RF17L8G)\n* [The History of the GPU - Eras and Environment](https://www.amazon.com/History-GPU-Eras-Environment/dp/3031135806)\n* [The History of the GPU - Steps to Invention](https://www.amazon.com/History-GPU-Steps-Invention/dp/3031109678)\n* [The History of the GPU - New Developments](https://www.amazon.com/History-GPU-New-Developments/dp/303114046X)\n* [UNIX: A History and a Memoir by Brian Kernighan ](https://www.amazon.com/UNIX-History-Memoir-Brian-Kernighan)\n* [Makers of the Microchip: A Documentary History of Fairchild Semiconductor](https://www.amazon.com/Makers-Microchip-Documentary-Fairchild-Semiconductor/dp/0262546264)\n* [Home Computers: 100 Icons that Defined a Digital Generation](https://www.amazon.com/Home-Computers-Defined-Digital-Generation/dp/0262044013)\n* [Silicon Planet: My Life in Computer Chips](https://www.amazon.com/Silicon-Planet-Life-Computer-Chips/dp/B0BYM8BZDT)\n* [Who Are You? : Nintendo's Game Boy Advance Platform](https://www.amazon.com/gp/product/0262044390)", "Reply Score": 2}, {"Reply": "* In the Beginning... Was the Command Line\n* iWoz\n* Masters of doom\n* Just for Fun: The Story of an Accidental Revolutionary\n* The Cuckoo's Egg: Tracking a Spy Through the Maze of Computer Espionage\n\n..would fit in this bookshelf", "Reply Score": 1}, {"Reply": "I'd add in at least The Closed World for some social history too.", "Reply Score": 1}, {"Reply": "Fabulous list!", "Reply Score": 1}]}, {"Comment": "More of a coffee table book, The Computer Book by Garfinkel and Grunspan has the breath, if not depth, you are looking for.", "Score": 5, "Replies": [{"Reply": ">The Computer Book by Garfinkel and Grunspan\n\nThanks. I will take a look to this one", "Reply Score": 1}]}, {"Comment": "Strongly recommend _The Mythical Man-Month_ for history of OS/360.", "Score": 4, "Replies": [{"Reply": "Thanks for the reccomendation", "Reply Score": 1}]}, {"Comment": "Soul of a New Machine for a crazy look into the late 70's. Also see https://www.faughnan.com/papers/eaglecomp.pdf\n\niWoz for his side of the story\n\nDive into the Vintage Computer Federation keynote speaker videos - https://www.youtube.com/@vcfederation/videos\n\nFor example - the ENIAC programmers - https://youtu.be/cA6tKzZtbhc?si=r_tonNIug1-ZEntD\n\nand any of the Burger Becky talks", "Score": 4, "Replies": [{"Reply": "Soul of a new Machine is such a good read!", "Reply Score": 1}]}, {"Comment": "I dont know of a book that covers all of that, but `IBM: The Rise and Fall and Reinvention of a Global Icon` covers some of that.", "Score": 3, "Replies": [{"Reply": "No book covers all and even is difficult to know a combination of books that complement well each other. That's why appreciate your suggestion.", "Reply Score": 1}]}, {"Comment": "Hackers by Steven Levy covers the change from mainframes to personal computing, that's more the history of people than OSes and chips and architectures.", "Score": 3, "Replies": [{"Reply": "Thanks :)", "Reply Score": 1}]}, {"Comment": "[deleted]", "Score": 1, "Replies": [{"Reply": "it's not about history, but about making an entire 8-bit cpu from basic logical elements", "Reply Score": 1}]}, {"Comment": "My bank account now hates you with a passion.\n\nThanks!", "Score": 3, "Replies": []}, {"Comment": "Well. that's a list for sure. Thank you", "Score": 4, "Replies": []}, {"Comment": "[The First Computers\u2014History and Architectures](https://www.amazon.com/First-Computers-History-Architectures-History-Computing/dp/0262681374)\n\n[Racing the Beam: The Atari Video Computer System](https://en.wikipedia.org/wiki/Racing_the_Beam)", "Score": 2, "Replies": []}, {"Comment": "i'd like to add these as well\n\n* [Cellular: An Economic and Business History of the International Mobile-Phone Industry](https://www.amazon.com/gp/product/B09RF17L8G)\n* [The History of the GPU - Eras and Environment](https://www.amazon.com/History-GPU-Eras-Environment/dp/3031135806)\n* [The History of the GPU - Steps to Invention](https://www.amazon.com/History-GPU-Steps-Invention/dp/3031109678)\n* [The History of the GPU - New Developments](https://www.amazon.com/History-GPU-New-Developments/dp/303114046X)\n* [UNIX: A History and a Memoir by Brian Kernighan ](https://www.amazon.com/UNIX-History-Memoir-Brian-Kernighan)\n* [Makers of the Microchip: A Documentary History of Fairchild Semiconductor](https://www.amazon.com/Makers-Microchip-Documentary-Fairchild-Semiconductor/dp/0262546264)\n* [Home Computers: 100 Icons that Defined a Digital Generation](https://www.amazon.com/Home-Computers-Defined-Digital-Generation/dp/0262044013)\n* [Silicon Planet: My Life in Computer Chips](https://www.amazon.com/Silicon-Planet-Life-Computer-Chips/dp/B0BYM8BZDT)\n* [Who Are You? : Nintendo's Game Boy Advance Platform](https://www.amazon.com/gp/product/0262044390)", "Score": 2, "Replies": []}, {"Comment": "* In the Beginning... Was the Command Line\n* iWoz\n* Masters of doom\n* Just for Fun: The Story of an Accidental Revolutionary\n* The Cuckoo's Egg: Tracking a Spy Through the Maze of Computer Espionage\n\n..would fit in this bookshelf", "Score": 1, "Replies": []}, {"Comment": "I'd add in at least The Closed World for some social history too.", "Score": 1, "Replies": []}, {"Comment": "Fabulous list!", "Score": 1, "Replies": []}, {"Comment": ">The Computer Book by Garfinkel and Grunspan\n\nThanks. I will take a look to this one", "Score": 1, "Replies": []}, {"Comment": "Thanks for the reccomendation", "Score": 1, "Replies": []}, {"Comment": "Soul of a new Machine is such a good read!", "Score": 1, "Replies": []}, {"Comment": "No book covers all and even is difficult to know a combination of books that complement well each other. That's why appreciate your suggestion.", "Score": 1, "Replies": []}, {"Comment": "Thanks :)", "Score": 1, "Replies": []}, {"Comment": "it's not about history, but about making an entire 8-bit cpu from basic logical elements", "Score": 1, "Replies": []}]},{"Title": "80s BASIC vs Modern languages (in the context of coding education)", "Score": 32, "URL": "https://www.reddit.com/r/compsci/comments/193n8qi/80s_basic_vs_modern_languages_in_the_context_of/", "CreatedAt": 1704930874.0, "Full Content": "How do you compare the coding tools available for young students today with microcomputers from the 80s, such as the ZX Spectrum, BBC Micro, Commodore 64, etc.?\n\n  \nDespite their limited specifications, many people considered those early machines excellent for coding education. They provided a built-in language (BASIC), a built-in code editor, and a graphical-based API for printing and drawing on the screen.\n\n  \nDo you agree, or do you think that Scratch and Python offers more advantages?\n\n&#x200B;\n\nhttps://preview.redd.it/wyn1ng4tapbc1.jpg?width=1420&format=pjpg&auto=webp&s=3c7474c8e4fc6be688f141f7e5c7a8bb062b86d9", "CntComments": 47, "Comments": [{"Comment": "\"built-in code editor\" : I'd like to hear of some examples.  Of those machines I had any experience with (c64, apple II, maybe the TI99), the process of editing was dumping the lines to the screen, and then retyping the one you want to change.  Though I seem to recall some utilities you could load in that let you move the cursor around to \"type in\" lines that were already listed.  Inserting characters was a no-go though.\n\nBut to your overall question, yeah, there was something very accessible about a machine you could turn on, and within seconds, start typing code and seeing results.  And I would agree that between about 1990-2010 we'd lost that ability to just turn on our ever more powerful machines, and be able to experiment with telling them what to do.  On those early machines, you learned to program because you had to in order to use it at all.  Once usable operating systems and applications became ubiquitous, people could get the value they needed without ever needing to do that.\n\nNow though, a decent programming environment is as easy to download, install, and use as any other application on your PC, so there's comparatively nothing stopping you if that's what you decide you want to do.\n\nAs for BASIC itself?  Eeeeehh, it did not adapt well to the needs of modern computing.  Please don't ever ask me to read or write any VBScript ever again.", "Score": 16, "Replies": [{"Reply": "There are times when I think it\u2019d be useful to build a computer and an operating system that, when you turn it on, pretty much dumps you into a code editor and requires you to either load a program to run it, or to write your own program\u2014just as a learning tool.", "Reply Score": 4}, {"Reply": ">\"built-in code editor\" : I'd like to hear of some examples.\n\nThis started to show up by the DOS era, e.g. QBASIC.", "Reply Score": 1}]}, {"Comment": "> Do you agree, or do you think that Scratch and Python offers more advantages?\n\nMy background is with the Atari 400 and Commodore 64, so I will speak from that perspective, but other computers from that era are similar.  \n\nThe biggest thing that BASIC did was provide PEEK and POKE to directly read and write memory.  Using graphics and sound hardware effectively required knowing the memory map of the system, knowing where specific memory was used by other hardware, and modifying it directly to achieve a desired effect.\n\nWhat this did for the person learning it was given them an understanding of computer architecture in addition to programming.  Despite being high level, BASIC of that era is not dissimilar to assembly language in terms of direct hardware access. \n\nModern languages like Scratch and Python will teach someone the basics of logical flow, input and output, but it's abstracted away from the underlying hardware.  In a world of multi-tasking OSes, directly accessing hardware is a much rarer thing.  Generally someone is using some audio library or graphics library to access that functionality.\n\nSo I think modern tools may be more user friendly, but there's definitely something lost in terms of learning how computers are implemented and how CPUs, and peripheral devices function at a hardware level.  I can say that having that information has given me an intuition that's served me well in both my education (Electrical Engineering / Computer Science) and also in industry.  A lot of times I can think through how a computer system is likely working under the hood at the software level and down to the hardware level before digging into documentation to confirm it.\n\nThat kind of learning is probably lost in today's computing environments unless someone is playing with a Raspberry Pi or an Arduino and connecting to external hardware/circuits.", "Score": 14, "Replies": [{"Reply": "I had gotten my start with programming on the Commodore 64, and was about to make a similar comment.  Commodore 64 BASIC required developing an intimate understanding of how the hardware works - largely because it was missing many useful commands.  The benefit is that you can understand what many of the memory addresses were used for and how to do bit manipulation.  The drawback is that a lot of it was difficult to figure out - like how hard it was to use a bitmap graphics mode.\n\nModern languages, like Python, can allow someone to do a lot more with less typing, but it is all hidden magic.  There is a whole generation of programmers who don't have a strong understanding of how things work, but instead rely on memorizing how black boxes are used.  There are a large number of programmers who have only used JavaScript.\n\nIt definitely changes how one thinks about programming - especially optimization.  Now, unoptimized and wasteful code executes quickly and the programmer is unaware of what is going on under the surface.  With 1980s computers, they were so slow that small optimizations could have a huge impact on performance.  To a modern programmer, it would seem impossible to do anything with hardware that has a 2MHz CPU and only 64KB of RAM.  It would seem unimaginable to someone who doesn't think twice about simple applications using hundreds of megabytes of RAM.\n\nI think someone interested in computer science should have low level experience - not because they would likely get hired to write assembly code, but because it provides a different perspective on programming.\n\nI'm not sure if low level languages is best for an introduction to computer programming, though - especially with kids, who are easily confused and quickly get bored.  Being able to quickly do something fun helps with maintaining motivation and not feeling overwhelmed.  So, it's hard to know what would be better for education.  Maybe both should be taught.", "Reply Score": 6}, {"Reply": "I would agree with that. \n\nThe simple computer architecture and esp, Peek and Poke meant that as a kid ( I was like 8 ), I could dip my toes into machine code. \nI was self taught and managed to write a two player snake game on the C64 in machine code at about 11 years old. \nI can still remember it. \nI wrote basic, which would then poke the assembly program into the right place and run it. \n\nThis was only possible because you could mix and match basic with the lower level - so I could learn over time. And of course the sound was done using poke\u2026 which is a great intro to the memory map.", "Reply Score": 2}, {"Reply": "From C16/plus4 perspective TEDmon was so great that I never used peek or poke in BASIC.  SYS is the only command I need. Later on PC I used C. It was faster and less constrained than BASIC and I stopped writing assembler although the linker in C encourages a assembler modules..", "Reply Score": 1}]}, {"Comment": "I grew up with a ZX Spectrum and for nostalgic fun, code up Advent of Code solutions that run on the classic 48k model (or the 16k model sometimes), written in ZX BASIC.\n\nHere are some of my favorite videos showing the code running:\n\n* [2023, Day 12](https://www.youtube.com/watch?v=Myc9sIdAC8o)\n* [2023, Day 13](https://www.youtube.com/watch?v=12cJUXsNt04)\n* [2022, Day 16](https://www.youtube.com/watch?v=or7xOU9lKNs)\n* [2022, Day 18](https://www.youtube.com/watch?v=eAPl7BAsqd4)\n* [2022, Day 22](https://www.youtube.com/watch?v=1zP6lFSIzbs)\n* [2022, Day 24](https://www.youtube.com/watch?v=3U-gdUKx7ho)\n\nThese videos showcase the fact that you can use a language like ZX BASIC to implement interesting algorithms and provide neat visualizations of those algorithms running. Writing each one had some challenges; for example, the first program above requires numbers with more than 40 bits of precision, a hash table, and recursive functions, none of which have direct support in ZX BASIC.\n\nAnd a language like ZX BASIC very much encourages graphical display. Plotting points, drawing lines, using color, it's all very readily available, and that is perhaps the best thing. And in the 1980s, the ZX BASIC editor was pretty decent for a home micro.\n\nBut on the other hand, while today it's a fun challenge for me today to implement recursive algorithms or hash tables in ZX BASIC, it's not _easy_.\n\nIn short, coding in ZX BASIC, you can achieve cool things, and maybe you'll think of things you wouldn't do in some other language. But we could probably say that about most languages.\n\nI'd say overall that when it comes to learning to code, the language doesn't matter as much as how well the teaching is done, how much the programs capture people's imaginations, etc. Is a challenge fun, or is it drudgery? Is making something easy an amplifier that lets you dive deeper, or a way to make things feel trivial and uninteresting? It's all eye of the beholder, in the end.", "Score": 6, "Replies": []}, {"Comment": "The BASIC of the 1980s was ineffective at teaching [structured programming](https://en.wikipedia.org/wiki/Structured_programming).", "Score": 18, "Replies": [{"Reply": "While that may be true for dialects like Commodore BASIC and others, there were also dialects like Amiga Basic (1985\u20131988) that supported structured programming just fine.", "Reply Score": 3}, {"Reply": "You only need structure for a large program. My programs fit on screen. Then I moved to a PC. Same with assembly. So it can only be an introduction.", "Reply Score": 3}]}, {"Comment": "The graphics you could create with Applesoft Basic were amazing\u2026 by the standards of 1982. And that\u2019s the catch. I learned to code on an Apple II+ and thought it was magical. But the video games I wrote then would seem ridiculously archaic and boring to kids today. I don\u2019t think there\u2019s any way to get back that combination of simplicity and feeling of accomplishment.", "Score": 4, "Replies": []}, {"Comment": "Personally, I think the fact that those machines were so limited, as was the BASIC of the day, was what made them such terrific tools on which to learn.  Sure, you could argue that Python is today's BASIC (it isn't, but...).  The problem with Python, as with everything today, is you have a paralysing ocean of choice before you!  Back then, you had your primitive box with a few Kbytes of RAM and no devices to speak of, a display with a fixed layout, and one language with no libraries (unless you were prepared to load such things from tape).\n\nOver and over again in my life I have seen that constraints breed creativity: there is just less to think about.  Microsoft's SmallBasic was a good stab at re-producing the old 80s environment.", "Score": 4, "Replies": []}, {"Comment": "I don\u2019t think any modern language has captured the \u201cjust type and things happen\u201d feel of BASIC. No, it wasn\u2019t a great language, but that wasn\u2019t the point. The goal of BASIC was to shrink the loop between writing code and seeing output to be almost instantaneous. \n\nIMO the modern successor to BASIC is SonicPi, which is audio not graphics, but captures the same feeling. A SonicPi-for-graphics would be kinda amazing.", "Score": 6, "Replies": [{"Reply": "https://pythonsandbox.com/turtle", "Reply Score": 5}, {"Reply": "For a lot of these old machines, BASIC was effectively used as the operating system.  It was impossible to use these machines without using at least a little BASIC - even if it was just the LOAD and RUN commands.", "Reply Score": 2}]}, {"Comment": "There were multi-user business machines in the 70s that only used BASIC like the Wang 2200.  70s BASIC was nearly identical to 80s BASIC.  The transition was mostly evolution of 8-bit Microsoft BASIC to the 16-bit Microsoft BASIC that was built into the IBM PC ROM (although Wang BASIC was quite a bit different).", "Score": 3, "Replies": []}, {"Comment": "There's a lot of nostalgia around them, but the 80s basic machines are basically a nightmare to program on. BASIC is horrifically slow, and BASIC of that era is just a garbage language, even for the era. That's without getting into the problem of editor, saving/loading, etc.\n\nModern Python is far simpler than 80s basic. It does far more and largely stays out of the newbie's way, but also can be still used for virtually any kind of advanced learning where basic is less than useless.", "Score": 5, "Replies": [{"Reply": "Python can\u2019t even do multi-line lambdas. I like indent, but I would propose Kotlin or Java or C# or rust or swift. I don\u2019t get the fuss about Go or Dart.", "Reply Score": -2}]}, {"Comment": "I learned coding on a TRS-80 in the early 90s. My parents had upgraded to an Epson x86 clone and threw it in a closet so my 8 year old self pulled it out and hooked it up, and copied a few games from the magazines I found in the box. For an 8 Y/O, GOTO programming in BASIC was really approachable, but as others have said it's outdated now.\n\nI went on to learn Visual Basic, which gave me the logical start I needed to learn C, Java, Python, PHP, Lua, Progress, etc.\n\nPython isn't bad, but it's more complex than BASIC", "Score": 2, "Replies": []}, {"Comment": "Python is about as simple as BASIC, and there's no good reason to learn something that can't be used for any practical purpose.", "Score": 5, "Replies": [{"Reply": "BASIC allows you to draw on the screen with a single line of code.", "Reply Score": 5}]}, {"Comment": "I don't know of anything that tries to popularise or teach this, but for universal availability, there is nothing to beat writing javascript within html and then just loading that file into your browser.", "Score": 0, "Replies": []}, {"Comment": "Good luck motivating your students with BASIC and a Commodore 64.", "Score": 1, "Replies": []}, {"Comment": "I got my start on basic. Python is better by far. Scratch is great for teaching kids the building blocks of how to code", "Score": 1, "Replies": []}, {"Comment": "I also grew up on the speccie but c'mon\n..now we have you tube and stack overflow. Doesn't matter if you're doing basic or anything, there's ANSWERS now", "Score": 1, "Replies": []}, {"Comment": "I am highly biased in how you teach computer science fundamentals, because I taught SICP back when I was in grad school and I don't think any other book/curriculum does it better.\n\nBut, while SICP was originally taught in an \"80s language\", Scheme, the language doesn't actually matter, and a couple of years ago, a rewritten SICP was released that teaches the entire course in Javascript.  If you're teaching the fundamentals correctly, the language you're teaching them in doesn't matter terribly much.\n\nSo, are modern languages worse or better?  They have advantages and they have pitfalls.  They are more readable and have better debugging.  They are also \"easy to use\" in a way that encourages students to just \"learn Javascript\" instead of learning the CS fundamentals because it's just so easy to type \"npm install express\".  But, on the other hand, it makes basic programming tasks more accessible to more people.", "Score": 1, "Replies": []}, {"Comment": "Modern Basics are much faster than Python, often as fast as C code.\nPython however is more portable and is very easy to install/use/bind to C libraries.\nTry to install something like FreeBasic from source code(there are no linux packages on repos) to run a single QBasic game is a pain.", "Score": 1, "Replies": []}, {"Comment": "They were good for learning to program machines which were commonly in use at the time. A language like Python is good for programming machines in use now. I don't think one or the other is inherently better or worse for learning to program, but learning to program for a modern machine is probably more available and relevant to a learner's everyday life.", "Score": 1, "Replies": []}, {"Comment": "TI-99/4a heiss", "Score": 1, "Replies": []}, {"Comment": "For what it's worth, I have an anecdote about learning on these machines:\n\nI got a Commodore 64 when I was 12, my grandma found it at a garage sale and knew I liked computers. So, I flipped it on and saw a cursor blinking. And that was all. Just a cursor. Mind you, we already had \"IBM 486\" computers by then, so I was already used to Windows opening and hadn't messed around with DOS much.\n\nThe reason this is important is because it made me \\*want\\* to learn how to use it and how to use it was BASIC commands. I wanted it to do something and that was usually the biggest hurdle to my learning them: the drive and impulse.", "Score": 1, "Replies": []}, {"Comment": "I started out in the early 80s as a preteen on the TRS-80 color computer. My choices were basic and 6809 assembly language.\n\nMy interest was games, and after making several programs in basic and a few attempts at some games, I eventually realized that basic wasn't going to cut it, so I started studying 6809 assembly language.\n\n>Do you agree, or do you think that Scratch and Python offers more advantages?\n\nThose older machines were valuable learning tools. But I think more modern languages are far better than the built in basic interpreters of the day. There wasn't a text editor really, you had to specify line numbers and work one line at a time. It was pretty crazy.", "Score": 1, "Replies": []}, {"Comment": "I see there is no way to compare these. We are talking completely different contexts, business momentum, hardware specification and complexity of the code.\n\nWe can endlessly discuss this, but it's founded in relative opinions of old farts like me.", "Score": 1, "Replies": []}, {"Comment": "I used the zx81, amstrad cpc 464, and atari 512 st. I started with basic. The gfa basic was pretty good. But what I used most on those machines was assembly that's how I learned computer science.\n\nThe main difference is that at the time, if you wanted to do something with your computer other than gaming, you had to learn programming and basic was embedded by default. Today you don't need to learn python to use a computer.", "Score": 1, "Replies": []}, {"Comment": "I grew up with a ZX Spectrum and learnt programming with it: BASIC, Z80 assembly, FORTH and even micro-Prolog!\n\nI think the biggest shift when comparing with a modern machine and languages is that the 8-bit machines were simple enough that one person could just about fully understand the connection between hardware and software. This is no longer possible.", "Score": 1, "Replies": []}, {"Comment": "There are times when I think it\u2019d be useful to build a computer and an operating system that, when you turn it on, pretty much dumps you into a code editor and requires you to either load a program to run it, or to write your own program\u2014just as a learning tool.", "Score": 4, "Replies": [{"Reply": "TempleOS", "Reply Score": 4}, {"Reply": "Thats sort of how I started learning low-level programming.\n\nBack in the day I had a Sega Dreamcast. There was a boot loader available where it would read your program from CD and load it into memory and run it. The framebuffer was a fixed physical address so you could directly set pixels.\n\n\nLater on Sony had the OtherOS feature in the PS3 which would do the same thing. Luckily you could load the code from a USB stick instead of CD.", "Reply Score": 1}]}, {"Comment": ">\"built-in code editor\" : I'd like to hear of some examples.\n\nThis started to show up by the DOS era, e.g. QBASIC.", "Score": 1, "Replies": []}, {"Comment": "I had gotten my start with programming on the Commodore 64, and was about to make a similar comment.  Commodore 64 BASIC required developing an intimate understanding of how the hardware works - largely because it was missing many useful commands.  The benefit is that you can understand what many of the memory addresses were used for and how to do bit manipulation.  The drawback is that a lot of it was difficult to figure out - like how hard it was to use a bitmap graphics mode.\n\nModern languages, like Python, can allow someone to do a lot more with less typing, but it is all hidden magic.  There is a whole generation of programmers who don't have a strong understanding of how things work, but instead rely on memorizing how black boxes are used.  There are a large number of programmers who have only used JavaScript.\n\nIt definitely changes how one thinks about programming - especially optimization.  Now, unoptimized and wasteful code executes quickly and the programmer is unaware of what is going on under the surface.  With 1980s computers, they were so slow that small optimizations could have a huge impact on performance.  To a modern programmer, it would seem impossible to do anything with hardware that has a 2MHz CPU and only 64KB of RAM.  It would seem unimaginable to someone who doesn't think twice about simple applications using hundreds of megabytes of RAM.\n\nI think someone interested in computer science should have low level experience - not because they would likely get hired to write assembly code, but because it provides a different perspective on programming.\n\nI'm not sure if low level languages is best for an introduction to computer programming, though - especially with kids, who are easily confused and quickly get bored.  Being able to quickly do something fun helps with maintaining motivation and not feeling overwhelmed.  So, it's hard to know what would be better for education.  Maybe both should be taught.", "Score": 6, "Replies": []}, {"Comment": "I would agree with that. \n\nThe simple computer architecture and esp, Peek and Poke meant that as a kid ( I was like 8 ), I could dip my toes into machine code. \nI was self taught and managed to write a two player snake game on the C64 in machine code at about 11 years old. \nI can still remember it. \nI wrote basic, which would then poke the assembly program into the right place and run it. \n\nThis was only possible because you could mix and match basic with the lower level - so I could learn over time. And of course the sound was done using poke\u2026 which is a great intro to the memory map.", "Score": 2, "Replies": []}, {"Comment": "From C16/plus4 perspective TEDmon was so great that I never used peek or poke in BASIC.  SYS is the only command I need. Later on PC I used C. It was faster and less constrained than BASIC and I stopped writing assembler although the linker in C encourages a assembler modules..", "Score": 1, "Replies": []}, {"Comment": "While that may be true for dialects like Commodore BASIC and others, there were also dialects like Amiga Basic (1985\u20131988) that supported structured programming just fine.", "Score": 3, "Replies": []}, {"Comment": "You only need structure for a large program. My programs fit on screen. Then I moved to a PC. Same with assembly. So it can only be an introduction.", "Score": 3, "Replies": []}, {"Comment": "https://pythonsandbox.com/turtle", "Score": 5, "Replies": []}, {"Comment": "For a lot of these old machines, BASIC was effectively used as the operating system.  It was impossible to use these machines without using at least a little BASIC - even if it was just the LOAD and RUN commands.", "Score": 2, "Replies": []}, {"Comment": "Python can\u2019t even do multi-line lambdas. I like indent, but I would propose Kotlin or Java or C# or rust or swift. I don\u2019t get the fuss about Go or Dart.", "Score": -2, "Replies": [{"Reply": "Is there a BASIC that actually implements lambda?", "Reply Score": 1}]}, {"Comment": "BASIC allows you to draw on the screen with a single line of code.", "Score": 5, "Replies": [{"Reply": "Drawing on the screen! A great tool for education.\n\nNot BASIC, but see slide 31 here: [https://codeguppy.com/site/download/coding\\_course.pdf](https://codeguppy.com/site/download/coding_course.pdf)", "Reply Score": 4}, {"Reply": "The undergrad CS program at the university I'm at uses Python, the Turtle library, and Thonny in the absolute beginner programming course to good effect.", "Reply Score": 2}, {"Reply": "Yes.\n\nHow can it be of use in 2024?", "Reply Score": 1}, {"Reply": "I hate scratch where I you have to be careful to hit the screen. Old computers had the well known 160x192 range which will surely end up on my CRT.", "Reply Score": 1}]}, {"Comment": "TempleOS", "Score": 4, "Replies": []}, {"Comment": "Thats sort of how I started learning low-level programming.\n\nBack in the day I had a Sega Dreamcast. There was a boot loader available where it would read your program from CD and load it into memory and run it. The framebuffer was a fixed physical address so you could directly set pixels.\n\n\nLater on Sony had the OtherOS feature in the PS3 which would do the same thing. Luckily you could load the code from a USB stick instead of CD.", "Score": 1, "Replies": []}, {"Comment": "Is there a BASIC that actually implements lambda?", "Score": 1, "Replies": [{"Reply": "VB.NET", "Reply Score": 1}]}, {"Comment": "Drawing on the screen! A great tool for education.\n\nNot BASIC, but see slide 31 here: [https://codeguppy.com/site/download/coding\\_course.pdf](https://codeguppy.com/site/download/coding_course.pdf)", "Score": 4, "Replies": []}, {"Comment": "The undergrad CS program at the university I'm at uses Python, the Turtle library, and Thonny in the absolute beginner programming course to good effect.", "Score": 2, "Replies": []}, {"Comment": "Yes.\n\nHow can it be of use in 2024?", "Score": 1, "Replies": [{"Reply": "I think that kind of rapid, visual feedback is a huge thing for teaching people programming. Whether it's a Turtle on just 2D primitives, I think intro-to-programming languages should be focused more on the *way they expose the results of programming* than the language syntax itself.\n\nLearning programming languages is easy. I'd argue it's the most trivial part of programming. Making programming compelling and accessible without needing a love of abstract models like Fibonacci sequences is the hardest part of teaching programming.", "Reply Score": 8}]}, {"Comment": "I hate scratch where I you have to be careful to hit the screen. Old computers had the well known 160x192 range which will surely end up on my CRT.", "Score": 1, "Replies": []}, {"Comment": "VB.NET", "Score": 1, "Replies": []}, {"Comment": "I think that kind of rapid, visual feedback is a huge thing for teaching people programming. Whether it's a Turtle on just 2D primitives, I think intro-to-programming languages should be focused more on the *way they expose the results of programming* than the language syntax itself.\n\nLearning programming languages is easy. I'd argue it's the most trivial part of programming. Making programming compelling and accessible without needing a love of abstract models like Fibonacci sequences is the hardest part of teaching programming.", "Score": 8, "Replies": [{"Reply": "Drawing on the screen is valid concept. It really that ticks with children. That\u2019s why Roblox is a thing. That\u2019s why they desperately try to learn Unity. \n\nSo you can create library and have environment where you will have canvas screen and let children play with that canvas. \n\nI for example take existing library, translate it to Ukrainian and Russian and now you can speak with children closer to your mother tongue and don\u2019t have to ask them translate in their head. Anyway point is that this one library away in Python. \n\nIf you by a chance interested in my experiment here the link. https://github.com/kant2002/funsharp (f#)", "Reply Score": 1}, {"Reply": "This is Python code that draws a line:\n\n    import tkinter as tk\n\ndef draw_line(x1, y1, x2, y2):\n    # Create a top-level widget\n    root = tk.Tk()\n    root.title(\"Line Drawing\")\n\n    # Create a canvas widget\n    canvas = tk.Canvas(root, width=400, height=300)\n    canvas.pack()\n\n    # Draw a line on the canvas using user-provided coordinates\n    canvas.create_line(x1, y1, x2, y2, fill=\"blue\", width=5)\n\n    # Start the GUI event loop\n    root.mainloop()\n\n# Get user input for coordinates\nx1 = int(input(\"Enter starting x-coordinate: \"))\ny1 = int(input(\"Enter starting y-coordinate: \"))\nx2 = int(input(\"Enter ending x-coordinate: \"))\ny2 = int(input(\"Enter ending y-coordinate: \"))\n\ndraw_line(x1, y1, x2, y2)\n\nAlthough it is a bit more complicated than that in BASIC, it teaches many important concepts of modern programming and, with a little modification, can be used to draw plots from Excel tables.", "Reply Score": 1}]}, {"Comment": "Drawing on the screen is valid concept. It really that ticks with children. That\u2019s why Roblox is a thing. That\u2019s why they desperately try to learn Unity. \n\nSo you can create library and have environment where you will have canvas screen and let children play with that canvas. \n\nI for example take existing library, translate it to Ukrainian and Russian and now you can speak with children closer to your mother tongue and don\u2019t have to ask them translate in their head. Anyway point is that this one library away in Python. \n\nIf you by a chance interested in my experiment here the link. https://github.com/kant2002/funsharp (f#)", "Score": 1, "Replies": [{"Reply": "Ooh. And writing simple arcade games from 80s is more than enough to both show programming and entertain.", "Reply Score": 2}]}, {"Comment": "This is Python code that draws a line:\n\n    import tkinter as tk\n\ndef draw_line(x1, y1, x2, y2):\n    # Create a top-level widget\n    root = tk.Tk()\n    root.title(\"Line Drawing\")\n\n    # Create a canvas widget\n    canvas = tk.Canvas(root, width=400, height=300)\n    canvas.pack()\n\n    # Draw a line on the canvas using user-provided coordinates\n    canvas.create_line(x1, y1, x2, y2, fill=\"blue\", width=5)\n\n    # Start the GUI event loop\n    root.mainloop()\n\n# Get user input for coordinates\nx1 = int(input(\"Enter starting x-coordinate: \"))\ny1 = int(input(\"Enter starting y-coordinate: \"))\nx2 = int(input(\"Enter ending x-coordinate: \"))\ny2 = int(input(\"Enter ending y-coordinate: \"))\n\ndraw_line(x1, y1, x2, y2)\n\nAlthough it is a bit more complicated than that in BASIC, it teaches many important concepts of modern programming and, with a little modification, can be used to draw plots from Excel tables.", "Score": 1, "Replies": []}, {"Comment": "Ooh. And writing simple arcade games from 80s is more than enough to both show programming and entertain.", "Score": 2, "Replies": []}]},{"Title": "How related is compiler to a person interested in computer security?", "Score": 0, "URL": "https://www.reddit.com/r/compsci/comments/193v5yw/how_related_is_compiler_to_a_person_interested_in/", "CreatedAt": 1704954862.0, "Full Content": "IE., is a compiler class crucial for people who want to pursue graduate studies in security?  \nMy gut is wrenching for not taking it but I didn't understand anything first-day at that course (front end compiler).", "CntComments": 7, "Comments": [{"Comment": "This is really going to depend on what part of the extremely wide field of security you want to focus on. There are sections of computer security where you would be better suited taking a screenwriting class than a compilers class, and there are other areas where taking a compiler course is vital. \n\nI've personally been working in different areas of computer security for over 20 years, and I've barely touched on what I learned in my compilers class back in the day; your future milage may greatly vary.", "Score": 11, "Replies": []}, {"Comment": "[Reflections on Trusting Trust](https://www.reddit.com/r/linux/comments/xqzqu2/ken_thompson_reflections_on_trusting_trust_turing/) is what you're looking for. Sorry, but lots of bad news for your gut, and probably your sleep, as well... :\\", "Score": 7, "Replies": []}, {"Comment": "If you want to do software security, yes. This is the class where you learn the theory and implementation of basic static analysis.", "Score": 2, "Replies": []}, {"Comment": "What part of computer security are you interested in? Cryptography? Packet sniffing? Security policies, designing infrastructure like Active Directory and Kerberos to handle authentication and authorization in a complicated environment? Maybe compiler theory will be of less use to you. If you're interest in reverse engineering or malware analysis or binary exploitation, where you'll be reading a lot of machine instructions generated by compilers, then some understanding of what that compiler is doing may be invaluable for you.", "Score": 2, "Replies": []}, {"Comment": "There are a few ways in which a good knowledge of compilers can be relevant. One is knowing that a malicious compiler can insert malware of any kind into the program it is compiling (and inspecting the source code of the compiler would not reveal that this is happening, you would have to read the object code of the compiler). A second aspect is knowing how a compiler can generate code in a style which is much harder for a virus or hacker to modify. And finally knowledge of how a compiler analyzes source code is useful if you want to develop tools which scan source code or object code for malware.", "Score": 1, "Replies": []}, {"Comment": "I e worked I. A wide array of IT Security for over 20 years,\u2026learn every facet of every relatable portion of IT that you can handle. You never know when a the lump of coal you\u2019re doing right now will make you sparkle like a diamond. \u201cOh, I didn\u2019t know that you knew how to do that\u201d, \u201cyea bitch, and the more you pay me, the more I\u2019ll impress you\u201d.", "Score": 1, "Replies": []}, {"Comment": "It really depends on what kind of computer security you plan on focusing on. It certainly helps to know how an executable is compiled if you want to reverse engineer them.\n\nPersonally, I found my undergrad compiler class to be one of the most rewarding of my entire undergrad experience, so i might be biased.", "Score": 1, "Replies": []}]},{"Title": "Increasing confidence in your software with formal verification", "Score": 4, "URL": "https://www.stackbuilders.com/blog/increasing-confidence-in-your-software-with-formal-verification/", "CreatedAt": 1704914709.0, "Full Content": "", "CntComments": 1, "Comments": [{"Comment": "I doubt the blogger has even tried TLA+, Coq, or even knows what formal verification is.", "Score": 1, "Replies": []}]},{"Title": "Pure, Mixed, and Entangled Quantum States (article + Jupyter notebook)", "Score": 0, "URL": "https://www.reddit.com/r/compsci/comments/193pkt7/pure_mixed_and_entangled_quantum_states_article/", "CreatedAt": 1704937271.0, "Full Content": "https://medium.com/@emilmarinov/pure-mixed-and-entangled-quantum-states-84e8a4a8dd16", "CntComments": 0, "Comments": []},{"Title": "Non-Functional Software Requirements - Guide", "Score": 14, "URL": "https://www.reddit.com/r/compsci/comments/1933g9x/nonfunctional_software_requirements_guide/", "CreatedAt": 1704874918.0, "Full Content": "While functional requirements define the \u201cwhat\u201d of software, non-functional requirements define how well it accomplishes its tasks. The following guide explains how these qualities ensures your software meets user expectations: [Why are Non-Functional Requirements Important - Guide](https://www.codium.ai/blog/why-are-non-functional-requirements-important/)\n\n* Scalability\n* Performance\n* Security\n* Usablity\n* Reliability", "CntComments": 2, "Comments": [{"Comment": "[deleted]", "Score": 1, "Replies": [{"Reply": "A very wise and meaningful advice, thanks a lot for sharing such a detailed review, it actually should be considered and even more important as comparing to all other stuff listed in the article.", "Reply Score": 2}]}, {"Comment": "Not even wrong...", "Score": 1, "Replies": []}, {"Comment": "A very wise and meaningful advice, thanks a lot for sharing such a detailed review, it actually should be considered and even more important as comparing to all other stuff listed in the article.", "Score": 2, "Replies": []}]},{"Title": "We can solve 3-SAT \"quickly\" by applying it to superposed booleans on interaction nets", "Score": 25, "URL": "https://twitter.com/VictorTaelin/status/1744788091833917696", "CreatedAt": 1704825922.0, "Full Content": "", "CntComments": 30, "Comments": [{"Comment": "I presume that there's some pathological cases that mean that this is still exponential in the general case, but this is still quite a cool feature! Getting something like DPLL for free is neat.", "Score": 17, "Replies": [{"Reply": "I think it's mostly bullshit to fabricate hype for the author's HVM project. It's a pathologically contrived example. If you want to learn about interaction nets, scholar.google.com can find a lot of great reading material without the unnecessary bullshit/marketing.", "Reply Score": 2}]}, {"Comment": "Inets are cool. Can you run a benchmark against some threshold random SAT instances versus a [state-of-the-art SAT solver](https://satcompetition.github.io/2023/results.html)? It would be interesting just to see how competitive your solver can be against one of these monsters, and, who knows, maybe inets are a future technique which could be introduced into competition?", "Score": 11, "Replies": [{"Reply": "TBH I'd be extremely surprised if it was comparable to state-of-the-art, let alone competitive. This is a very simple algorithm.", "Reply Score": 7}]}, {"Comment": "Did the brute force version do anything like lookahead or was it literally just trying every possible value of boolean assignments?\n\nI'm not at all familiar with interaction nets, but did my PhD in Constraint Processing (specifically constraint satisfaction problems), which are *very* closely related to 3-SAT in terms of problem formulation (i.e., it's pretty trivial to compose a CSP based on a given 3-SAT).  \n\nFrom a very cursory overview of interaction nets on Wikipedia, it seems like they sort of model parallel solving of unrelated portions of a problem?  If so, I would wager that this method falls apart pretty quickly as problem size scales up, but again, I've never heard of interaction nets prior to this post.", "Score": 6, "Replies": [{"Reply": "I don't know, this is just a quick note in an effect I observed in the same day I posted it. I still have little information on why this algorithm behaves so much better than brute-force. It is also not trivial to figure out, given that interaction net execution can result in extremely complex and interwired graph expansions that are hard to analyze, in a similar way to neural nets, but for different reasons. I wish I had more info to share, but, for now, this post should be seen as a quick note about an observed effect, rather than a through analysis.", "Reply Score": 0}]}, {"Comment": "Can this be converted to C/C++ code and benchmarked with long list of terms(preferably in the millions)?", "Score": 2, "Replies": [{"Reply": "Hi! I'm the person in the screenshot of the linked tweet.\n\nThis is *already* as performant as the dumb algorithm can get. It's using the fastest available runtime for Interaction Nets, which is the model of computation that allows the program to execute quickly. \n\nIt's hard or impossible to \"convert it\" to C/C++ code, because CPUs can't natively reduce interaction nets without a runtime. The best available solution to reduce Interaction Nets is the current runtime, which is a Rust program that reduces nets using multiple cores and an atomic link procedure.\n\nThat being said, it might be possible to manually \"compile\" some parts of the reduction, which might make them execute faster. But that would be an unnecessarily complex task. But writing a complex program is sort of pointless, because the point of this demonstration is showing that HVM can execute dumb, simple algorithms (in this case, SAT brute-force) very quickly using methods like the one described in the tweet.", "Reply Score": 7}, {"Reply": "Um no.", "Reply Score": -5}]}, {"Comment": "Cool application (even though it's useless in practice), but totally misleading title.\n\nAlso there are way better alternatives for solving constraint satisfaction problems, which also require writing just a few lines of code, such as answer set programming.", "Score": 2, "Replies": []}, {"Comment": "I'm not sure how \"superposed values\" are supposed to work at the hardware level to bring the claimed performance boost?\n\nWhile I can imagine some SIMD processor for some types of programs that perform loops on the superposed values, this would break in a program that has non-terminal recursion, demanding exponentially more resources to compute a function with \"superposed values\"?", "Score": 3, "Replies": [{"Reply": "It is not a hardware thing, it just allows expressions/computations to be dynamically recycled across different function calls. It is also fully lazy. If you're familiar with how Haskell/GHC works, it is like that but on steroids.", "Reply Score": 6}]}, {"Comment": "LOL. I needed a laugh.", "Score": -2, "Replies": [{"Reply": "wdym?", "Reply Score": 1}]}, {"Comment": "I think it's mostly bullshit to fabricate hype for the author's HVM project. It's a pathologically contrived example. If you want to learn about interaction nets, scholar.google.com can find a lot of great reading material without the unnecessary bullshit/marketing.", "Score": 2, "Replies": []}, {"Comment": "TBH I'd be extremely surprised if it was comparable to state-of-the-art, let alone competitive. This is a very simple algorithm.", "Score": 7, "Replies": [{"Reply": "That's not really the point. The point is to benchmark it, that is, measure how well it *does* perform as a percentage of (known) achievable performance. \"It's better than exhaustive search\" is not very informative, but \"it's 40% of achievable SOTA performance\" is informative.", "Reply Score": 13}]}, {"Comment": "I don't know, this is just a quick note in an effect I observed in the same day I posted it. I still have little information on why this algorithm behaves so much better than brute-force. It is also not trivial to figure out, given that interaction net execution can result in extremely complex and interwired graph expansions that are hard to analyze, in a similar way to neural nets, but for different reasons. I wish I had more info to share, but, for now, this post should be seen as a quick note about an observed effect, rather than a through analysis.", "Score": 0, "Replies": []}, {"Comment": "Hi! I'm the person in the screenshot of the linked tweet.\n\nThis is *already* as performant as the dumb algorithm can get. It's using the fastest available runtime for Interaction Nets, which is the model of computation that allows the program to execute quickly. \n\nIt's hard or impossible to \"convert it\" to C/C++ code, because CPUs can't natively reduce interaction nets without a runtime. The best available solution to reduce Interaction Nets is the current runtime, which is a Rust program that reduces nets using multiple cores and an atomic link procedure.\n\nThat being said, it might be possible to manually \"compile\" some parts of the reduction, which might make them execute faster. But that would be an unnecessarily complex task. But writing a complex program is sort of pointless, because the point of this demonstration is showing that HVM can execute dumb, simple algorithms (in this case, SAT brute-force) very quickly using methods like the one described in the tweet.", "Score": 7, "Replies": []}, {"Comment": "Um no.", "Score": -5, "Replies": []}, {"Comment": "It is not a hardware thing, it just allows expressions/computations to be dynamically recycled across different function calls. It is also fully lazy. If you're familiar with how Haskell/GHC works, it is like that but on steroids.", "Score": 6, "Replies": [{"Reply": "in a word it's hype", "Reply Score": -5}]}, {"Comment": "wdym?", "Score": 1, "Replies": [{"Reply": "I can't tell what you mean, but I'll explain a bit.\n\n* \" I have no idea what its asymtotics are\" This is a key indicator of a bogus result for any np-complete problem.\n* People have long solved very big 3-SAT instances. Being hard somewhere is different than being hard everywhere. 3-SAT is NP-complete because it's hard somewhere, not everywhere.\n* If they don't know asymptotics, they with very high probability don't know how to define a random 3-sat problem instance problem properly. Randomness is not to be toyed with, and probably the most misunderstood thing I see among newbies in theory. \n\n&#x200B;\n\nJust for the record, we know that SAT is easy depending upon the ratio of clauses to variables.  Intuition:\n\n* If you have a high number of variables to clauses, then easy and most likely SAT. Why?  Many unconstrained variables (most extreme case is each clause has entirely new variables) means unlikely to find a conflict, and almost any assignment is SAT.\n* If the number of variables to clauses is very low, then it's also easy and likely unsat.  Why? Chances are you will find easily conflicting clauses where you have A and \\\\not A being required. \n* Somewhere in the middle is hard. You end up with lots of backward propagation as you find an assignment of values to variables conflicts.  Classic paper (but not the latest result) is in the journal Science behind a paywall; free link here: [https://aiichironakano.github.io/cs653/Mezard-RSAT-Science02.pdf](https://aiichironakano.github.io/cs653/Mezard-RSAT-Science02.pdf)\n\nBasically this entire post is like someone saying \"I solved the Riemann hypothesis\" on twitter. Could be, but so unlikley it's not even worth digging in.", "Reply Score": -12}]}, {"Comment": "That's not really the point. The point is to benchmark it, that is, measure how well it *does* perform as a percentage of (known) achievable performance. \"It's better than exhaustive search\" is not very informative, but \"it's 40% of achievable SOTA performance\" is informative.", "Score": 13, "Replies": [{"Reply": "I did provide a simple benchmark though, it runs in 3 minutes in a Rust brute-forcer with loops, vs 1 second in HVM with superpositions. Obviously that is just a single example. It would be cool to benchmark against existing solutions, but I'd expect it to be drastically slower.", "Reply Score": -2}]}, {"Comment": "in a word it's hype", "Score": -5, "Replies": []}, {"Comment": "I can't tell what you mean, but I'll explain a bit.\n\n* \" I have no idea what its asymtotics are\" This is a key indicator of a bogus result for any np-complete problem.\n* People have long solved very big 3-SAT instances. Being hard somewhere is different than being hard everywhere. 3-SAT is NP-complete because it's hard somewhere, not everywhere.\n* If they don't know asymptotics, they with very high probability don't know how to define a random 3-sat problem instance problem properly. Randomness is not to be toyed with, and probably the most misunderstood thing I see among newbies in theory. \n\n&#x200B;\n\nJust for the record, we know that SAT is easy depending upon the ratio of clauses to variables.  Intuition:\n\n* If you have a high number of variables to clauses, then easy and most likely SAT. Why?  Many unconstrained variables (most extreme case is each clause has entirely new variables) means unlikely to find a conflict, and almost any assignment is SAT.\n* If the number of variables to clauses is very low, then it's also easy and likely unsat.  Why? Chances are you will find easily conflicting clauses where you have A and \\\\not A being required. \n* Somewhere in the middle is hard. You end up with lots of backward propagation as you find an assignment of values to variables conflicts.  Classic paper (but not the latest result) is in the journal Science behind a paywall; free link here: [https://aiichironakano.github.io/cs653/Mezard-RSAT-Science02.pdf](https://aiichironakano.github.io/cs653/Mezard-RSAT-Science02.pdf)\n\nBasically this entire post is like someone saying \"I solved the Riemann hypothesis\" on twitter. Could be, but so unlikley it's not even worth digging in.", "Score": -12, "Replies": [{"Reply": "I see. Based on that answer, I suspect you may have completely misinterpreted the post. The post is merely showing an optimization trick in the context of interaction nets; specifically, I show how fan-nodes can be used to make a brute-force search faster by a few orders of magnitude. At no point it is claimed to be a general polynomial-time solution to SAT, much less claiming to solve any similarly hard problem like the Rianmann hypothesis. I even make it clear that I don't know the asymptotics of *this* algorithm - which is not necessary to the argument, and completely different from not knowing how asymptotics work. \n\nEvery point you raised is true, yet completely irrelevant, because they have nothing to do with the post. I suspect you didn't even read it, because you're counter-arguing against points that were never made. Have you? It also seems like you're fairly new to these concepts in general, since you can't tell the difference between claiming that a brute-force solution can be sped up in a specific context, versus claiming that the problem can be solved in polynomial time, which are grossly different claims.", "Reply Score": 28}, {"Reply": "Lol at the downvotes. You are spot on, don't delete your comment, it's highly educational", "Reply Score": 2}]}, {"Comment": "I did provide a simple benchmark though, it runs in 3 minutes in a Rust brute-forcer with loops, vs 1 second in HVM with superpositions. Obviously that is just a single example. It would be cool to benchmark against existing solutions, but I'd expect it to be drastically slower.", "Score": -2, "Replies": [{"Reply": "That\u2019s not a meaningful benchmark.", "Reply Score": 5}]}, {"Comment": "I see. Based on that answer, I suspect you may have completely misinterpreted the post. The post is merely showing an optimization trick in the context of interaction nets; specifically, I show how fan-nodes can be used to make a brute-force search faster by a few orders of magnitude. At no point it is claimed to be a general polynomial-time solution to SAT, much less claiming to solve any similarly hard problem like the Rianmann hypothesis. I even make it clear that I don't know the asymptotics of *this* algorithm - which is not necessary to the argument, and completely different from not knowing how asymptotics work. \n\nEvery point you raised is true, yet completely irrelevant, because they have nothing to do with the post. I suspect you didn't even read it, because you're counter-arguing against points that were never made. Have you? It also seems like you're fairly new to these concepts in general, since you can't tell the difference between claiming that a brute-force solution can be sped up in a specific context, versus claiming that the problem can be solved in polynomial time, which are grossly different claims.", "Score": 28, "Replies": [{"Reply": "> At no point it is claimed to be a general polynomial-time solution to SAT, much less claiming to solve any similarly hard problem\n\nThe title literally says \"We can solve 3-SAT\"\n\nAnd the post says 32 variables. SAT solvers routinely solve instances with thousands of variables.\n\nThis may be a cool thing, but the presentation is disingenuous at best.", "Reply Score": 3}, {"Reply": "First, this post shows you are learning something new, which is awesome!  no problem there. \n\nThe problem is in your presentation that the thing you just learned about is somehow better than other things.\n\nYou are right; I didn't read it. The obligation isn't on the reader to figure out why a crazy headline is true.\n\nTLDR: you're eye-catching headline is \"So, we can solve 3-SAT quickly\", and your article doesn't show this. It also under-defines terms, and lacks enough rigor to make any claim about 3-SAT solving time.\n\nHigh level suggestion: rephrase your post as \"Writing a simple SAT solver with HVM is easy and with reasonable performance.\"\n\nYou implemented two algorithms and one was faster than the other on a dataset you created. This is a classic blunder: you need to establish each one of these things are reasonable and representative.\n\n&#x200B;\n\nNit:\n\n\\> I show how fan-nodes can be used to make a brute-force search faster by a few orders of magnitude.\n\nA search of the article shows \"fan\" is mentioned 0 times, so you've not quite said what you think you've said.\n\nI've now read the article, and I'll point even after some scrutiny why I have the same opinion.\n\n\\> \"one-line\" SAT solver on Interaction Nets can outperform brute-force by orders of magnitude\n\nOk, technically you qualify this as you were amazed. However, it comes across as though inets are superior to brute force.\n\nSome more:\n\n* The article talks about superposed values, but doesn't relate that to a normal model of computation. A quick glance at HVM makes it seem like it's parallel computing. You talk about the operations, but do not give an intuition on when an HVM model of computation is superior. Certainly the no free lunch theorem would say it can't always be so.\n* You don't really define brute force, so I can only guess here. Are you comparing multiple threaded inet against single-thread brute force on multiple cores? What if you ran both on a single core with a single thread?  This would offer a ton more insight.\n* Random isn't defined. As my post above shows, this is a tricky subject. SAT solvers, by definition, are heuristics optimized for particular types of sat instances.\n* You don't run multiple experiments.\n* You don't show your brute force algorithm. You can be brute force and still do memoization, for example. Did you?\n* \\> \\`F(x) = x \\* 10\\`, then \\`F({1 2 3 4})\\` is seen as \\`{F(1) F(2) F(3) F(4)}\\`, which reduces to \\`{10 20 30 40}\\`? How is this different than a map of memoized values? That's the obvious question the reader is left with.\n* \\`(1234 \\^ 4321) % 2\\` would take a long time to compute. No, it doesn't. It's 0. An even number to any power > 0 is even, thus 0 mod 2. Suppose you're trying to talk about modular exponentiation more generally? Are you talking about optimized square-and-multiply using sliding windows with karatsuba multiplication and montgomery reductions, or something else? Generally we'd see something like this is O(log(\ud835\udc5a))2\u22c5log(\ud835\udc5b) for base m and exponent n.  Your article implies that somehow inets are faster.  (I'm assuming this is just a bad example where you are saying memoization is good.)\n\nTons of stuff like this.\n\nHope this helps!", "Reply Score": 0}]}, {"Comment": "Lol at the downvotes. You are spot on, don't delete your comment, it's highly educational", "Score": 2, "Replies": []}, {"Comment": "That\u2019s not a meaningful benchmark.", "Score": 5, "Replies": [{"Reply": "... but I'm already telling you will probably be drastically slower, why you want to benchmark against SAT solvers? The point of this post is how this specific feature can be used to speed up brute-force in general, not how to construct a competitive SAT solver. I don't get how a benchmark would aggregate to that point?", "Reply Score": 1}]}, {"Comment": "> At no point it is claimed to be a general polynomial-time solution to SAT, much less claiming to solve any similarly hard problem\n\nThe title literally says \"We can solve 3-SAT\"\n\nAnd the post says 32 variables. SAT solvers routinely solve instances with thousands of variables.\n\nThis may be a cool thing, but the presentation is disingenuous at best.", "Score": 3, "Replies": [{"Reply": "... which is correct? Being able to solve instances of something faster than brute-force (which is what the entire post is about) is completely different from being able to solve it faster than state-of-art techniques, let alone in polynomial time, which was never written, mentioned, not even hinted in any possible way through the entire post. I can't even comprehend how anyone would interpret it so drastically differently than what was objectively written. Does everyone just read headlines and then extrapolate with their own words?", "Reply Score": 3}]}, {"Comment": "First, this post shows you are learning something new, which is awesome!  no problem there. \n\nThe problem is in your presentation that the thing you just learned about is somehow better than other things.\n\nYou are right; I didn't read it. The obligation isn't on the reader to figure out why a crazy headline is true.\n\nTLDR: you're eye-catching headline is \"So, we can solve 3-SAT quickly\", and your article doesn't show this. It also under-defines terms, and lacks enough rigor to make any claim about 3-SAT solving time.\n\nHigh level suggestion: rephrase your post as \"Writing a simple SAT solver with HVM is easy and with reasonable performance.\"\n\nYou implemented two algorithms and one was faster than the other on a dataset you created. This is a classic blunder: you need to establish each one of these things are reasonable and representative.\n\n&#x200B;\n\nNit:\n\n\\> I show how fan-nodes can be used to make a brute-force search faster by a few orders of magnitude.\n\nA search of the article shows \"fan\" is mentioned 0 times, so you've not quite said what you think you've said.\n\nI've now read the article, and I'll point even after some scrutiny why I have the same opinion.\n\n\\> \"one-line\" SAT solver on Interaction Nets can outperform brute-force by orders of magnitude\n\nOk, technically you qualify this as you were amazed. However, it comes across as though inets are superior to brute force.\n\nSome more:\n\n* The article talks about superposed values, but doesn't relate that to a normal model of computation. A quick glance at HVM makes it seem like it's parallel computing. You talk about the operations, but do not give an intuition on when an HVM model of computation is superior. Certainly the no free lunch theorem would say it can't always be so.\n* You don't really define brute force, so I can only guess here. Are you comparing multiple threaded inet against single-thread brute force on multiple cores? What if you ran both on a single core with a single thread?  This would offer a ton more insight.\n* Random isn't defined. As my post above shows, this is a tricky subject. SAT solvers, by definition, are heuristics optimized for particular types of sat instances.\n* You don't run multiple experiments.\n* You don't show your brute force algorithm. You can be brute force and still do memoization, for example. Did you?\n* \\> \\`F(x) = x \\* 10\\`, then \\`F({1 2 3 4})\\` is seen as \\`{F(1) F(2) F(3) F(4)}\\`, which reduces to \\`{10 20 30 40}\\`? How is this different than a map of memoized values? That's the obvious question the reader is left with.\n* \\`(1234 \\^ 4321) % 2\\` would take a long time to compute. No, it doesn't. It's 0. An even number to any power > 0 is even, thus 0 mod 2. Suppose you're trying to talk about modular exponentiation more generally? Are you talking about optimized square-and-multiply using sliding windows with karatsuba multiplication and montgomery reductions, or something else? Generally we'd see something like this is O(log(\ud835\udc5a))2\u22c5log(\ud835\udc5b) for base m and exponent n.  Your article implies that somehow inets are faster.  (I'm assuming this is just a bad example where you are saying memoization is good.)\n\nTons of stuff like this.\n\nHope this helps!", "Score": 0, "Replies": [{"Reply": "> First, this post shows you are learning something new, which is awesome! \n\nLearning new things is awesome indeed, and sharing them is even better. I opted to share these notes because they do represent new techniques that are absolutely not published anywhere; specifically, how to use fan-nodes to optimize brute-force. That said, I didn't have the time (nor interest) to dig deeply into this topic, much less to perform a deeper analysis of this effect - I'm busy with many things - yet, I still decided to post the bits of information I had, for these who could find it useful. If I had done a more extensive analysis, I'd be publishing a proper paper, not a Gist with some quick notes, right?\n\n> Writing a simple SAT solver with HVM is easy and with reasonable performance.\n\nGiven how some people extrapolate the meaning of objective claims, that'd be a better title, I agree.\n\n> A search of the article shows \"fan\" is mentioned 0 times\n\nLook for dup-nodes, which is a synonym of fan-nodes in the context of interaction nets.\n\n> You don't show your brute force algorithm.\n\nThe complete code and algorithm is in the post.\n\n> However, it comes across as though inets are superior to brute force.\n\nIt **is** superior to brute-force, while also not being comparable to state-of-art algorithms. Both claims are compatible.\n\n> Are you comparing multiple threaded inet against single-thread brute force on multiple cores?\n\nBoth tests run in single-core. Randomness is irrelevant to the post, which is more focused in showing the technique than fully analyzing it.\n\n> You don't run multiple experiments.\n\nDefinitely running multiple experiments, a comprehensive benchmark suite and performance analysis would make the post much richer. That said, it wasn't in the scope of this quick Gist note.\n\n> How is this different than a map of memoized values?\n\nIn this case, it is similar. In others, it is completely different. For example, INet's model allow you perform repeated application (`F^n(x)`) for certain classes of functions in `log(N)`, which, here, does asymptotically outperform state-of-art alternatives. This is well described in the interaction net literature, and I've written about it in the past.\n\n> `(1234 ^ 4321) % 2` would take a long time to compute. No, it doesn't. It's 0. \n\nIt evaluates to 0, after taking some time to compute, in any conventional runtime. I think, from context, it should be obvious that this is just meant to be a placeholder expression representing a slow computation. Given that caused confusion, I'll replace it by `slow(999999)`, which may better illustrate the point.", "Reply Score": 2}]}, {"Comment": "... but I'm already telling you will probably be drastically slower, why you want to benchmark against SAT solvers? The point of this post is how this specific feature can be used to speed up brute-force in general, not how to construct a competitive SAT solver. I don't get how a benchmark would aggregate to that point?", "Score": 1, "Replies": [{"Reply": "I think applying this to a non NP-complete problem would demonstrate your point better.  There are entire fields dedicated to reasoning about NP-complete problem spaces, and the techniques being used to solve these problems have gone way beyond just assign-and-check.  Making a post in a computer science forum comparing performance of a 3-SAT solver is going to prompt these sorts of questions.\n\nIt might still be valuable to see how this approach scales relative to somewhat simple approaches in existing SAT solver, or even an empirical evaluation of how it scales relative to itself.  As the problem size grows, does memory with this approach grow exponentially?  What about CPU usage?  This is just a single data point that might be an outlier.", "Reply Score": 2}]}, {"Comment": "... which is correct? Being able to solve instances of something faster than brute-force (which is what the entire post is about) is completely different from being able to solve it faster than state-of-art techniques, let alone in polynomial time, which was never written, mentioned, not even hinted in any possible way through the entire post. I can't even comprehend how anyone would interpret it so drastically differently than what was objectively written. Does everyone just read headlines and then extrapolate with their own words?", "Score": 3, "Replies": [{"Reply": "Not question asker, and ignoring the defensiveness but want to clarify the point.\n\nYou use this term \"faster\", which has a meaning within the CS community.  To use this term, you need to define the time complexity of brute force, and then the complexity of inets to make this case.", "Reply Score": -1}]}, {"Comment": "> First, this post shows you are learning something new, which is awesome! \n\nLearning new things is awesome indeed, and sharing them is even better. I opted to share these notes because they do represent new techniques that are absolutely not published anywhere; specifically, how to use fan-nodes to optimize brute-force. That said, I didn't have the time (nor interest) to dig deeply into this topic, much less to perform a deeper analysis of this effect - I'm busy with many things - yet, I still decided to post the bits of information I had, for these who could find it useful. If I had done a more extensive analysis, I'd be publishing a proper paper, not a Gist with some quick notes, right?\n\n> Writing a simple SAT solver with HVM is easy and with reasonable performance.\n\nGiven how some people extrapolate the meaning of objective claims, that'd be a better title, I agree.\n\n> A search of the article shows \"fan\" is mentioned 0 times\n\nLook for dup-nodes, which is a synonym of fan-nodes in the context of interaction nets.\n\n> You don't show your brute force algorithm.\n\nThe complete code and algorithm is in the post.\n\n> However, it comes across as though inets are superior to brute force.\n\nIt **is** superior to brute-force, while also not being comparable to state-of-art algorithms. Both claims are compatible.\n\n> Are you comparing multiple threaded inet against single-thread brute force on multiple cores?\n\nBoth tests run in single-core. Randomness is irrelevant to the post, which is more focused in showing the technique than fully analyzing it.\n\n> You don't run multiple experiments.\n\nDefinitely running multiple experiments, a comprehensive benchmark suite and performance analysis would make the post much richer. That said, it wasn't in the scope of this quick Gist note.\n\n> How is this different than a map of memoized values?\n\nIn this case, it is similar. In others, it is completely different. For example, INet's model allow you perform repeated application (`F^n(x)`) for certain classes of functions in `log(N)`, which, here, does asymptotically outperform state-of-art alternatives. This is well described in the interaction net literature, and I've written about it in the past.\n\n> `(1234 ^ 4321) % 2` would take a long time to compute. No, it doesn't. It's 0. \n\nIt evaluates to 0, after taking some time to compute, in any conventional runtime. I think, from context, it should be obvious that this is just meant to be a placeholder expression representing a slow computation. Given that caused confusion, I'll replace it by `slow(999999)`, which may better illustrate the point.", "Score": 2, "Replies": []}, {"Comment": "I think applying this to a non NP-complete problem would demonstrate your point better.  There are entire fields dedicated to reasoning about NP-complete problem spaces, and the techniques being used to solve these problems have gone way beyond just assign-and-check.  Making a post in a computer science forum comparing performance of a 3-SAT solver is going to prompt these sorts of questions.\n\nIt might still be valuable to see how this approach scales relative to somewhat simple approaches in existing SAT solver, or even an empirical evaluation of how it scales relative to itself.  As the problem size grows, does memory with this approach grow exponentially?  What about CPU usage?  This is just a single data point that might be an outlier.", "Score": 2, "Replies": [{"Reply": "That makes sense. I choose 3-SAT kinda arbitrarily because it is a simple hard algorithm to test a fast-brute-forcer on, which is what this is. And now everyone is confusing the intention!", "Reply Score": 1}]}, {"Comment": "Not question asker, and ignoring the defensiveness but want to clarify the point.\n\nYou use this term \"faster\", which has a meaning within the CS community.  To use this term, you need to define the time complexity of brute force, and then the complexity of inets to make this case.", "Score": -1, "Replies": [{"Reply": "Faster and asymptotically faster are distinct claims. If you optimize a program by 20%, it is faster, even though big-O don't change. I didn't even imagine people could mistake it like that because this is the common terminology.", "Reply Score": 1}]}, {"Comment": "That makes sense. I choose 3-SAT kinda arbitrarily because it is a simple hard algorithm to test a fast-brute-forcer on, which is what this is. And now everyone is confusing the intention!", "Score": 1, "Replies": []}, {"Comment": "Faster and asymptotically faster are distinct claims. If you optimize a program by 20%, it is faster, even though big-O don't change. I didn't even imagine people could mistake it like that because this is the common terminology.", "Score": 1, "Replies": []}]},{"Title": "Which recently discovered algorithms have made the biggest impact in standard libraries - or at least in close to one-to-one drop-ins?", "Score": 51, "URL": "https://www.reddit.com/r/compsci/comments/1912ffl/which_recently_discovered_algorithms_have_made/", "CreatedAt": 1704661283.0, "Full Content": "Examples of the kind of thing I'm thinking of:\n\n* [Timsort](https://news.ycombinator.com/item?id=21196555) as a standard part of Python and Java\n* [Swiss hashing](https://abseil.io/blog/20180927-swisstables) - third-party C++ library)\n* [Faster Remainder By Direct Computation](https://arxiv.org/pdf/1902.01961.pdf) - not sure if it's implemented in libraries yet but that is fairly directly what the paper is intending\n\nThese three address some very common and general algorithms-and-data-structures use cases.  What are some other recent discoveries or developments that would affect the performance of some of our most common uses?", "CntComments": 9, "Comments": [{"Comment": "Erik Demaine's work on cache-oblivious datastructures and algorithms I think is probably the largest improvement of that sort in the last 20 years.\n\nLemire also does some interesting work in simple arithmetic algorithms.   There was a fundamental improvement to the speed of the FFT within the last 20 years, I have some extremely minor unpublished results I should finish but until then, look for work done by Anthony Blake about a decade ago.  \n\nBut really, yeah, considering how useful and fundamental b-trees are, and how ubiquitous multi level caching is in the real world, I think the development of a model, a framework, for thinking about cache oblivious datastructures and algorithms (and, to a lesser extent, wait-free datastructures and algorithms) takes the cake.", "Score": 25, "Replies": []}, {"Comment": "I feel that the pace of algorithms research (excl. ML) has slowed down considerably. Most of the algorithms we use were invented in the 60s to 80s. Even the \"recently discovered\" TimSort is 22 years old now. \n\nThere are exceptions to this; mostly subjects that didn't need algorithms until faster computers made them possible/necessary. 3D rendering in the 90s, large-scale databases and web servers in the early 2000s, or ML right now. But the simple basic algorithms don't change much.", "Score": 14, "Replies": []}, {"Comment": "Found [2 relevant code implementations](https://www.catalyzex.com/paper/arxiv:1902.01961/code) for \"Faster Remainder by Direct Computation: Applications to Compilers and Software Libraries\".\n\nIf you have code to share with the community, please add it [here](https://www.catalyzex.com/add_code?paper_url=https://arxiv.org/abs/1902.01961&title=Faster+Remainder+by+Direct+Computation%3A+Applications+to+Compilers+and+Software+Libraries) \ud83d\ude0a\ud83d\ude4f\n\n--\n\nTo opt out from receiving code links, DM me.", "Score": 3, "Replies": []}, {"Comment": "Channels", "Score": -3, "Replies": [{"Reply": "You mean Go's channels? My understanding is that's based off of Hoare's _Communicating Sequential Processes_ (with some queues added for pragmatism), which was first published in 1978. It's not really recent.", "Reply Score": 6}]}, {"Comment": "You mean Go's channels? My understanding is that's based off of Hoare's _Communicating Sequential Processes_ (with some queues added for pragmatism), which was first published in 1978. It's not really recent.", "Score": 6, "Replies": [{"Reply": "In fact, I'm going to double by simply saying AI was theorised over 100 years ago, yet we still consider AI new. This is because its implementation is new, but the concept is old, it is the same with channels. This is why channels are not implemented in any older languages and only in newer languages as in 1978 they didn't have multicore CPU's. I know I can't argue with stupid, so I'm going to block you", "Reply Score": 2}, {"Reply": "And Rust these are the only languages I know the implement them. So they are new", "Reply Score": -4}]}, {"Comment": "In fact, I'm going to double by simply saying AI was theorised over 100 years ago, yet we still consider AI new. This is because its implementation is new, but the concept is old, it is the same with channels. This is why channels are not implemented in any older languages and only in newer languages as in 1978 they didn't have multicore CPU's. I know I can't argue with stupid, so I'm going to block you", "Score": 2, "Replies": []}, {"Comment": "And Rust these are the only languages I know the implement them. So they are new", "Score": -4, "Replies": [{"Reply": "Kotlin has channels as well. Again, the stuff it\u2019s based on is not new as described above.", "Reply Score": 1}]}, {"Comment": "Kotlin has channels as well. Again, the stuff it\u2019s based on is not new as described above.", "Score": 1, "Replies": [{"Reply": "So Kotlin, Rust and Go, all new languages have implemented these. Still, anyone has yet to mention a language which is old and implements channels.", "Reply Score": 1}]}, {"Comment": "So Kotlin, Rust and Go, all new languages have implemented these. Still, anyone has yet to mention a language which is old and implements channels.", "Score": 1, "Replies": []}]},{"Title": "Complexity class for constant number of NP oracle calls", "Score": 2, "URL": "https://www.reddit.com/r/compsci/comments/191i3so/complexity_class_for_constant_number_of_np_oracle/", "CreatedAt": 1704709272.0, "Full Content": "So I\u2018ve been looking a bit at the polynomial hierarchy and was wondering whether there exists some papers on algorithms that run in polytime, when giving access to a constant number of NP oracle calls", "CntComments": 5, "Comments": [{"Comment": "Why isn't that just NP?", "Score": 2, "Replies": [{"Reply": "Both NP and coNP are subsets of P^NP", "Reply Score": 1}, {"Reply": "We may need a negative answer to the first oracle call that we do not get in NP. Consider:\n\nIf there exists a vertex cover of size between k and k+3, does there exist a dominating set of the minimum size vertex cover of size at least k and at most k+3?\n\nUsing 5 np-oracle calls this problem is solvable. However, the problem is not in NP, as if there exists no vertex cover of size k, there would exist no possible witness.", "Reply Score": 1}]}, {"Comment": "Both NP and coNP are subsets of P^NP", "Score": 1, "Replies": []}, {"Comment": "We may need a negative answer to the first oracle call that we do not get in NP. Consider:\n\nIf there exists a vertex cover of size between k and k+3, does there exist a dominating set of the minimum size vertex cover of size at least k and at most k+3?\n\nUsing 5 np-oracle calls this problem is solvable. However, the problem is not in NP, as if there exists no vertex cover of size k, there would exist no possible witness.", "Score": 1, "Replies": [{"Reply": "How would you solve this problem using 5 NP-oracle calls?", "Reply Score": 1}]}, {"Comment": "How would you solve this problem using 5 NP-oracle calls?", "Score": 1, "Replies": [{"Reply": "Three NP-oracle calls: doest there exist a Vertex Cover of size [k-1,k+3]?\nVia binary search we can determine this for all values.\n\nIf the answer for k-1 is yes, this is a yes-instance. \nIf the answer for k+3 is no, same.\nOtherwise we use one more NP-oracle call: Does there exist a dominating set of size at most k\u2018, where k\u2018 is the previously determined  min vertex cover", "Reply Score": 1}]}, {"Comment": "Three NP-oracle calls: doest there exist a Vertex Cover of size [k-1,k+3]?\nVia binary search we can determine this for all values.\n\nIf the answer for k-1 is yes, this is a yes-instance. \nIf the answer for k+3 is no, same.\nOtherwise we use one more NP-oracle call: Does there exist a dominating set of size at most k\u2018, where k\u2018 is the previously determined  min vertex cover", "Score": 1, "Replies": []}]},{"Title": "Essential Languages/Projects to Practice?", "Score": 4, "URL": "https://www.reddit.com/r/compsci/comments/190ma5c/essential_languagesprojects_to_practice/", "CreatedAt": 1704610713.0, "Full Content": "Hey everyone! Im a sophomore in university and I am beginning to practice CS more seriously for my career once I graduate. Im currently thinking SWE, DevOps and FullStack. Also interested in cybersecurity but I don\u2019t think thats a lot of coding. \n\nWhat are some essential languages I should practice? Also, what are some projects you recommend to create for learning and experience? Currently doing my first project, it\u2019s a diophantine equation calculator for 2 distinct integers (in Java)!", "CntComments": 2, "Comments": [{"Comment": "Practice the languages that you actually enjoy using. There are jobs for a lot of languages, but using one you actually enjoy will likely make you better at it, thus giving you an edge.\n\nAt the same time, don't focus that much on languages. Read Leslie Lamport's \"Computation and State Machines\" for the reasoning.\n\nHaving said that, you can't go wrong with knowing Python.", "Score": 3, "Replies": []}, {"Comment": "I'd say that some variety of Shell is required for everyone, especially if you are going DevOps. Python is a nice bonus, also especially for DevOps, knowing Go might help a bit too. There might be other languages involved, but that depends on your setup.\n\nIf you are going FullStack, then you can't run from JavaScript on the frontend (with HTML and CSS for markup and presentation), Java is OK for the backend part (but JS, Python, and Go might fit the bill also, depending on your goals), and for the database you'll need SQL as well as some kind of NoSQL solution (e.g. Redis, which is usually used as a cache, but can also be a database and a message bus in a pinch) to feel the difference.\n\nThere might be also things that will be required depending on your set of tools (e.g. many tools use YAML/JSON/XML for their properties, but some use full-fledged programming languages like Groovy to handle their configuration).\n\nAs for the project, choose something that you like, but try to start small and then extend to include the next thing you want to learn. E.g. after you get that equation program working in CLI, you can set up a CI/CD environment for it, then get it accessible via an API, then restrict the API to specific users and store user info in a DB.", "Score": 3, "Replies": []}]},{"Title": "Best YouTube channels to casually learn Operating Systems", "Score": 41, "URL": "https://www.reddit.com/r/compsci/comments/18z0c2w/best_youtube_channels_to_casually_learn_operating/", "CreatedAt": 1704436725.0, "Full Content": "I\u2019m going through a class on Operating Systems and it\u2019s phenomenal (specifically CS 162 from Berkeley by John Kubiatowicz).\n\nThough the lectures are 1.5 hours long and sometimes I lack the attention span.\n\nI want to keep up this knowledge and sometimes dive deeper into parts of operating systems less covered or just more fun to learn about tbh. Anything low level in general is also fun to learn about.\n\nI personally found [Low Level Learning](https://youtube.com/@LowLevelLearning) and [Ben Eater](https://youtube.com/@BenEater) to be very fun channels to watch. Do y\u2019all have any other YouTube channel recommendations?", "CntComments": 11, "Comments": [{"Comment": "Not a Youtube channel but Remzi's [Operating Systems: Three Easy Pieces](https://pages.cs.wisc.edu/~remzi/OSTEP/) is a fabulous book. It doesn't dump OS info on you and instead comes from an inquisitive perspective asking questions you might come up with and answering them in the text, not necessarily directly.", "Score": 8, "Replies": [{"Reply": "Thanks for the rec! I\u2019ll get to it if I have time!", "Reply Score": 2}]}, {"Comment": "Thanks. I've actually got lectures for operating system in my next semester so this helps!", "Score": 3, "Replies": [{"Reply": "zealous wrong disgusted dinner bewildered sip party cats whistle depend\n\n *This post was mass deleted and anonymized with [Redact](https://redact.dev)*", "Reply Score": 3}, {"Reply": "Ooh that\u2019s awesome! Hope you have fun! I\u2019m actually preparing for an OS class next semester as well (starts next week!)", "Reply Score": 2}]}, {"Comment": "Yes, Jacob Sorber, and CodeVault are very good as well. They mainly cover C programming specifically. Edit: Fun fact, LowLevelLearning used to work with a coworker of mine", "Score": 3, "Replies": [{"Reply": "Ooh Jacob Sorber looks like a fun guy! Just watched one of his videos on your recommendation, and it seems fun that he asks a tough question and answers it with an experiment! Thank you!", "Reply Score": 1}]}, {"Comment": "LLL and Ben Eater are great. DepthBuffer has a few good videos. This is a little out-there, but I found Retro Game Mechanics Explained to be very informative, giving practical examples of applied CS principles in old games.", "Score": 2, "Replies": []}, {"Comment": "[kimylamp](https://youtube.com/@kimylamp?si=FYC83Bp7wDbtCEc_) is also pretty good", "Score": 2, "Replies": []}, {"Comment": "If you're just watching the lectures, you'll not learn enough.\n\nYou'll learn by doing the homework and problems.\n\nAre you doing the HW from CS 162?", "Score": 1, "Replies": [{"Reply": "Oh yeah I am. I\u2019ve done 2 of the 3 projects so far. But I was looking for something that I can watch when I\u2019m lounging around too.", "Reply Score": 2}]}, {"Comment": "Thanks for the rec! I\u2019ll get to it if I have time!", "Score": 2, "Replies": []}, {"Comment": "zealous wrong disgusted dinner bewildered sip party cats whistle depend\n\n *This post was mass deleted and anonymized with [Redact](https://redact.dev)*", "Score": 3, "Replies": []}, {"Comment": "Ooh that\u2019s awesome! Hope you have fun! I\u2019m actually preparing for an OS class next semester as well (starts next week!)", "Score": 2, "Replies": []}, {"Comment": "Ooh Jacob Sorber looks like a fun guy! Just watched one of his videos on your recommendation, and it seems fun that he asks a tough question and answers it with an experiment! Thank you!", "Score": 1, "Replies": []}, {"Comment": "Oh yeah I am. I\u2019ve done 2 of the 3 projects so far. But I was looking for something that I can watch when I\u2019m lounging around too.", "Score": 2, "Replies": []}]},{"Title": "How important is Discrete Structures in Computer Science?", "Score": 26, "URL": "https://www.reddit.com/r/compsci/comments/18z1bdp/how_important_is_discrete_structures_in_computer/", "CreatedAt": 1704440460.0, "Full Content": "\nI have the option to take discrete mathematics to cover my discrete credit for both my math and cs major. If i take discrete structures, i\u2019ll have to take discrete mathematics anyway. Discrete structures focuses more on trees and heaps, and discrete mathematics focuses more on the math side of things. would it be worth just to take discrete mathematics or both discrete mathematics and discrete structures?\n\nDiscrete structures covers \u201cset algebra including mappings and relations; algebraic structures; elements of the theory of directed and undirected graphs; Boolean algebra and propositional logic; these structures applied to various areas of computer science\u201d\n\n\nand discrete mathematics involves \u201cformal logic, set theory, counting, discrete probability, graph theory, and number theory. Emphasis on reading and writing rigorous mathematics\u201d", "CntComments": 23, "Comments": [{"Comment": "Personally, after 9 years working with software, the last time I used discrete mathematics knowledge was about 5 minutes ago. Ability to simplify or reorganize/clean-up boolean expressions in itself is quite valuable.", "Score": 70, "Replies": [{"Reply": "I love this comment, some people keeps insisting that you don\u2019t need math to program, but except trivial things is math all the way down.", "Reply Score": 24}, {"Reply": "Agreed. SQL, regexes, etc.", "Reply Score": 3}]}, {"Comment": "It's pretty fundamental.\n\nI can't imagine a CS degree where it is optional.", "Score": 32, "Replies": []}, {"Comment": "Probably one of the most useful classes you will take is discrete mathematics.", "Score": 14, "Replies": []}, {"Comment": "I\u2019d recommend discrete structures for the Boolean logic alone. You\u2019ll probably have to build some kind of RSC later on in a class, and you use a ton of Boolean algebra to make some fancy logic gates. That\u2019s just my two cents though. Good luck!", "Score": 20, "Replies": [{"Reply": "Actually, just now understood that you would have to take both classes at that point. If it\u2019s not going to make you take extra hours towards your degree and mess with financial aid any, then I\u2019d do it. But if it just ends up being an extra class it may not be worth it.", "Reply Score": 3}]}, {"Comment": "Strictly speaking, your discrete structures course will be more tailored towards concrete applications of discrete data structures; it sounds more hands-on (and may be more useful at first for a software engineer/compsci student).\n\nTaking a course on discrete mathematics will prepare you with the combinatorial reasoning that takes place in finding solutions to problems in compsci, which you can then apply to an incredible amount of use cases. \n\nI will say that I am biased here, but learning mathematical foundation will also make it easier to learn the specific compsci cases.\n\nEdit: Without actually seeing the course content it would be difficult to actually say anything, but I'll take a guess and say that the following example would be reasonably indicative of the differences between the courses:\n\n* When talking about trees/graphs for instance, the discrete structures course may be more focused on tree inorder/preorder/postorder traversal, (the balancing of) binary trees, the importance of the depth and storing the tree in memory, adjacency matrices and lists etc.\n* In comparison, the discrete mathematics course will instead focus more on the handshake lemma, matchings, planarity and bipartite graphs, connectivity etc.", "Score": 8, "Replies": [{"Reply": "I took discrete structures in a CC before my current university. Yeah it was a lot more hands on. We had coding assignments/labs related to graph theory and other topics. My friends that took discrete math (only one my university offers), it was as you said.", "Reply Score": 2}]}, {"Comment": "insurance rustic political obtainable towering disagreeable materialistic humor plucky lush\n\n *This post was mass deleted and anonymized with [Redact](https://redact.dev)*", "Score": 6, "Replies": []}, {"Comment": "One of the most useful math courses you will ever take. Use that type if math almost every day", "Score": 4, "Replies": []}, {"Comment": "At my school, discrete structures was the gateway class to the rest of CS. They taught a bunch of proof techniques, set theory, etc. \n\nIt was also typically taken concurrently with \u201cData Structures\u201d, where we learned hash tables, linked lists, etc. Combine these two classes and you had enough knowledge to understand pretty much any CS subject.\n\nLearning how to prove things was also immensely helpful for my Math minor.", "Score": 3, "Replies": []}, {"Comment": "Very.", "Score": 3, "Replies": []}, {"Comment": "\u201cdiscrete structures\u201dis going to be a watered down version of discrete math, so just take discrete math. the cs department is allowing you to substitute the course so they clearly think it\u2019s sufficient preparation.", "Score": 3, "Replies": []}, {"Comment": "In my experience with discrete structures, skipping it wouldn't hold you back in other computer science courses at all. If you feel comfortable thinking through sets, graphs, and boolean logic, I think you'll be okay. Many concepts relate to other courses, but not in a way that would hold you back in any significant way. It's a good class, but I expect you can do only discrete math and still be fine. \n\nI don't know much about discrete math, but I expect it to be the harder class of the two, and maybe you'd even come out of it with more knowledge than you'd have gained in discrete structures.", "Score": 2, "Replies": []}, {"Comment": "Most BS in CS require it. So you have that.  If you are a self study and don't want to do it, then don't.", "Score": 1, "Replies": []}, {"Comment": "It\u2019s the entire thing.", "Score": 1, "Replies": []}, {"Comment": "Im in software engineering and I had to take discrete math, It wasnt bad as far as I remeber. The Most dreadfull enemy Is calculus", "Score": 1, "Replies": []}, {"Comment": "Discrete Mathematics is fundamental part of CS. I'd go as far as to say you don't need a great deal of knowledge in advanced calculus/ real analysis aside from understanding the basic concepts of differentiation, and maybe integration.   \n\n\nBut when it comes to Discrete Mathematics, it's essential.", "Score": 1, "Replies": [{"Reply": "is there a difference in discrete for cs and discrete for mathematics", "Reply Score": 1}]}, {"Comment": "It's super useful, gives you sort of mathematical tools to better analyse the code. Tbh I couldn't see its merits in college(had to go through it twice, dreaded it).", "Score": 1, "Replies": []}, {"Comment": "I love this comment, some people keeps insisting that you don\u2019t need math to program, but except trivial things is math all the way down.", "Score": 24, "Replies": []}, {"Comment": "Agreed. SQL, regexes, etc.", "Score": 3, "Replies": []}, {"Comment": "Actually, just now understood that you would have to take both classes at that point. If it\u2019s not going to make you take extra hours towards your degree and mess with financial aid any, then I\u2019d do it. But if it just ends up being an extra class it may not be worth it.", "Score": 3, "Replies": []}, {"Comment": "I took discrete structures in a CC before my current university. Yeah it was a lot more hands on. We had coding assignments/labs related to graph theory and other topics. My friends that took discrete math (only one my university offers), it was as you said.", "Score": 2, "Replies": []}, {"Comment": "is there a difference in discrete for cs and discrete for mathematics", "Score": 1, "Replies": [{"Reply": "It's the same thing, but maybe there are more topics covered or in more depth for mathematics majors. Either way, I only started \"understanding\" how to read and interpret those voodoo theorems when I got accustomed to understanding what implication is, how boolean algebra is constructed ( this is meta-intro to abstract algebra), how are proofs constructed, what is a contrapositive, inverse, converse etc.  \n\n\nGood books are from Susanna S.Epp, and Kenneth Rosen. Oscar Levin's Discrete Mathematics book is free and very good. Also an honourable mention is Thomas Koshy's Discrete Mathematics book.", "Reply Score": 1}]}, {"Comment": "It's the same thing, but maybe there are more topics covered or in more depth for mathematics majors. Either way, I only started \"understanding\" how to read and interpret those voodoo theorems when I got accustomed to understanding what implication is, how boolean algebra is constructed ( this is meta-intro to abstract algebra), how are proofs constructed, what is a contrapositive, inverse, converse etc.  \n\n\nGood books are from Susanna S.Epp, and Kenneth Rosen. Oscar Levin's Discrete Mathematics book is free and very good. Also an honourable mention is Thomas Koshy's Discrete Mathematics book.", "Score": 1, "Replies": [{"Reply": "okay, thanks. in that case, i\u2019ll take the math major version of discrete because it\u2019ll count towards both my cs and math major. i\u2019ll also check those books out. thanks!", "Reply Score": 1}]}, {"Comment": "okay, thanks. in that case, i\u2019ll take the math major version of discrete because it\u2019ll count towards both my cs and math major. i\u2019ll also check those books out. thanks!", "Score": 1, "Replies": []}]},{"Title": "Do things in a Linux os book apply on Windows as well as Linux.", "Score": 0, "URL": "https://www.reddit.com/r/compsci/comments/18z7ouy/do_things_in_a_linux_os_book_apply_on_windows_as/", "CreatedAt": 1704463640.0, "Full Content": "I read a book called (os three easy pieces) (not the whole book though) but it only talks about Linux.\nCan I use what I learn from this book to understand windows or do I have to read a different book which explains about windows?", "CntComments": 7, "Comments": [{"Comment": "Yes and no. The fundamental ideas: yes. Things like there is a process, memory management, etc.\n\nThe specific details like fork/exec: no. Windows uses a different model.", "Score": 20, "Replies": []}, {"Comment": "That book teaches principles of OS design. The principles are mostly universal, but implementations are not.\n\nLinux and Windows both need a scheme for scheduling processes to be executed, for example - but take different approaches in how the actual scheduling is done.\n\nFor example, a OS design book will focus on describing the problem of CPU scheduling, different approaches to solving it, and possibly dive into the implementation specifics of Linux *as an example* (since its code is freely available).\n\nIf you want to make your own OS or get the gist of what's going on in the OS world, you have everything you need.\n\nIf you want hyper specifics of how Windows or Linux are implemented, you will want to then move on to other books after reading it.\n\nHopefully that makes sense.", "Score": 12, "Replies": []}, {"Comment": "One major difference between Windows and Linux is the process model. Windows doesn\u2019t have a fork/exec process model; all processes are created from nothing by NtCreateProcess.\n\nThe threading model and API is also completely different. The Linux kernel actually treats each thread as a \u201cprocess\u201d, and those are the things it schedules. On Linux, the \u201cpid\u201d of a userland process is actually the thread ID (\u201ctid\u201d) of its main thread. The Windows kernel doesn\u2019t conflate these things at all. Processes have processes IDs, and each process has its own collection of threads.", "Score": 2, "Replies": [{"Reply": "Yeah but in Linux, processes also have ids and each process has its own collection of threads. Is this really a material difference? Threads are what ultimately run on the CPU in both cases.", "Reply Score": 2}]}, {"Comment": "The theoretical parts do, most of the relevant stuff like the actual commands won't.", "Score": 1, "Replies": []}, {"Comment": "Linux and Windows are similar in many ways but processes are different, which I believe is a very important difference if you are dealing with computer-intensive applications in your CS career.\n\nFor reference, I have worked with CUDA and Tensorflow, and Linux is just so much better, specifically because of compatibility issues of packages", "Score": 1, "Replies": []}, {"Comment": "Yeah but in Linux, processes also have ids and each process has its own collection of threads. Is this really a material difference? Threads are what ultimately run on the CPU in both cases.", "Score": 2, "Replies": [{"Reply": "Like I said, the \u201cprocess id\u201d that we\u2019re all familiar with is actually the thread id of the process\u2019s main thread. It\u2019s also called the threadgroup ID. The identity of a Linux process is thus inexorably tied to its main thread.", "Reply Score": 2}]}, {"Comment": "Like I said, the \u201cprocess id\u201d that we\u2019re all familiar with is actually the thread id of the process\u2019s main thread. It\u2019s also called the threadgroup ID. The identity of a Linux process is thus inexorably tied to its main thread.", "Score": 2, "Replies": []}]},{"Title": "[D] Setting up a small HPC for orchestrating a small teams AI research", "Score": 0, "URL": "/r/MachineLearning/comments/18yxwlh/d_setting_up_a_small_hpc_for_orchestrating_a/", "CreatedAt": 1704429430.0, "Full Content": "", "CntComments": 0, "Comments": []},{"Title": "Recommendations of open academic profile(like google scholar) database or API", "Score": 7, "URL": "https://www.reddit.com/r/compsci/comments/18ygwc4/recommendations_of_open_academic_profilelike/", "CreatedAt": 1704385049.0, "Full Content": "Hey, I'm developing a platform that allows for scientists to publish analysis and opinions on public policy in my country.\n\nI'd like to integrate the users' profiles with something akin to google scholar for keywords, citations, main topics, h-index, etc.\n\nThat way, users reading a post may judge how relevant the poster's analysis is considering his scientific work.\n\nThe problem with google scholar is that it doesn't have an API. There is a 3rd party one called SerpAPI, but I'm not sure about its stability/longevity and adherence to google ToS really.\n\nSo, do you know any other tools I might use for this that are legal and don't require webscraping? Thank you in advance!", "CntComments": 3, "Comments": [{"Comment": "Semantic Scholar has an API", "Score": 3, "Replies": [{"Reply": "Thank you! That's pretty good.", "Reply Score": 1}]}, {"Comment": "Check out ORCID API. Edit: Also arXiv.", "Score": 2, "Replies": []}, {"Comment": "Thank you! That's pretty good.", "Score": 1, "Replies": []}]},{"Title": "Eigendecomposition Explained", "Score": 12, "URL": "https://www.reddit.com/r/compsci/comments/18y7vw8/eigendecomposition_explained/", "CreatedAt": 1704355452.0, "Full Content": "Hi there,\n\nI've created a video [here](https://youtu.be/ihUr2LbdYlE) where I explain how we can factorize a square matrix using eigendecomposition and why this transformation can be useful in solving machine learning problems.\n\nI hope it may be of use to some of you out there. Feedback is more than welcomed! :)", "CntComments": 8, "Comments": [{"Comment": "Great contribution, thanks!", "Score": 2, "Replies": [{"Reply": "Thanks! :)", "Reply Score": 1}]}, {"Comment": "Great video, really informative and easy to follow!", "Score": 2, "Replies": [{"Reply": "Thanks! Happy you liked it! :)", "Reply Score": 2}]}, {"Comment": "Great video!", "Score": 2, "Replies": [{"Reply": "Thanks! :)", "Reply Score": 1}]}, {"Comment": "That\u2019s why I keeping opening this app. Great job", "Score": 1, "Replies": [{"Reply": "Thanks! :)", "Reply Score": 1}]}, {"Comment": "Thanks! :)", "Score": 1, "Replies": []}, {"Comment": "Thanks! Happy you liked it! :)", "Score": 2, "Replies": []}, {"Comment": "Thanks! :)", "Score": 1, "Replies": []}, {"Comment": "Thanks! :)", "Score": 1, "Replies": []}]},{"Title": "SAT Solving for Instruction Count in Game of Life", "Score": 5, "URL": "https://www.reddit.com/r/compsci/comments/18ya5ve/sat_solving_for_instruction_count_in_game_of_life/", "CreatedAt": 1704364664.0, "Full Content": "I have what I think is an interesting problem for SAT solving and was wondering if I could maybe receive some help to fully solve my problem! In particular, a friend an I have been looking at a problem of optimizing the code in a hot-loop for a fast game-of-life simulator.\n\nWe know that the minimum number of instructions in this hot-loop is bound from below by 8 and above by 10, which we found by applying some assumptions. The hot-loop computes the next iteration of a cell, given its state and the state of its `n` neighbors.  The table below shows whether this step can be computed using `m` [lop3](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#logic-and-shift-instructions-lop3) instructions.\n\n|n/m|2|3|4|5|6|7|8|9|10|\n|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|\n|3|**UNSAT**|**SAT**|SAT|SAT|SAT|SAT|SAT|SAT|SAT|\n|4|UNSAT|**UNSAT**|**SAT**|SAT|SAT|SAT|SAT|SAT|SAT|\n|5|UNSAT|UNSAT|UNSAT|**UNSAT**|**SAT**|SAT|SAT|SAT|SAT|\n|6|UNSAT|UNSAT|UNSAT|UNSAT|**UNSAT**|**SAT**|SAT|SAT|SAT|\n|7|UNSAT|UNSAT|UNSAT|UNSAT|UNSAT|**UNSAT**|**SAT**|SAT|SAT|\n|8|UNSAT|UNSAT|UNSAT|UNSAT|UNSAT|UNSAT|???|???|SAT|\n\nThis table was made by first encoding the problem in DIMACS CNF, after which we ran various SAT solvers including Kissat, ParKissat and PRS both locally and on the high performance cluster at our university.\n\nSadly, however, the most interesting row is incomplete for the standard implementation of game of life on a rectangular grid - `n = 8`. Our solvers received a time-out after 7 days with 128 cores on the high performance cluster.\n\nWe have attempted to exploit as much symmetry as possible in order to reduce the search space, such as ordering of inputs and instructions. We also enforced the requirement that each input and instruction output must be used. Additionally, we have tried to encode the problem in Z3 to little success (most likely due to inexperience using it).\n\nWe would be very interested in hearing if you've got any ideas on how to fill in these last two entries in the table. If needed, we can also provide more of the gory details of our encoding as well!\n\nRelevant:\n\n* [CNF Encoding](https://github.com/binary-banter/sat-generator/tree/new-attempt/src) (note that `n` in our encoding includes the center cell)\n* [Terrible Z3 Attempt](https://gist.github.com/Vlamonster/0d91fd2a633f5ea26c155f0a7ca464a9)\n* [Blogpost on Method](https://binary-banter.github.io/game-of-life/)\n* [Implemenation of Best Hot-Loop](https://github.com/binary-banter/fast-game-of-life/blob/4732749f8a550e43baadf2c6d0b32d3f6a2dc694/src/kernels/gol.cu#L11) (see `sub_step`)", "CntComments": 9, "Comments": [{"Comment": "> Sadly, however, the most interesting row is incomplete for the standard implementation of game of life on a rectangular grid - n = 8. Our solvers received a time-out after 7 days with 128 cores on the high performance cluster.\n\nYou're better off letting a sequential solver crunch on this for a month.  SAT is hard to parallelise and parallel SAT solvers do not perform super well.", "Score": 3, "Replies": [{"Reply": "We are considering this, but are unsure if it will yield any results. When using Kissat with printing enabled, it seems to get stuck at around 150k clauses and 12.8k variables for the largest problem. It can take anywhere between 2-3 hours to eliminate even 1 more variable. Would it still be worth an attempt, and if so, are there any important considerations for the parameters for the solver?  \n\n\nAnd if we were to run for such a long time, are there any SAT solvers that save their state, so that in case of power-outage it can be started again from where it left off?", "Reply Score": 1}]}, {"Comment": "I'm curious if you've tried any more general functions and what sizes you found.", "Score": 1, "Replies": []}, {"Comment": "We are considering this, but are unsure if it will yield any results. When using Kissat with printing enabled, it seems to get stuck at around 150k clauses and 12.8k variables for the largest problem. It can take anywhere between 2-3 hours to eliminate even 1 more variable. Would it still be worth an attempt, and if so, are there any important considerations for the parameters for the solver?  \n\n\nAnd if we were to run for such a long time, are there any SAT solvers that save their state, so that in case of power-outage it can be started again from where it left off?", "Score": 1, "Replies": [{"Reply": "> We are considering this, but are unsure if it will yield any results. When using Kissat with printing enabled, it seems to get stuck at around 150k clauses and 12.8k variables for the largest problem. It can take anywhere between 2-3 hours to eliminate even 1 more variable. Would it still be worth an attempt, and if so, are there any important considerations for the parameters for the solver?\n\nThe progress in SAT problems is in general hard to judge.  Number of eliminated variables is not a particularly good measure of progress and neither is anything else.\n\nYou're basically trying to solve circuit minimization using a SAT solver.  For some background on this, I think Knuth vol 4a has a section on this problem.  Knuth vol 4b has more on SAT and may also have interesting ideas.\n\nThe term you're trying to solve is symmetric, so you can rule out any solutions such that the input variables could be permuted to yield a lexicographically lower equivalent solution.  Adding this sort of symmetry breaking contraint to your formula is often very helpful.\n\nAnother strategy that might help you is to split the problem into less complex subproblems.  E.g. you could apply all possible lop3 functions to the first three inputs (the only choice you have), accounting for symmetry, and obtain a number of slightly simpler SAT problems.  Take these subproblems and feed them to sequential SAT solvers running in parallel.  If any of them finds a solution, your problem is SAT otherwise it is UNSAT.  You can iterate this to obtain more subproblems.  The important bit is to split the problem such that the subproblems have less degrees of freedom so to say.\n\nYou may also have bad instance encoding hampering your progress.  But as I don't know how you encode you circuit minimization problem, it's hard for me to say what the problem is.", "Reply Score": 1}]}, {"Comment": "> We are considering this, but are unsure if it will yield any results. When using Kissat with printing enabled, it seems to get stuck at around 150k clauses and 12.8k variables for the largest problem. It can take anywhere between 2-3 hours to eliminate even 1 more variable. Would it still be worth an attempt, and if so, are there any important considerations for the parameters for the solver?\n\nThe progress in SAT problems is in general hard to judge.  Number of eliminated variables is not a particularly good measure of progress and neither is anything else.\n\nYou're basically trying to solve circuit minimization using a SAT solver.  For some background on this, I think Knuth vol 4a has a section on this problem.  Knuth vol 4b has more on SAT and may also have interesting ideas.\n\nThe term you're trying to solve is symmetric, so you can rule out any solutions such that the input variables could be permuted to yield a lexicographically lower equivalent solution.  Adding this sort of symmetry breaking contraint to your formula is often very helpful.\n\nAnother strategy that might help you is to split the problem into less complex subproblems.  E.g. you could apply all possible lop3 functions to the first three inputs (the only choice you have), accounting for symmetry, and obtain a number of slightly simpler SAT problems.  Take these subproblems and feed them to sequential SAT solvers running in parallel.  If any of them finds a solution, your problem is SAT otherwise it is UNSAT.  You can iterate this to obtain more subproblems.  The important bit is to split the problem such that the subproblems have less degrees of freedom so to say.\n\nYou may also have bad instance encoding hampering your progress.  But as I don't know how you encode you circuit minimization problem, it's hard for me to say what the problem is.", "Score": 1, "Replies": [{"Reply": "Small update! We tried the strategy of breaking up the problem for `n = 8` and `m = 8` and were able to prove it unsat. Curiously the instance that took the longest to solve (almost 3 days) was one where the last instruction happened to be a ternary XOR (out of all 256 possible instructions that can be encoded). Just one cell entry left!\n\nOur solve times for the various instances are displayed [here](https://i.imgur.com/bhlWK7w.png).", "Reply Score": 2}, {"Reply": ">The term you're trying to solve is symmetric, so you can rule out any solutions such that the input variables could be permuted to yield a lexicographically lower equivalent solution. Adding this sort of symmetry breaking contraint to your formula is often very helpful.\n\nWe are indeed doing this. We use \\`<\\` and \\`<=\\` constraints on our one-hot encoding of which inputs (including outputs of previous components) connect to which components.\n\n>Another strategy that might help you is to split the problem into less complex subproblems. E.g. you could apply all possible lop3 functions to the first three inputs (the only choice you have), accounting for symmetry, and obtain a number of slightly simpler SAT problems. Take these subproblems and feed them to sequential SAT solvers running in parallel. If any of them finds a solution, your problem is SAT otherwise it is UNSAT. You can iterate this to obtain more subproblems. The important bit is to split the problem such that the subproblems have less degrees of freedom so to say.\n\nThis sounds like something we might try. We estimate a lot of the fixed instructions will lead to trivial UNSAT proofs - especially if we fix the final instruction.\n\n>You may also have bad instance encoding hampering your progress. But as I don't know how you encode you circuit minimization problem, it's hard for me to say what the problem is.\n\nI linked our repository for encoding the problem.", "Reply Score": 1}]}, {"Comment": "Small update! We tried the strategy of breaking up the problem for `n = 8` and `m = 8` and were able to prove it unsat. Curiously the instance that took the longest to solve (almost 3 days) was one where the last instruction happened to be a ternary XOR (out of all 256 possible instructions that can be encoded). Just one cell entry left!\n\nOur solve times for the various instances are displayed [here](https://i.imgur.com/bhlWK7w.png).", "Score": 2, "Replies": [{"Reply": "That's cool!  I'm happy you were able to work it out!", "Reply Score": 1}]}, {"Comment": ">The term you're trying to solve is symmetric, so you can rule out any solutions such that the input variables could be permuted to yield a lexicographically lower equivalent solution. Adding this sort of symmetry breaking contraint to your formula is often very helpful.\n\nWe are indeed doing this. We use \\`<\\` and \\`<=\\` constraints on our one-hot encoding of which inputs (including outputs of previous components) connect to which components.\n\n>Another strategy that might help you is to split the problem into less complex subproblems. E.g. you could apply all possible lop3 functions to the first three inputs (the only choice you have), accounting for symmetry, and obtain a number of slightly simpler SAT problems. Take these subproblems and feed them to sequential SAT solvers running in parallel. If any of them finds a solution, your problem is SAT otherwise it is UNSAT. You can iterate this to obtain more subproblems. The important bit is to split the problem such that the subproblems have less degrees of freedom so to say.\n\nThis sounds like something we might try. We estimate a lot of the fixed instructions will lead to trivial UNSAT proofs - especially if we fix the final instruction.\n\n>You may also have bad instance encoding hampering your progress. But as I don't know how you encode you circuit minimization problem, it's hard for me to say what the problem is.\n\nI linked our repository for encoding the problem.", "Score": 1, "Replies": [{"Reply": "Sorry, unfortunately I don't do Rust and it's kind of hard for me to understand your code.  Could you perhaps summarise your approach?  How do you encode \"k out of n\" type constraints?", "Reply Score": 1}]}, {"Comment": "That's cool!  I'm happy you were able to work it out!", "Score": 1, "Replies": []}, {"Comment": "Sorry, unfortunately I don't do Rust and it's kind of hard for me to understand your code.  Could you perhaps summarise your approach?  How do you encode \"k out of n\" type constraints?", "Score": 1, "Replies": [{"Reply": "We only have 1 out of n constraints, and we encode them using the commander encoding.\n\nI turned writing clauses into basic arithmetic, i.e. \\`+\\` appends a variable to a clause as a positive literal and \\`-\\` appends a variable as a negative literal and appends it to the clause. I also implemented `Sum` which allows one to simply OR all variables/literals in an iterator.", "Reply Score": 1}]}, {"Comment": "We only have 1 out of n constraints, and we encode them using the commander encoding.\n\nI turned writing clauses into basic arithmetic, i.e. \\`+\\` appends a variable to a clause as a positive literal and \\`-\\` appends a variable as a negative literal and appends it to the clause. I also implemented `Sum` which allows one to simply OR all variables/literals in an iterator.", "Score": 1, "Replies": []}]},{"Title": "Need some help finding some articles or other scholarly sources for writing a paper", "Score": 6, "URL": "https://www.reddit.com/r/compsci/comments/18wi6ww/need_some_help_finding_some_articles_or_other/", "CreatedAt": 1704175132.0, "Full Content": "I'm in my last semester as a Computer Science student and we have to write a persuasive ethics paper for my capstone class. The topic that I was assigned is \"Social media apps reduce isolation and are, on balance, a good thing\". We have to have 3 sources for the pro and 3 for the against. I have found papers easily for the against, but can not find anything for the pro. I have tried Google Scholar, ieeexplore and Researchgate, but I haven't been really successful. Are there any articles out there that any of you know of that support a statement like this: \"technology brings us closer together and makes us more social by allowing us to make and maintain relationships without respect to geographic location.\" Any help would be greatly appreciated.", "CntComments": 3, "Comments": [{"Comment": "I have nothing at my fingertips to recommend to you, but I spent a few minutes on the search tool provided by my school library, which searches, among other things, what I assume to be a pretty standard set of academic databases. I found a peer-reviewed journal called The Journal of Social Media in Society, which I imagine you can farm for a source or two. For example, Vol. 11, Issue 2 contains a study by a Belgian PhD student named Darian Harff called \"Political Content From Virtual 'Friends': How Influencers Arouse Young Women's Political Interest via Parasocial Relationships.\" I won't pretend I read it, but the abstract seems suggestive of positive relationships fostered by social media (or, um, positive \"parasocial\" relationships anyway):\n\nInfluencers have been at the center point of many studies in the past few years, which have investigated how they draw followers' attention to products and brands. However, despite influencers' growing communication of political topics, literature on their political impact is scarce. Using the concept of parasocial relationships, this study explores to what extent imaginary bonds held with influencers facilitate arousal of followers' political interest. For this purpose, a cross-sectional online survey of N = 1312 female participants was conducted. Structural equation modeling was used to analyze the relationships between the latent variables. Results showed that source similarity and trustworthiness predicted parasocial relationships, which were in turn positively related with arousal of political interest. Moreover, source similarity emerged as an important determinant of arousal of political interest. These findings indicate that influencers may raise interest for topics beyond lifestyle and entertainment, explained by followers' perceived similarity to influencers and close bonds held with them.\n\nAlso - and this might be a pretty obvious suggestion - I would imagine that you will find more useful sources in the broader social sciences literature than you will in the literature that more explicitly explores ethical issues relating to technology.", "Score": 3, "Replies": []}, {"Comment": "Check with your College/University's library. And I mean actually get in touch with a librarian: they most likely have a STEM specialized staff member that can help you hunt down sources.", "Score": 3, "Replies": []}, {"Comment": "Have you gone to your library to ask them to search peer reviewed articles for you?", "Score": 1, "Replies": []}]},{"Title": "Why does random access memory have a time complexity of O(1) and not O(log(n))?", "Score": 0, "URL": "https://www.reddit.com/r/compsci/comments/18x0xm6/why_does_random_access_memory_have_a_time/", "CreatedAt": 1704230996.0, "Full Content": "Say that I have an array of size n, therefore each memory address is comprised of log(n) digits, just reading the address will take the processor log(n) operations. So how is it O(1)?", "CntComments": 33, "Comments": [{"Comment": "Complexity is a measure of how the number of operations will scale when input size is increased.\n\nComputers have a fixed number of memory address digits hence it is considered constant time.\n\nAlso the meaning of random access is that it will take the same time for accessing the first Byte or thr last byte... Memory access time is constant.", "Score": 43, "Replies": [{"Reply": "then what is the point of space complexsity? just say that every data structure on the computer necessary has a fixed number of digits and so its allways O(1). The whole point of it is that it can tend to infinity even if the computer currently doesn't allow it", "Reply Score": -37}]}, {"Comment": "[deleted]", "Score": 66, "Replies": [{"Reply": "If anyone wants to learn about memory, addresses, and how they kind of work, check out the Wikipedia of MMU's: [link](https://en.wikipedia.org/wiki/Memory_management_unit).", "Reply Score": 2}, {"Reply": "This is misleading. Once you have a memory address, in order to actually access\nthat memory there is (conceptually) a big multiplexer in hardware that actually selects the row in DRAM that contains the address. The depth of that multiplexer has to scale with respect to the memory size, usually linear in the number of bits of the address space. So in today\u2019s hardware, you actually DO get logarithmic access time. It\u2019s just that # of bits is effectively a constant for DRAM, so we don\u2019t pay attention to it.\n\nThis is also one the of tradeoffs with making a cache bigger or smaller. If you increase e.g. L3 cache size, you will be able to store more data (good) but every access will be slightly slower. That\u2019s also why we have multilevel caches - not only is L1 faster to access because it\u2019s physically closer to the core itself, but it is smaller than L2.", "Reply Score": -2}]}, {"Comment": "> Say that I have an array of size n, therefore each memory address is comprised of log(n) digits, just reading the address will take the processor log(n) operations.\n\nIf you have a 64 bit computer, and you have an array of size n, the size of a memory address is 64 bits. The computer doesn't read memory addresses one bit at a time, the computer reads the whole thingamabooble in one instruction.", "Score": 14, "Replies": [{"Reply": "by that reasoning the array itself has a size of no more than 2\\^64 which is constant so the complexity of an array is O(1)", "Reply Score": -9}]}, {"Comment": "It's not. Not necessarily at least. It depends on the model you're using. Basic Turing machines are not able to access random memory in constant time. But Turing machines also don't represent real life computers well. \n\nYou also have to consider what \"time complexity\" means. It doesn't have to relate to real physical time. For example, talking about sorting algorithms, you often count a comparison of two elements as constant time and thus end up with n*log(n) sorting complexity. If you were to analyse sorting of string inputs on Turing machines, the required number of machine steps would grow faster than that.\n\nSo, in short, it's constant because it's defined to be constant.", "Score": 23, "Replies": []}, {"Comment": "You may be interested in reading about the [word RAM model of computation](https://en.wikipedia.org/wiki/Word_RAM), which to my understanding is the model (implicitly) used in most of modern CS theory.", "Score": 10, "Replies": []}, {"Comment": "I recommend checking out https://www.ilikebigbits.com/2014_04_21_myth_of_ram_1.html . It is tangentially related to this question though.  The article argues that memory, in general, actually follows O(n^0.5) or even O(n^(1/3)) complexity", "Score": 2, "Replies": [{"Reply": "thanks!", "Reply Score": 3}]}, {"Comment": "> Say that I have an array of size n, therefore each memory address is comprised of log(n) digits, just reading the address will take the processor log(n) operations. So how is it O(1)?\"\n\nYou can make an O(log n) memory bank using registered multiplexers. The \"registered\" part means that we store the output of each multiplexer on each clock, rather than \"passing through\" the result from the selected memory cell directly to the output. To help yourself understand why RAM is not O(log n), I recommend you draw this out on paper (or whatever). Take 8 1-bit memory cells and draw out the multiplexer circuit to read any of those memory cells, registering the output of each multiplexer. It will take 3 clock cycles for the selected cell to reach the output, because this is an O(log n) memory structure. But when you take away the registers, and allow the selected memory cell to pass through all the multiplexers to the output, that is O(1), because there is no \"wait time\" at each multiplexer, as a result of registration.\n\nThe reason for even mentioning registers is that this is a real design problem in large-scale memory systems. For a very large memory system, you will either need to register or electrically-buffer (i.e. NOT(NOT)) the data value being read from memory (or reverse, for writes) because the signal degrades as it is passed from one multiplexer to the next. Propagation delay does increase logarithmically but we get around this by pipelining which, in turn, uses registers. So, from the standpoint of physical intuition, you're not completely off-base, but from the standpoint of idealized logic (no propagation delay, no registers or electrical buffers needed), there is no issue. Idealized RAM is O(1).", "Score": 2, "Replies": []}, {"Comment": "Random access on rotating drive media id not constant, but for memory it is constant event with paging. \n\nIf the processor has 8, 16, 32, and 64 fetching instructions, the choice between them only increase the n0 and c on Ordo definition: n > n0:  T(n) < c*O(n), where T(n) is the number of operations, and O(n) is the complexity class function. \n\nO(1) does not mean the execution time is constant, but it is bound by a constant. Thus the execution time may vary between 1,2 or 4 ticks, and still remain within constant complexity class.", "Score": 1, "Replies": []}, {"Comment": "[deleted]", "Score": 1, "Replies": [{"Reply": "> In the asymptotic limit, O(log(n)) is probably correct\n\nNot in the standard model or word RAM model, though.", "Reply Score": 5}]}, {"Comment": "An address is the same size regardless of how large a collection is. If a collection defined by two pointers with bounds checking, each time you access, the program must ensure begin+offset < end. This takes the same amount of time whether begin and end are the same place or the maximum possible distance. With respect to N, the size of the collection, the complexity to apply an offset or compare two pointers is the same.", "Score": 0, "Replies": []}, {"Comment": "> just reading the address will take the processor log(n) operations. So how is it O(1)?\n\nYour presumption about reading the memory taking log(n) is false, both in physical hardware and in the abstract model of computation conventionally used for time complexity analysis of algorithms. It's only log(n) in the alternative model of computation that you are proposing here, which is not used for time complexity analysis. \n\nTime complexity is typically measured under a model of a computer called a random access machine (or a similar alternative). These abstract models of a computer have unbounded memory and can perform memory access as a single atomic operation, among other features. It is so by definition.", "Score": 0, "Replies": []}, {"Comment": "That's the reason why RAM is called Random access memory", "Score": -1, "Replies": []}, {"Comment": "When you work with arbitrary-precision arithmetic, that is the case - but the address is fixed-size, and word-size-operations are implemented as O(1) in the CPU.\n\nInterestingly, in DDR memory, to keep the number of wires down to reasonable counts, addresses are sent split in 2 parts - RAS and CAS, and depending on the sequence of addresses it may take N or 2N cycles to fetch the next contents.", "Score": 1, "Replies": []}, {"Comment": "The amount of digits in the address is not the bottleneck. The variation in time for higher value addresses is insignificant.\n\n&#x200B;\n\nEven if it did vary, operations on addresses take a single cycle anyways, that's why cpu architecture and ram are tightly defined around word size. So even if an operation on a small address ends up taking less time, that saved time will be less than a cycle and will thus be lost.\n\n&#x200B;\n\nIf the actual fetch varies depends on the ram itself, which is unlikely to be significant and predictable in terms of address number.\n\n&#x200B;\n\nWhat might be true would be that smaller architectures may be faster, but that is a time complexity that varies according to hardware design time decisions, not programming time decisions or execution variables like the length of a value.", "Score": 1, "Replies": []}, {"Comment": "Not directly answering you question, but there's actually some recent work trying to patch complexity theory by taking the cache hierarchy into account. Algorithms/theory people typically assume O(1) memory access, but in systems-level optimization it matters whether you're getting the data from L1 cache, L2 cache, DRAM, or main memory. As such, there's some locality theory people that use a theory of time complexity that assumes that random memory access is not necessarily O(1).", "Score": 1, "Replies": []}, {"Comment": "then what is the point of space complexsity? just say that every data structure on the computer necessary has a fixed number of digits and so its allways O(1). The whole point of it is that it can tend to infinity even if the computer currently doesn't allow it", "Score": -37, "Replies": [{"Reply": "No, every data structure on the computer does not have a fixed number of digits. Vary the input size of your algorithm and the storage needed will change. Complexity is nothing but the measure of how fast it will change.", "Reply Score": 18}, {"Reply": "Since computers don't actually have infinite memory complexity calculations are best thought of as running on theoretical computers with infinite memory. This theoretical computer is assumed be able to read an address and access the corresponding memory in constant time. This assumption is made because it accurately describes real computers operating under reasonable conditions.", "Reply Score": 4}, {"Reply": "You are being downvoted, but you are asking really good questions. You are completely right, it is very strange to consider all memory size constant for some aspects and then talk about space complexity. The reason is that in reality we do not know how to create turing complete machines, because we cannot create memory with infinite capacity. Therefore, in praxis we will always have finite memory, and since it is relatively small, we can actually resolve the address in one CPU cycle. Therefore, it makes sense to use a random access model. However, the programs we run are not always of the same size, and we do not always have unencumbered access to all of the memory, therefore it is still of interest to consider space complexity. Barring [galactic algorithms](https://en.m.wikipedia.org/wiki/Galactic_algorithm), we can expect O(n) to use considerably less memory than O(2^n), and thus solve much larger problems without running out of memory.", "Reply Score": 3}]}, {"Comment": "If anyone wants to learn about memory, addresses, and how they kind of work, check out the Wikipedia of MMU's: [link](https://en.wikipedia.org/wiki/Memory_management_unit).", "Score": 2, "Replies": []}, {"Comment": "This is misleading. Once you have a memory address, in order to actually access\nthat memory there is (conceptually) a big multiplexer in hardware that actually selects the row in DRAM that contains the address. The depth of that multiplexer has to scale with respect to the memory size, usually linear in the number of bits of the address space. So in today\u2019s hardware, you actually DO get logarithmic access time. It\u2019s just that # of bits is effectively a constant for DRAM, so we don\u2019t pay attention to it.\n\nThis is also one the of tradeoffs with making a cache bigger or smaller. If you increase e.g. L3 cache size, you will be able to store more data (good) but every access will be slightly slower. That\u2019s also why we have multilevel caches - not only is L1 faster to access because it\u2019s physically closer to the core itself, but it is smaller than L2.", "Score": -2, "Replies": []}, {"Comment": "by that reasoning the array itself has a size of no more than 2\\^64 which is constant so the complexity of an array is O(1)", "Score": -9, "Replies": [{"Reply": "Not by that reasoning, but by a more extreme version. It just turns out to not be useful. So we use either the normal model (where operations take unit time) or the word RAM model, as mentioned by others.", "Reply Score": 13}, {"Reply": "You can process 64 bits at once, because the hardware is wired up to support that. You cannot handle 2^64 elements of a data structure at once; the hardware to support that would be ludicrous.", "Reply Score": 8}, {"Reply": "Technically, that is correct but not particularly useful. If there is a fixed address size, then there's going to be a fixed largest array that is addressable. Everything (basically) you can do is asymptotically constant (heck, just write out the input and output for each function by hand, then use another array to look up the answer). It will be bounded by some constant, but that constant will likely be larger than the number of atoms in the universe. The asymptotic analysis makes things a lot easier to reason about as the problem space changes (say, going from a 32-bit address to 64 bits).\n\nIt reminds me of a novel way to solve the halting problem using the busy beaver function. The busy beaver function says, given a particular Turing machine type, find the particular Turing machine that runs the longest before halting. If you can figure out the busy beaver function, then solving the halting problem is \"easy\" just run the algorithm on a given Turing machine, as soon as the Turing machine runs longer than what's given by the busy beaver function, then you can definitely say it's non-halting. In practice, solving the busy beaver problem becomes intractable very very quickly.", "Reply Score": 2}]}, {"Comment": "thanks!", "Score": 3, "Replies": []}, {"Comment": "> In the asymptotic limit, O(log(n)) is probably correct\n\nNot in the standard model or word RAM model, though.", "Score": 5, "Replies": []}, {"Comment": "No, every data structure on the computer does not have a fixed number of digits. Vary the input size of your algorithm and the storage needed will change. Complexity is nothing but the measure of how fast it will change.", "Score": 18, "Replies": [{"Reply": "No he's right. Technically speaking, since computer memory is finite there is an upper bound on the maximum size of any data structure and so calling the operations O(1) is technically correct.\n\n\nEdit: actually I think I've made a logical error. I think it just doesn't make sense to think of complexity on a real computer because the function becomes undefined for large enough n. You need to use a reasonable theoretical model.", "Reply Score": -11}]}, {"Comment": "Since computers don't actually have infinite memory complexity calculations are best thought of as running on theoretical computers with infinite memory. This theoretical computer is assumed be able to read an address and access the corresponding memory in constant time. This assumption is made because it accurately describes real computers operating under reasonable conditions.", "Score": 4, "Replies": []}, {"Comment": "You are being downvoted, but you are asking really good questions. You are completely right, it is very strange to consider all memory size constant for some aspects and then talk about space complexity. The reason is that in reality we do not know how to create turing complete machines, because we cannot create memory with infinite capacity. Therefore, in praxis we will always have finite memory, and since it is relatively small, we can actually resolve the address in one CPU cycle. Therefore, it makes sense to use a random access model. However, the programs we run are not always of the same size, and we do not always have unencumbered access to all of the memory, therefore it is still of interest to consider space complexity. Barring [galactic algorithms](https://en.m.wikipedia.org/wiki/Galactic_algorithm), we can expect O(n) to use considerably less memory than O(2^n), and thus solve much larger problems without running out of memory.", "Score": 3, "Replies": []}, {"Comment": "Not by that reasoning, but by a more extreme version. It just turns out to not be useful. So we use either the normal model (where operations take unit time) or the word RAM model, as mentioned by others.", "Score": 13, "Replies": [{"Reply": "surprised word RAM is that famous. never even heard of it and it barely have a Wikipedia page. where did you all studied it?", "Reply Score": -8}]}, {"Comment": "You can process 64 bits at once, because the hardware is wired up to support that. You cannot handle 2^64 elements of a data structure at once; the hardware to support that would be ludicrous.", "Score": 8, "Replies": [{"Reply": "of course but it is still a finite number so i could build a computer that handles it.", "Reply Score": 0}]}, {"Comment": "Technically, that is correct but not particularly useful. If there is a fixed address size, then there's going to be a fixed largest array that is addressable. Everything (basically) you can do is asymptotically constant (heck, just write out the input and output for each function by hand, then use another array to look up the answer). It will be bounded by some constant, but that constant will likely be larger than the number of atoms in the universe. The asymptotic analysis makes things a lot easier to reason about as the problem space changes (say, going from a 32-bit address to 64 bits).\n\nIt reminds me of a novel way to solve the halting problem using the busy beaver function. The busy beaver function says, given a particular Turing machine type, find the particular Turing machine that runs the longest before halting. If you can figure out the busy beaver function, then solving the halting problem is \"easy\" just run the algorithm on a given Turing machine, as soon as the Turing machine runs longer than what's given by the busy beaver function, then you can definitely say it's non-halting. In practice, solving the busy beaver problem becomes intractable very very quickly.", "Score": 2, "Replies": []}, {"Comment": "No he's right. Technically speaking, since computer memory is finite there is an upper bound on the maximum size of any data structure and so calling the operations O(1) is technically correct.\n\n\nEdit: actually I think I've made a logical error. I think it just doesn't make sense to think of complexity on a real computer because the function becomes undefined for large enough n. You need to use a reasonable theoretical model.", "Score": -11, "Replies": [{"Reply": "Yeah and the observable universe is finite so everything is O(1), but that\u2019s also not very useful :p", "Reply Score": 17}, {"Reply": "I guess you could think about a series of computers where each has a larger memory than the last and the demand is that the function would work on one computer and every computer that comes after it in the series. \n\nalso, do you happen to have any direction regarding my original question?", "Reply Score": -5}]}, {"Comment": "surprised word RAM is that famous. never even heard of it and it barely have a Wikipedia page. where did you all studied it?", "Score": -8, "Replies": [{"Reply": "It's the default assumption in most of computer science, and so perhaps goes unnamed. All modern conventional computers use words of either 32- or 64-bits, meaning that 32- or 64- size is the unit most computation is performed on. On 32-bit architectures, CPU registers are 32 bits, integers are 32-bits, pointers are 32-bits, etc. The hardware is built to read, write, and compute in that word size, which is why 32-bit arithmetic is all O(1), reading and writing to memory regardless of address is O(1), and so on.", "Reply Score": 6}]}, {"Comment": "of course but it is still a finite number so i could build a computer that handles it.", "Score": 0, "Replies": [{"Reply": "Sure. Then you'd have a computer where that's essentially the size of one element, kind of like 64-bits can be one element for many of our computers today. And yet, linearly traversing multiple elements of that size would still be an O(n) operation. Time/space complexity is always talking about how the algorithm's processing time grows with the size of the input (i.e. increase in the number of input elements).", "Reply Score": 8}, {"Reply": "Good luck attempting to do so.\n\n2^64 words, or even bytes, is a pretty big number, roughly 2 10^19 or 20 exabytes.\n\nThere is more storage currently estimated to be accessible worldwide, roughly 20 zettabytes, so you need access to storage what currently is equivalent to about 1/1000th of the entire world's capacity. \n\nYou are going to need some pretty fancy methods to hook that all up.  Probably require you to think long and hard about how best to do it using O(1) or O( log n) operations rather than O(n) or O(n^2 ).", "Reply Score": 1}]}, {"Comment": "Yeah and the observable universe is finite so everything is O(1), but that\u2019s also not very useful :p", "Score": 17, "Replies": []}, {"Comment": "I guess you could think about a series of computers where each has a larger memory than the last and the demand is that the function would work on one computer and every computer that comes after it in the series. \n\nalso, do you happen to have any direction regarding my original question?", "Score": -5, "Replies": []}, {"Comment": "It's the default assumption in most of computer science, and so perhaps goes unnamed. All modern conventional computers use words of either 32- or 64-bits, meaning that 32- or 64- size is the unit most computation is performed on. On 32-bit architectures, CPU registers are 32 bits, integers are 32-bits, pointers are 32-bits, etc. The hardware is built to read, write, and compute in that word size, which is why 32-bit arithmetic is all O(1), reading and writing to memory regardless of address is O(1), and so on.", "Score": 6, "Replies": []}, {"Comment": "Sure. Then you'd have a computer where that's essentially the size of one element, kind of like 64-bits can be one element for many of our computers today. And yet, linearly traversing multiple elements of that size would still be an O(n) operation. Time/space complexity is always talking about how the algorithm's processing time grows with the size of the input (i.e. increase in the number of input elements).", "Score": 8, "Replies": []}, {"Comment": "Good luck attempting to do so.\n\n2^64 words, or even bytes, is a pretty big number, roughly 2 10^19 or 20 exabytes.\n\nThere is more storage currently estimated to be accessible worldwide, roughly 20 zettabytes, so you need access to storage what currently is equivalent to about 1/1000th of the entire world's capacity. \n\nYou are going to need some pretty fancy methods to hook that all up.  Probably require you to think long and hard about how best to do it using O(1) or O( log n) operations rather than O(n) or O(n^2 ).", "Score": 1, "Replies": []}]},{"Title": "Mag Amp Computer", "Score": 0, "URL": "https://www.reddit.com/r/compsci/comments/18x0vy2/mag_amp_computer/", "CreatedAt": 1704230881.0, "Full Content": "# I've studied the use of magamps to make computers and honestly think they'd be good for making less advanced but still working computers compared to vacuum tubes. But if not then I'd like to know why???\n\n&#x200B;", "CntComments": 7, "Comments": [{"Comment": "[Magnetic logic](https://en.m.wikipedia.org/wiki/Magnetic_logic) was a thing for a while in the 50s, and a few early computers like the ALWAC 800 were entirely magnetic. \n\n>But if not then I'd like to know why??? \n\nTransistors are just better. They can be made much much smaller (only a few atoms across), and run at much higher clock speeds with far less power.", "Score": 5, "Replies": [{"Reply": "Yeah but I can barely find any computers based on this despite how better t is to vacuum tubes.\n\nAlso would be pretty cool to just make one and give it a display to run slower but still working programs.", "Reply Score": 0}]}, {"Comment": "Check out some railroad control and management sources (textbooks, etc.).\n\nMag amps were used by the railroads during the Cold War as a technology that was capable to withstand the EMP attack. I don't think that there are any online sources available, you'll have to do a dead-tree library searches.", "Score": 2, "Replies": []}, {"Comment": "Yeah but I can barely find any computers based on this despite how better t is to vacuum tubes.\n\nAlso would be pretty cool to just make one and give it a display to run slower but still working programs.", "Score": 0, "Replies": [{"Reply": "It just wasn't around for very long before transistors took over. Very few were built. You might be able to find one in a museum, or someone's retro computing collection.", "Reply Score": 3}, {"Reply": "Then just make one if you want to.  You don't need us to approve your work.", "Reply Score": 2}]}, {"Comment": "It just wasn't around for very long before transistors took over. Very few were built. You might be able to find one in a museum, or someone's retro computing collection.", "Score": 3, "Replies": [{"Reply": "I they are anything like vcuum tubes they'd be useful for people who can't afford buying any computer or the kits needed to make em. Just to make 1 and run some computer experiment. Give them a display or other stuff and make a pseudo computer.\n\n  \nThat is if it's possible to do that.", "Reply Score": 0}]}, {"Comment": "Then just make one if you want to.  You don't need us to approve your work.", "Score": 2, "Replies": [{"Reply": "I was asking if it was possible and if anyone knew anything about Magnetic amplifier computers", "Reply Score": 1}]}, {"Comment": "I they are anything like vcuum tubes they'd be useful for people who can't afford buying any computer or the kits needed to make em. Just to make 1 and run some computer experiment. Give them a display or other stuff and make a pseudo computer.\n\n  \nThat is if it's possible to do that.", "Score": 0, "Replies": []}, {"Comment": "I was asking if it was possible and if anyone knew anything about Magnetic amplifier computers", "Score": 1, "Replies": []}]},{"Title": "How are applications patched?", "Score": 0, "URL": "https://www.reddit.com/r/compsci/comments/18wtu9p/how_are_applications_patched/", "CreatedAt": 1704214030.0, "Full Content": "I was wondering about the details of how different programs are patched. For example when a game or a program like discord gets a patch whats actually happening under the hood? Its not like the entire application is redownloaded, only the parts that changed. \n\nI read about binary patching, is this the go to method, or are there different options?\n\nThanks!", "CntComments": 6, "Comments": [{"Comment": "Programs are often made of many files - in the example of downloading a game, the textures and sounds and cinematics may all be in separate asset files independent from the executable code. So yes, you can use binary patching to update only a piece of a file - but even if an update replaces the entire executable file, that doesn't require re-downloading all the multimedia assets, which are likely the majority of the game's size on disk.", "Score": 7, "Replies": []}, {"Comment": "It depends, as usual in this field, on your constraints and the experience/expectations of your users.\n\nIf the app is small, redownloading it makes the most sense, no reason to complicate things.\n\nIf the app is large and good connectivity can reasonably be expected, e.g. online games, then download the files that changed and replace the old ones. It helps that these types of apps are already segmented by their nature.\n\nIf the connectivity cannot be assumed to be adequate and the application is relatively large given the connectivity limitations, then doing a binary diff and implementing a client side patch mechanism can make sense.\n\nThese are just three examples but there are definitely other methods.", "Score": 5, "Replies": []}, {"Comment": "In theory, you can check the difference in each file from one version to another and only change what needs to be changed. This is fairly common for text files, and there are commands on linux, [`diff`](https://man7.org/linux/man-pages/man1/diff.1.html) and [`patch`](https://man7.org/linux/man-pages/man1/patch.1.html) to do exactly that. The popular version control system [git](https://en.wikipedia.org/wiki/Git) uses these commands under the hood.\n\nIn practice, it's way more common to simply replace entire files for software updates. Video games are already broken up into many small files, so replacing just some of them is already way better that re-downloading everything. Besides, many file formats are compressed, so small changes in an image could cause the compressed file to be completely different.", "Score": 1, "Replies": []}, {"Comment": "It depends on the application, there's no one universal method. \n\nIf your application is small you can just download the whole thing and overwrite the old files.(or do what chrome does and have version install folder that you symlink)\n\nIf your application is large or has large assets you want to be more sophisticated and change the files that need to be changed \n\nHere's a riot blog on what they do for league of legends \n\nhttps://technology.riotgames.com/news/supercharging-data-delivery-new-league-patcher", "Score": 1, "Replies": []}, {"Comment": "As others have mentioned, usually entire files are replaced these days rather than computing diffs to minimize download size.\n\nHowever, just for fun there are even more elaborate forms of patching: hot patching, where a binary is patched while the application is still running. A short Windows-specific article worth reading is: [https://devblogs.microsoft.com/oldnewthing/20221109-00/?p=107373](https://devblogs.microsoft.com/oldnewthing/20221109-00/?p=107373)", "Score": 1, "Replies": []}, {"Comment": "I've used bsdiff/bspatch for patching Over-The-Air updates before for mobile apps. It's pretty well documented on how it works.", "Score": 1, "Replies": []}]},{"Title": "Multi-Head/Multi-Query/Grouped-Query Attentions Explained", "Score": 1, "URL": "https://www.reddit.com/r/compsci/comments/18wlwr9/multiheadmultiquerygroupedquery_attentions/", "CreatedAt": 1704189551.0, "Full Content": "Hi there,\n\nI've created a video [here](https://youtu.be/o68RRGxAtDo) where I explain how the Multi-Head Attention (MHA), Multi-Query Attention (MQA) and Grouped-Query Attention (GQA) work, and what are the pros and cons in using each one of them\n\nI hope it may be of use to some of you out there. Feedback is more than welcomed! :", "CntComments": 0, "Comments": []},{"Title": "Exploring the Intriguing World of Computer Science: Seeking Your Insights on \"A Computer Science Degree is (Mostly) A BAD Decision\" Video: https://www.youtube.com/watch?v=lcYTlAPhYrU", "Score": 0, "URL": "https://www.reddit.com/r/compsci/comments/18vqcf7/exploring_the_intriguing_world_of_computer/", "CreatedAt": 1704088835.0, "Full Content": " **https://www.youtube.com/watch?v=lcYTlAPhYrU** \n\n&#x200B;", "CntComments": 14, "Comments": [{"Comment": "Can you summarize? Video is not the greatest  medium to communicate ideas.", "Score": 19, "Replies": [{"Reply": "I think just dumping a link and not doing communication as the bulk of it should be banned.", "Reply Score": 27}, {"Reply": "I posted a [summary and rebuttal](https://www.reddit.com/r/compsci/s/X4cjMrDiCW) in a separate comment in this thread.", "Reply Score": 3}]}, {"Comment": "I would argue that there are very few people with enough self discipline who could put in a full-time learning effort without any examination or assessed worked to keep you focused to learn content.\n\nYes, you could watch great videos on topics like propositional logic, but without testing I feel like it's never going to be learnt, just watched/read.\n\nI think that computer science isn't about becoming a software developer, but more about how software works with hardware (digital circuits and logic) and foundational math topics that give a bit more meaning to algorithms and how they are applied to computer systems/software. If you want to just learn a language and program without scratching the itch of why and how it all works then self learnt is great.\n\nLastly, finding your first job is hard... Universities typically provide lots of opportunities to apply and work with companies in the local area or provide access to network groups etc. If you go it alone, it is a tougher prospect to convince a company of your capabilities, maybe there are some industry certifications you can complete to give you a leg up.", "Score": 30, "Replies": [{"Reply": "I agree on the last point. This is the hard truth. No matter how good you are at something, at the end of the day it matters if you could make your meal.  And firms always look for such brand name colleges, whether it's for engineering or management roles.\n\nBut yes, once you get job, you can skill up and learn coz there is no standardized test after that.", "Reply Score": 3}, {"Reply": "I've hired, mentored, and trained a good mix of developers over the years with traditional degrees, self-taught/boot camp folk, and those with no experience at all.\n\nHaving a degree tells me a few important things about you. First, you can stick with something for 3+ years and see it through to completion. The amount of \"self taught\" folk I've mentored who changed their minds within a few months and are no longer devs is staggering. No disrespect to those people; being a dev isn't for everyone. But having a degree means I'm much less likely to see you give up in under 6 months.\n\nSecond, while it doesn't guarantee that you're good at everything you need to be good at, it offers me reasonable confidence in your skills at _most_ things. Many self taught/bootcamp devs have glaring holes in their skillsets; never used a relational database, no understanding at all of computational complexity, never used a typed language, that kind of stuff. These skills may not be well developed among folk with degrees but they're very rarely _completely missing_. This ties in strongly to the point made above.\n\nAs an unrelated point, university education is about so much more than just your ability to make money for shareholders. The actual _science_ side of computer science is beautiful and fascinating, and you're not really getting that in any purely vocational education.\n\nIf you can spare the time and money for a university degree I would strongly recommend it for most people. The ones who can self-teach to a better standard than the same amount of time spent in a lecture hall are exceedingly rare.", "Reply Score": 4}, {"Reply": "\u2b06\ufe0f \u2b06\ufe0f \u2b06\ufe0f", "Reply Score": 0}, {"Reply": "Agreed 100%", "Reply Score": 1}]}, {"Comment": "First, a summary:\n\n**Exceptions**\n\n* If you can do so for an affordable price (*e.g.*, employer tuition benefit, low tuition, scholarships)\n* You live in a culture where education is paramount to future success (*e.g.*, India)\n\n**Reasons it\u2019s \u201c(mostly) a bad decision\u201d**\n\n1. Total cost, debt burden ($80k for in-state versus $200k for out-of-state in the U.S.)\n2. \u201cGreat\u201d alternatives are available (*e.g.*, CS50 Courses, bootcamps, Free Code Camp, or Brilliant.org)\n3. The indecisions, possibility of changing majors, or worst case of finishing degree only to hate the field\n4. Tech companies no longer require a CS degree (*cf.*, Google Career Certificates, Coursera offerings designed by Meta).\n5. \u201cIt\u2019s a different world now \u2026 Traditional paths are phasing out.\u201d\n\n**Author\u2019s recommended sequence**\n\nTake a year or two to\n\n1. Learn to code\n2. Practice at it\n3. Build legitimate projects\n4. Learn the algorithms to pass interviews\n5. Work on soft skills\n6. Apply to a million places\n7. Don\u2019t give up; devote yourself to it.\n8. Network on LinkedIn and Twitter\n\n---\n\n**My thoughts**\n\nSure, this is possible in theory. My first question is the probability of success. Our autodidact hero must have high executive function and will require external sources of guidance as to what to study, the types of projects to build, feedback on the projects, techniques, processes, and so on. The same goes for soft skills and social media marketing. The university system isn\u2019t perfect. What it does provide is a vetted path to get there and a recognized credential at the end. Our DIY dev faces the uphill climb of having to educate potential employers that a patchwork of Google, MIT OpenCourseWare, and Coursera certificates are comparable preparation. Granted, it\u2019s entirely different if the person bootstraps into significant contributions to GCC or another notable product or library. How many have done so successfully?\n\nThis may be a viable path for prodigies. For the remaining 9,999 in ten thousand, my recommendation is to invest effort into driving down the cost of a more traditional path.\n\n* Make good grades and test scores in high school to earn scholarships\n* Be creative about scholarships.\n  * I had an employee who paid for his undergraduate tuition by searching the web for any and every scholarship he qualified for. One was for orphans. Another he had to write an essay about his anticipated STEM career. He even got a small scholarship for having a beard. Yes, he had to hustle. The point is the funds are out there.\n* Make maximum use of dual enrollment programs in high school to spread out the cost of college and also to knock out some of the lower-division courses at a cheaper rate. Some high schools are producing graduates who already have associate\u2019s degrees.\n* Drop brand name snobbery and do a year or two in community college. Then finish up at a four-year state school. Apply for scholarships at the four-year based on the established track record.\n* Either straight out of high school or with a freshly issued A.S. (from high school or community college), get hired at an employer that has a tuition benefit.", "Score": 10, "Replies": [{"Reply": "Community colleges cannot be understated. In VA, completing your associates guarantees you admissions to any public university to get your bachelors. I\u2019ve encountered enough people that did that track and eventually got CS degrees from UVA & Virginia Tech that I know it\u2019s an excellent choice.", "Reply Score": 3}]}, {"Comment": "Maybe add a link to the video?", "Score": 2, "Replies": [{"Reply": "Added, thanks for the feedback.", "Reply Score": 1}]}, {"Comment": "Edit: was supposed to be a reply to [/u/gbacon](https://www.reddit.com/r/compsci/s/KM1OBOUJyF)\n\nGreat rebuttal. The foundational knowledge of a structured program is still very important for developers and support engineers. Understanding algorithms or network protocols might \u201conly\u201d save 1/2 second per API request seems silly, but when that api gets hit 1M times a day, that\u2019s huge. It shows in _how_ developers solve problems and can discuss with their peers.\n\nWhat too many people also don\u2019t accept is that they may just not be good at it. I knew people who got their CS degree at Georgia Tech who couldn\u2019t solve a technical problem logically and efficiently to save their life. They just couldn\u2019t deconstruct the problem well and then solve for the component parts and would write tons of code that basically functioned but would never be maintainable in the real world. That\u2019s a hard thing to accept but don\u2019t keep piling up debt to finish a degree that you can\u2019t back up with ability.", "Score": 2, "Replies": []}, {"Comment": "I think just dumping a link and not doing communication as the bulk of it should be banned.", "Score": 27, "Replies": []}, {"Comment": "I posted a [summary and rebuttal](https://www.reddit.com/r/compsci/s/X4cjMrDiCW) in a separate comment in this thread.", "Score": 3, "Replies": []}, {"Comment": "I agree on the last point. This is the hard truth. No matter how good you are at something, at the end of the day it matters if you could make your meal.  And firms always look for such brand name colleges, whether it's for engineering or management roles.\n\nBut yes, once you get job, you can skill up and learn coz there is no standardized test after that.", "Score": 3, "Replies": []}, {"Comment": "I've hired, mentored, and trained a good mix of developers over the years with traditional degrees, self-taught/boot camp folk, and those with no experience at all.\n\nHaving a degree tells me a few important things about you. First, you can stick with something for 3+ years and see it through to completion. The amount of \"self taught\" folk I've mentored who changed their minds within a few months and are no longer devs is staggering. No disrespect to those people; being a dev isn't for everyone. But having a degree means I'm much less likely to see you give up in under 6 months.\n\nSecond, while it doesn't guarantee that you're good at everything you need to be good at, it offers me reasonable confidence in your skills at _most_ things. Many self taught/bootcamp devs have glaring holes in their skillsets; never used a relational database, no understanding at all of computational complexity, never used a typed language, that kind of stuff. These skills may not be well developed among folk with degrees but they're very rarely _completely missing_. This ties in strongly to the point made above.\n\nAs an unrelated point, university education is about so much more than just your ability to make money for shareholders. The actual _science_ side of computer science is beautiful and fascinating, and you're not really getting that in any purely vocational education.\n\nIf you can spare the time and money for a university degree I would strongly recommend it for most people. The ones who can self-teach to a better standard than the same amount of time spent in a lecture hall are exceedingly rare.", "Score": 4, "Replies": []}, {"Comment": "\u2b06\ufe0f \u2b06\ufe0f \u2b06\ufe0f", "Score": 0, "Replies": []}, {"Comment": "Agreed 100%", "Score": 1, "Replies": []}, {"Comment": "Community colleges cannot be understated. In VA, completing your associates guarantees you admissions to any public university to get your bachelors. I\u2019ve encountered enough people that did that track and eventually got CS degrees from UVA & Virginia Tech that I know it\u2019s an excellent choice.", "Score": 3, "Replies": []}, {"Comment": "Added, thanks for the feedback.", "Score": 1, "Replies": [{"Reply": "Oof, that guy makes some good points. This is good advice for lots of folks who just want a job and don\u2019t care about being educated. I both do not have a degree and am very glad I spent 13 semesters at university. This guy seems to really not understand the value of an education. And still, I can\u2019t fault him very much because college is too expensive. I hate the idea that we should all forsake education because it\u2019s expensive, and yet this is a fact of American society.\n\nThe argument that 4 years of college is 4 years sacrificed is just plain wrong. While you\u2019re probably not earning income during that time, you\u2019re growing in so many ways it is asinine to think even 4 years of self study would come close to being as rewarding. And all that \u201cyou don\u2019t need to take English\u201d stuff is just wrong on its face. This guy would have benefited a lot from examining better social critiques in a classroom environment.\n\nAnd still, tuition is too damn high :(", "Reply Score": 4}]}, {"Comment": "Oof, that guy makes some good points. This is good advice for lots of folks who just want a job and don\u2019t care about being educated. I both do not have a degree and am very glad I spent 13 semesters at university. This guy seems to really not understand the value of an education. And still, I can\u2019t fault him very much because college is too expensive. I hate the idea that we should all forsake education because it\u2019s expensive, and yet this is a fact of American society.\n\nThe argument that 4 years of college is 4 years sacrificed is just plain wrong. While you\u2019re probably not earning income during that time, you\u2019re growing in so many ways it is asinine to think even 4 years of self study would come close to being as rewarding. And all that \u201cyou don\u2019t need to take English\u201d stuff is just wrong on its face. This guy would have benefited a lot from examining better social critiques in a classroom environment.\n\nAnd still, tuition is too damn high :(", "Score": 4, "Replies": []}]},{"Title": "\ud83c\udfc6 Most read articles across engineering blogs in 2023", "Score": 25, "URL": "https://www.reddit.com/r/compsci/comments/18unlmp/most_read_articles_across_engineering_blogs_in/", "CreatedAt": 1703964768.0, "Full Content": "I've recently compiled a list of the most read articles across engineering blogs in 2023.\n\nI considered the engagement across **Hackernews**, **Reddit**, and **X**. With some help of Python and Jupyter, I\u2019m excited to share the final list!\n\n1. \ud83e\udd47 [**\"How Meta built the infrastructure for Threads\"**](https://engineering.fb.com/2023/12/19/core-infra/how-meta-built-the-infrastructure-for-threads/) *by Laine Campbell, Chunqiang (CQ) Tang \u2e31 Meta \u2e31 9 min read \u2e31 19 Dec 2023*\\- Discusses the successful launch of Meta's Threads and the infrastructure behind- Describes the use of ZippyDB, a distributed key/value database, and how it was optimized for the Threads launch- Explores the role of Async, a serverless function platform, in scaling workload execution for Threads\n2. \ud83e\udd48 [**\"Slack\u2019s Migration to a Cellular Architecture\"**](https://slack.engineering/slacks-migration-to-a-cellular-architecture/) *by Cooper Bethea \u2e31 Slack \u2e31 9 min read \u2e31 22 Aug 2023*\\- Tells a story about migration from monolithic to cell-based architecture at Slack- Introduces the concept of gray failure in distributed systems- Explains how Availability Zones can be drained- Covers the implementation of siloing and traffic-shifting in cellular architecture\n3. \ud83e\udd49 [**\"Migrating Netflix to GraphQL Safely\"**](https://netflixtechblog.com/migrating-netflix-to-graphql-safely-8e1e4d4f1e72) *by Jennifer Shin, Tejas Shikhare, Will Emmanuel \u2e31 Netflix \u2e31 8 min read \u2e31 14 Jun 2023*\\- Describes the migration of Netflix's iOS and Android apps to GraphQL with zero downtime- Explores the use of three key testing strategies: AB Testing, Replay Testing, and Sticky Canaries, to ensure a safe and smooth migration- Covers the phased approach to migration, including the creation of a GraphQL Shim Service and the subsequent transition to GraphQL services owned by domain teams- Discusses the challenges and wins of each testing strategy- Shares insights into the tools developed, such as the Replay Testing framework and Sticky Canaries, to validate functional correctness, performance, and business metrics during the migration\n4. [**\"What is an inverted index, and why should you care?\"**](https://www.cockroachlabs.com/blog/inverted-indexes/) *by Charlie Custer \u2e31 Cockroach Labs \u2e31 7 min read \u2e31 17 Aug 2023*\\- Describes how inverted indexes work and their impact on database performance- Explores the downsides of using inverted indexes, specifically the minimal impact on write performance- Covers how to use inverted indexes, including when and how to create them- Shares examples and best practices for using inverted indexes in relational databases\n5. [**\"Scaling the Instagram Explore recommendations system\"**](https://engineering.fb.com/2023/08/09/ml-applications/scaling-instagram-explore-recommendations-system/) *by Vladislav Vorotilov, Ilnur Shugaepov \u2e31 Meta \u2e31 11 min read \u2e31 9 Aug 2023*\\- Discusses the use of Machine Learning in the Explore recommendation system on Instagram- Describes the use of Two Towers neural networks to make the recommendation system more scalable and flexible- Explores the use of task-specific DSL and a multi-stage approach to ranking in the system- Covers the use of caching and pre-computation with Two Towers neural network to build a more flexible and scalable ranking system- Introduces techniques such as Two Tower NN and user interactions history in the retrieval stage, and the use of Bayesian optimization and offline tuning for parameters tuning.\n6. [**\"Understanding Real-Time Application Monitoring\"**](https://medium.com/expedia-group-tech/essential-application-monitoring-metrics-a08519ecab9d) *by Ritesh Kapoor \u2e31 Expedia Group \u2e31 7 min read \u2e31 13 Jun 2023*\\- Covers the performance indicators and SLI/SLO/SLA concepts for application monitoring- Shares different categories of metrics, including application VM, API, database response, infrastructure, and more- Explores the importance of monitoring distributed tracing for troubleshooting requests with high latency or errors- Gives an overview of the challenges of improving operational performance and the benefits of monitoring applications with the right metrics and tools\n7. [**\"Improving Performance with HTTP Streaming\"**](https://medium.com/airbnb-engineering/improving-performance-with-http-streaming-ba9e72c66408) *by Victor \u2e31 Airbnb \u2e31 7 min read \u2e31 17 May 2023*\\- Describes how HTTP Streaming can improve page performance and how Airbnb enabled it on an existing codebase\n8. [**\"How does B-tree make your queries fast?\"**](https://blog.allegro.tech/2023/11/how-does-btree-make-your-queries-fast.html) *by Mateusz Ku\u017amik \u2e31 Allegro \u2e31 12 min read \u2e31 27 Nov 2023*\\- Introduces B-Tree as a data structure and clarifies B-Trees vs. BSTs- Explains B-Tree organization and search queries- Explores the practical implications of using B-trees on hardware, including CPU caches, RAM, and disk storage- Explains how packing multiple values into a single node reduces random access and enhances query performance- Addresses balancing in a B-Tree\n9. [**\"Meta developer tools: Working at scale\"**](https://engineering.fb.com/2023/06/27/developer-tools/meta-developer-tools-open-source/) *by Neil Mitchell \u2e31 Meta \u2e31 4 min read \u2e31 27 Jun 2023*\\- Describes Sapling, an open-source version control system designed for extreme scale- Covers Buck2, a build system supporting remote caching and execution for large-scale development- Explores testing and static analysis tools used at Meta, including Infer, RacerD, and Jest- Presents Sapienz, a tool for automatically testing mobile app\n10. [**\"How Gradle Reduced Build Scan Storage Costs on AWS by 75%\"**](https://gradle.com/blog/how-gradle-reduced-build-scan-storage-costs-on-aws-by-75/)  *by Oliver White \u2e31 Gradle \u2e31 4 min read \u2e31 23 Jun 2023*\\- Describes the challenge faced with inefficient cloud storage using Amazon RDS- Presents the decision to migrate to Amazon S3 as the solution- Shares the immediate 75% reduction in cloud expenses as a result of the migration- Explains the added benefit of enabling automatic deletion for unactivated scans after the migration\n11. [**\"Real-time Messaging\"**](https://slack.engineering/real-time-messaging/) *by Sameera Thangudu \u2e31 Slack \u2e31 7 min read \u2e31 11 Apr 2023*\\- Describes the architecture used to send real-time messages at scale- Discusses the setup of the Slack client, including the use of Webapp, Envoy, and GS to establish a websocket connection- Explains the process of broadcasting a message to all online clients following the journey of the message through the stack- Covers the different types of events, including regular traffic spikes for reminders, scheduled messages, and calendar events\n12. [**\"How Discord Stores Trillions of Messages\"**](https://discord.com/blog/how-discord-stores-trillions-of-messages) *by Bo Ingram \u2e31 Discord \u2e31 3 min read \u2e31 6 Mar 2023*\\- Describes problems with a Cassandra database storing billions of messagesCovers the impact of hot partitions on latency and end-user experience- Shares the challenges of cluster maintenance tasks and compactions- Discusses the frequent tuning of JVM's garbage collector and heap settings to address latency spikes\n\nI hope you enjoyed it!\n\n**I'm building a \ud83d\udcec newsletter called** [**Big Tech Digest**](http://bigtechdigest.substack.com) **where I send the latest articles  found across 300+ Big Tech and startup engineering blogs like Uber, Meta, Airbnb, Netflix, ... every two weeks.  I think you might find it useful.**\n\n**I'd also highly appreciate if you retweeted or liked** [**this X thread**](https://twitter.com/bigtechdigest/status/1740321118672838915)**.**", "CntComments": 0, "Comments": []},{"Title": "Best paper awards in computer science over the past 28 years (1996-2023)", "Score": 44, "URL": "https://jeffhuang.com/best_paper_awards/", "CreatedAt": 1703870846.0, "Full Content": "", "CntComments": 10, "Comments": [{"Comment": "This is a listing of best paper awards across 32 computer science conferences, for papers chosen in that year to be the \"best\" by committees in those conferences meant to be the broadest conferences in each subfield.", "Score": 4, "Replies": [{"Reply": "Not a lot of TCS here. Only FOCS. But amazing work nevertheless!", "Reply Score": 2}]}, {"Comment": "Really good selection!", "Score": 1, "Replies": []}, {"Comment": "bookmarked your site", "Score": 1, "Replies": []}, {"Comment": "Not a lot of TCS here. Only FOCS. But amazing work nevertheless!", "Score": 2, "Replies": [{"Reply": "Thanks! STOC is the other theory conference on there that's pretty broad, as far as I know.", "Reply Score": 1}]}, {"Comment": "Thanks! STOC is the other theory conference on there that's pretty broad, as far as I know.", "Score": 1, "Replies": [{"Reply": "Those are very \"north american\" venues with a clear emphasis on certain topics.\n\nI would add: SODA and LICS. European venues probably, a bit less prestigious, you got STACS and ICALP.", "Reply Score": 1}]}, {"Comment": "Those are very \"north american\" venues with a clear emphasis on certain topics.\n\nI would add: SODA and LICS. European venues probably, a bit less prestigious, you got STACS and ICALP.", "Score": 1, "Replies": [{"Reply": "I'd add COLT for learning theory, and for cryptography TCC and some of the IACR conferences like CRYPTO and EuroCrypt.", "Reply Score": 2}, {"Reply": "SODA is already there as well! I forgot to mention. The idea is to include a set of the lagrest broad conferences, rather than each one. It requires manually updating every year, about 40 hours or so, so it's hard to maintain anymore.", "Reply Score": 1}, {"Reply": "To be fair, I would also include KR (Next venue: [https://kr.org/KR2024/](https://kr.org/KR2024/)) which is a conference since 1989. That's inside the field of Knowledge Reasoning, a subset of AI, as big as AAAI/IJCAI/NIPS. Some people worked on clusterization of conferences in Computer Sciences ([http://coscinus.fr/](http://coscinus.fr/)). You may want to have a look, play with the clustering and the filters and see if you are satisfied with the list of conferences that you have :).", "Reply Score": 1}]}, {"Comment": "I'd add COLT for learning theory, and for cryptography TCC and some of the IACR conferences like CRYPTO and EuroCrypt.", "Score": 2, "Replies": []}, {"Comment": "SODA is already there as well! I forgot to mention. The idea is to include a set of the lagrest broad conferences, rather than each one. It requires manually updating every year, about 40 hours or so, so it's hard to maintain anymore.", "Score": 1, "Replies": [{"Reply": "What the hell? I searched for both STOC and SODA in my browser without success. My bad. Sorry for the noise.\n\nThis is a titanesque task. It is unfortunate that the information can't be found in the metadata of the paper, an automatic extraction would have been possible ...", "Reply Score": 2}]}, {"Comment": "To be fair, I would also include KR (Next venue: [https://kr.org/KR2024/](https://kr.org/KR2024/)) which is a conference since 1989. That's inside the field of Knowledge Reasoning, a subset of AI, as big as AAAI/IJCAI/NIPS. Some people worked on clusterization of conferences in Computer Sciences ([http://coscinus.fr/](http://coscinus.fr/)). You may want to have a look, play with the clustering and the filters and see if you are satisfied with the list of conferences that you have :).", "Score": 1, "Replies": []}, {"Comment": "What the hell? I searched for both STOC and SODA in my browser without success. My bad. Sorry for the noise.\n\nThis is a titanesque task. It is unfortunate that the information can't be found in the metadata of the paper, an automatic extraction would have been possible ...", "Score": 2, "Replies": []}]},{"Title": "How Neural Networks Work", "Score": 2, "URL": "https://www.reddit.com/r/compsci/comments/18u8n4j/how_neural_networks_work/", "CreatedAt": 1703916334.0, "Full Content": "I made a video explaining how neural networks work and function. If you get a chance, I would really appreciate it if you checked it out:  [https://youtu.be/WRSNrVH0wg8](https://youtu.be/WRSNrVH0wg8) ", "CntComments": 1, "Comments": [{"Comment": "[deleted]", "Score": 0, "Replies": [{"Reply": "Appreciate it, I got many more in the chamber!", "Reply Score": 3}]}, {"Comment": "Appreciate it, I got many more in the chamber!", "Score": 3, "Replies": []}]},{"Title": "I created a collision detection algorithm for arbitrary polygons. Im wondering if the method i used is already in existence somewhere.", "Score": 29, "URL": "https://www.reddit.com/r/compsci/comments/18t21a0/i_created_a_collision_detection_algorithm_for/", "CreatedAt": 1703791120.0, "Full Content": "So i thought of a way to detect if two arbitrary polygons, including concave ones, are colliding. Its really simple, way simpler than Separating Axis Theorem and per my tests seems even faster for higher edge counts.\n\nThe idea goes like this:\n\nStep 1 (optional): I perform area checks, as a break-early condition. I calculate negative space on both polygons by subtracting polygonal area from bounding box area, then if intersectional bounding box area is greater than the negative space of both polygons, i declare a collision has occured. I also do this with bounding circles to gracefully handle many-sided regular polygons.\n\nStep 2: I pick a single vertex on the smaller of the two polygons, preferably the one closest to the midpoint of the other polygon, and i perform a Point-In-Polygon / raycasting test on the other polygon. If true i declare a collision.\n\nStep 3: I calculate the sides of each polygon derived from its points, filter them based on the bounding box of the opposing polygon, then compare the sides of Polygon A to Polygon B. Between the two previous break early conditions and filtering the sides its usually efficient even with many-sided polygons, but in theory (i think), this could degenerate to exponential operations.\n\nThe rationale is that comparing sides will necessarily detect all edge collisions, then to handle the circumstance of one polygon being inside of the other with no intersecting sides, a single point in polygon test using any vertex will prove the entire shape is not in that polygon.\n\nI tested this against an implementation of SAT against 2 2000-sided polygons, my runtime was \\~3 ms while SAT was \\~50 ms.\n\nSo my question is, is the method im using well known somewhere, and if so, whats the actual advantage of separating axis theorem? Isnt SAT more linear in theory? But does linear in theory matter if in practice its the opposite?", "CntComments": 30, "Comments": [{"Comment": "I don't know whether it's in use or not but this is how I'd implement intersection for a GIS system.  You check whether and two edges cross, if they do then, there must be an intersection, otherwise one of the polygons contains the other or they're disjoint, so you check one point to disambiguate.  If you have a spatial index for the polygons you can make this even faster because you _only_ have to check the edges that might cross.", "Score": 8, "Replies": [{"Reply": "This is n^4 (n is number of vertices of each polygon, let's just say they're the same) right? I think it can be done better without too much work", "Reply Score": 3}]}, {"Comment": "# GJK \n\nhttps://winter.dev/articles/gjk-algorithm\n\n# Lin-Canny \n\nhttps://www.cs.purdue.edu/cgvlab/courses/434/434_Spring_2013/lectures/lec-collision.pdf\n\n#LCP\n\nLCP is acronym for  Linear complementarity Problem.   Software that solves LCP are called Linear complementarity solvers.   \n\nImagine you have a 3D triangle mesh of a wooden chair with four legs.   You want to know  whether when you place the chair down on a flat floor, whether all four feet of the chair will touch at the same time.   If the legs are imperfect, only three will touch with the short one being a little higher.   ALternatively, you could shift the chair slightly and another collection of 3 legs will touch.    \n\nNow say you want to write software that determines where the chair will be given the triangle mesh and the height of the flat floor.  That is,  write software that determines how the wonky chair will \"come to rest\" on the floor.   \n\nTHis problem is very difficult in the general case, and the mathematics is excruciating.   To solve it , you have to write a *Linear complementarity solver* as a separate  software component, and then plug it into a rigid body simulator.   \n\nhttps://lsa.umich.edu/content/dam/math-assets/reu-su22/Kodati,Rohan.REU2022.pdf", "Score": 5, "Replies": [{"Reply": "This kills the mathphobic", "Reply Score": 5}, {"Reply": "These all seem very different. My core algo doesnt require heavy math other than a line segment intersection and point in polygon test, the optional area check only a small amount of basic arithmetic.", "Reply Score": 1}]}, {"Comment": "I\u2019m not convinced on your step 1 (particularly on the efficiency), but it\u2019s my day off and I don\u2019t want to spend more time thinking about it.\n\nI don\u2019t know a formal theorem, but logically if polygon A and polygon B intersect, then at least one line segment from polygon A passes through at least one line segment from polygon B (unless one is fully inside the other as you pointed out).\n\nSo checking all line segment combinations would work. I\u2019m sure some could be skipped, but exhaustive approach should be fine.\n\nedit to add - I\u2019ve never heard of SAT and don\u2019t feel like googling. I have math and CS degrees so I\u2019m not a novice", "Score": -11, "Replies": [{"Reply": "> I\u2019m not convinced on your step 1 (particularly on the efficiency), but it\u2019s my day off and I don\u2019t want to spend more time thinking about it.\n\nI record and save the polygonal area. So in general i do have to do a linear O(n) pass through the points in general for the other parts of my algo. Once you have the polygonal area recorded this is the check: `if (iArea >= Math.max(b1Area - p1Area, b2Area-p2Area)) return true;` We do most likely have to calculate iArea and b1Area/b2Area each time which are all O(n) operations, but if nothing about position, orientation, or size changed then we could make it O(1) (although at that point a memory check would make more sense).\n\nAnyways, the point is that area checks help prevent the degenerate and highly intersecting scenarios where two polygons intersect so much it ends up being all sides vs all sides. It doesnt do it by itself though as area checks fail to catch anything on rotated elongated polygons, which is why my Point-In-Polygon check where i pick the one closest to the midpoint of the other shape is complementary.\n\n> So checking all line segment combinations would work. I\u2019m sure some could be skipped, but exhaustive approach should be fine.\n\nInstead of an exhaustive approach i filter edges/sides using the bounding box of the other polygon. So if you have two 100-gons only barely colliding on one edge, their bounding box intersections would be so tiny youd only have to compare about 1 side to 1 side, instead of 100^2 or 10k sides.", "Reply Score": 3}]}, {"Comment": "[deleted]", "Score": -4, "Replies": [{"Reply": "I did, it couldnt name anything.", "Reply Score": 2}]}, {"Comment": "Depending on what you use it for,step 1 might be pointless. For physics, each polygon will move gradually, so the later tests would trigger a collision well before step 1 could.\n\nIf the polygon positions are random, I\u2019m sure step 1 would help.", "Score": 1, "Replies": [{"Reply": "Youve got a point. Step 1 is heavily redundant.\n\nAnd in most situations, Step 2 would break early just like Step 1. The only advantage of step 1 is if im able to modify it further, maybe i can make it far more often a constant time operation. Because constant over linear time can be a big save.", "Reply Score": 1}]}, {"Comment": "This is n^4 (n is number of vertices of each polygon, let's just say they're the same) right? I think it can be done better without too much work", "Score": 3, "Replies": [{"Reply": "No it'd be N^2 naively since you have to check each pair of edges from each shape for crossing.  If you have an index it's much closer to linear in the number of \"close\" edges between the polygons (and thus output sensitive)", "Reply Score": 5}]}, {"Comment": "This kills the mathphobic", "Score": 5, "Replies": [{"Reply": "The mathphobic shouldn't be developing arbitrary polygon intersection algos :P", "Reply Score": 3}, {"Reply": "Im a mathphobic? What?", "Reply Score": 1}]}, {"Comment": "These all seem very different. My core algo doesnt require heavy math other than a line segment intersection and point in polygon test, the optional area check only a small amount of basic arithmetic.", "Score": 1, "Replies": []}, {"Comment": "> I\u2019m not convinced on your step 1 (particularly on the efficiency), but it\u2019s my day off and I don\u2019t want to spend more time thinking about it.\n\nI record and save the polygonal area. So in general i do have to do a linear O(n) pass through the points in general for the other parts of my algo. Once you have the polygonal area recorded this is the check: `if (iArea >= Math.max(b1Area - p1Area, b2Area-p2Area)) return true;` We do most likely have to calculate iArea and b1Area/b2Area each time which are all O(n) operations, but if nothing about position, orientation, or size changed then we could make it O(1) (although at that point a memory check would make more sense).\n\nAnyways, the point is that area checks help prevent the degenerate and highly intersecting scenarios where two polygons intersect so much it ends up being all sides vs all sides. It doesnt do it by itself though as area checks fail to catch anything on rotated elongated polygons, which is why my Point-In-Polygon check where i pick the one closest to the midpoint of the other shape is complementary.\n\n> So checking all line segment combinations would work. I\u2019m sure some could be skipped, but exhaustive approach should be fine.\n\nInstead of an exhaustive approach i filter edges/sides using the bounding box of the other polygon. So if you have two 100-gons only barely colliding on one edge, their bounding box intersections would be so tiny youd only have to compare about 1 side to 1 side, instead of 100^2 or 10k sides.", "Score": 3, "Replies": [{"Reply": "Using the bounding box makes sense and is fast to compute/determine. Using the areas only makes sense to me if you already need to compute the area for other purposes. If you already have it then it makes sense to use it but it could be expensive to compute area of irregular and non-convex polygons if you don\u2019t need to?", "Reply Score": 2}]}, {"Comment": "I did, it couldnt name anything.", "Score": 2, "Replies": []}, {"Comment": "Youve got a point. Step 1 is heavily redundant.\n\nAnd in most situations, Step 2 would break early just like Step 1. The only advantage of step 1 is if im able to modify it further, maybe i can make it far more often a constant time operation. Because constant over linear time can be a big save.", "Score": 1, "Replies": []}, {"Comment": "No it'd be N^2 naively since you have to check each pair of edges from each shape for crossing.  If you have an index it's much closer to linear in the number of \"close\" edges between the polygons (and thus output sensitive)", "Score": 5, "Replies": [{"Reply": "Right there's only n edges, my bad. N^2 still feels suboptimal though... Not sure", "Reply Score": 3}, {"Reply": "You can check if any two edges intersect in O(n log n) time with no preprocessing. Don't know the complexity of point-in-polygon though.", "Reply Score": 1}]}, {"Comment": "The mathphobic shouldn't be developing arbitrary polygon intersection algos :P", "Score": 3, "Replies": []}, {"Comment": "Im a mathphobic? What?", "Score": 1, "Replies": [{"Reply": "Sigh. Kill my pathetic joke why don't you.\n\nA \"mathphobic\", a word I made up, is someone that is scared of math.\n\nThe meme formula \"This kills the [insert thing here]\" is typically used after providing some sort of advice. For example, \"spray the bugs with raid, this kills the bugs\". But in a humorous way. Like, for example, a post explaining complex polygon intersection algorithms being so complicated and frightening the post kills people who are afraid of math.\n\nAll you killed was a joke.", "Reply Score": 2}]}, {"Comment": "Using the bounding box makes sense and is fast to compute/determine. Using the areas only makes sense to me if you already need to compute the area for other purposes. If you already have it then it makes sense to use it but it could be expensive to compute area of irregular and non-convex polygons if you don\u2019t need to?", "Score": 2, "Replies": [{"Reply": "I only compute the polygonal area once at the start. It doesnt seem expensive either way. Maybe the area checks are an overoptimization, i could imagine my point in polygon check handling most of the same scenarios (not all of them though)", "Reply Score": 2}]}, {"Comment": "Right there's only n edges, my bad. N^2 still feels suboptimal though... Not sure", "Score": 3, "Replies": [{"Reply": "Barring any pre-processing its the best you're going to do.  The easiest first step is as OP mentioned and at least having a bounding box to check first, with more sophisticated things reducing that further.", "Reply Score": 3}, {"Reply": "The cool thing is that in most circumstances, such as with arbitrarily many sides on a regular polygon, the point in polygon test (which is linear) will break out of the algo immediately, and we dont even need to worry about edge intersections at all.\n\nFor most other usecases, like the weirdest concave polygon you can think of, assuming time steps are incremental, even if the PIP test misses, we are only comparing a few edges to a few edges because the bounding boxes will be barely intersecting.\n\nSo we are only looking at n^2 degeneration if you superimpose two complex polygons on top of each other in a way that misses the PIP test (and also too much negative space for the area test to work). \n\nBut whats the alternative? Ear clipping then SAT on two sets of triangles? Thats also brute force by default... \n\nIn either case i think theres some hypothetical algo like sweep and prune which can fix these worst case scenarios, i just had no luck implementing it in a way thats faster yet.", "Reply Score": 3}]}, {"Comment": "You can check if any two edges intersect in O(n log n) time with no preprocessing. Don't know the complexity of point-in-polygon though.", "Score": 1, "Replies": [{"Reply": "Yes that's true with a sweep line algorithm (I'm used to working on the sphere where that's trickier to do).  PIP should be O(N) with the number of edges of the polygon you're testing against (unless it's pre-indexed).", "Reply Score": 1}]}, {"Comment": "Sigh. Kill my pathetic joke why don't you.\n\nA \"mathphobic\", a word I made up, is someone that is scared of math.\n\nThe meme formula \"This kills the [insert thing here]\" is typically used after providing some sort of advice. For example, \"spray the bugs with raid, this kills the bugs\". But in a humorous way. Like, for example, a post explaining complex polygon intersection algorithms being so complicated and frightening the post kills people who are afraid of math.\n\nAll you killed was a joke.", "Score": 2, "Replies": [{"Reply": "Oh. Well im always glad to help fight the system. Simple and works > elegant and unnecessarily complicated. #TeamMathphobe", "Reply Score": 2}]}, {"Comment": "I only compute the polygonal area once at the start. It doesnt seem expensive either way. Maybe the area checks are an overoptimization, i could imagine my point in polygon check handling most of the same scenarios (not all of them though)", "Score": 2, "Replies": [{"Reply": "A bit off-topic but I'm constantly astounded by Green's Theorem and the fact that it is extremely trivial to compute the area of an arbitrary closed polygon. I wonder if your implementation is so much faster because it specializes to two dimensions while SAT can be implemented for n-D collisions. Though now I want to think about if your arguments work in higher dimensions as well.", "Reply Score": 2}]}, {"Comment": "Barring any pre-processing its the best you're going to do.  The easiest first step is as OP mentioned and at least having a bounding box to check first, with more sophisticated things reducing that further.", "Score": 3, "Replies": [{"Reply": "Is it impossible to have an n log^k n?", "Reply Score": 1}]}, {"Comment": "The cool thing is that in most circumstances, such as with arbitrarily many sides on a regular polygon, the point in polygon test (which is linear) will break out of the algo immediately, and we dont even need to worry about edge intersections at all.\n\nFor most other usecases, like the weirdest concave polygon you can think of, assuming time steps are incremental, even if the PIP test misses, we are only comparing a few edges to a few edges because the bounding boxes will be barely intersecting.\n\nSo we are only looking at n^2 degeneration if you superimpose two complex polygons on top of each other in a way that misses the PIP test (and also too much negative space for the area test to work). \n\nBut whats the alternative? Ear clipping then SAT on two sets of triangles? Thats also brute force by default... \n\nIn either case i think theres some hypothetical algo like sweep and prune which can fix these worst case scenarios, i just had no luck implementing it in a way thats faster yet.", "Score": 3, "Replies": []}, {"Comment": "Yes that's true with a sweep line algorithm (I'm used to working on the sphere where that's trickier to do).  PIP should be O(N) with the number of edges of the polygon you're testing against (unless it's pre-indexed).", "Score": 1, "Replies": [{"Reply": "I tried to develop a sweep and prune style algo for my side intersection. I dont know what went wrong, because ive developed SaP before, but it just wasnt faster than brute force comparing the prefiltered sides.\n\nIf you have a working and tested sweep line / sweep and prune algo that can be transpiled to javascript id be interested in seeing if it can speed up my algo", "Reply Score": 1}]}, {"Comment": "Oh. Well im always glad to help fight the system. Simple and works > elegant and unnecessarily complicated. #TeamMathphobe", "Score": 2, "Replies": []}, {"Comment": "A bit off-topic but I'm constantly astounded by Green's Theorem and the fact that it is extremely trivial to compute the area of an arbitrary closed polygon. I wonder if your implementation is so much faster because it specializes to two dimensions while SAT can be implemented for n-D collisions. Though now I want to think about if your arguments work in higher dimensions as well.", "Score": 2, "Replies": [{"Reply": "My arguments require line intersections representing the sides of a polygon. In 3D we have planes and theyd be intersecting in 3 dimensions. No idea if you can do something similar, youd need a function for intersecting planes though.", "Reply Score": 1}]}, {"Comment": "Is it impossible to have an n log^k n?", "Score": 1, "Replies": []}, {"Comment": "I tried to develop a sweep and prune style algo for my side intersection. I dont know what went wrong, because ive developed SaP before, but it just wasnt faster than brute force comparing the prefiltered sides.\n\nIf you have a working and tested sweep line / sweep and prune algo that can be transpiled to javascript id be interested in seeing if it can speed up my algo", "Score": 1, "Replies": [{"Reply": "I don't sadly, I'd imagine you can grab any number off github and compile them to WASM though...", "Reply Score": 1}]}, {"Comment": "My arguments require line intersections representing the sides of a polygon. In 3D we have planes and theyd be intersecting in 3 dimensions. No idea if you can do something similar, youd need a function for intersecting planes though.", "Score": 1, "Replies": [{"Reply": "The cross product of the plane normals is the direction v of the line they intersect in. If you know one point p on one of the planes you can parameterize it with p + s * v + t * (n x v) where n is the normal and s, t are real numbers. Take the dot product with the other plane normal m and you get (p + t * (n x v)) . m = q . m where q is some point on the m plane. Solve for t and the earlier equation with s is a parameterization of the line.\n\nI don\u2019t see how you\u2019d extend the area argument to 3D though, the bounding box becomes a volume but the polygon might have zero volume.", "Reply Score": 1}]}, {"Comment": "I don't sadly, I'd imagine you can grab any number off github and compile them to WASM though...", "Score": 1, "Replies": []}, {"Comment": "The cross product of the plane normals is the direction v of the line they intersect in. If you know one point p on one of the planes you can parameterize it with p + s * v + t * (n x v) where n is the normal and s, t are real numbers. Take the dot product with the other plane normal m and you get (p + t * (n x v)) . m = q . m where q is some point on the m plane. Solve for t and the earlier equation with s is a parameterization of the line.\n\nI don\u2019t see how you\u2019d extend the area argument to 3D though, the bounding box becomes a volume but the polygon might have zero volume.", "Score": 1, "Replies": [{"Reply": "The area argument is heavily optional. If you can do a point in Polygon check using the closest vertex of the smaller of two polygons  then check for plane intersections on bounding box intersecting planes, thatd be the meat and potatoes of my algorithm. Simple and easy.\n\nThe area argument just requires knowing polygonal area and bounding box area, but you can substitute for volume in 3D. So if you have a cube you calculate the volume of the cube, the volume of its bounding vox, then bVol - pVol = Negative space. You then see if the two shapes bounding boxes (bounding cubes, whatever) intersect with more volume than either of the object's negative space. If theres a greater intersectional volume than negative space on either object, you know A Priori theres a collision.\n\nUnless you are comparing collision for objects with different dimensions, like a cube and a square, the area check should make sense in N dimensions.", "Reply Score": 1}]}, {"Comment": "The area argument is heavily optional. If you can do a point in Polygon check using the closest vertex of the smaller of two polygons  then check for plane intersections on bounding box intersecting planes, thatd be the meat and potatoes of my algorithm. Simple and easy.\n\nThe area argument just requires knowing polygonal area and bounding box area, but you can substitute for volume in 3D. So if you have a cube you calculate the volume of the cube, the volume of its bounding vox, then bVol - pVol = Negative space. You then see if the two shapes bounding boxes (bounding cubes, whatever) intersect with more volume than either of the object's negative space. If theres a greater intersectional volume than negative space on either object, you know A Priori theres a collision.\n\nUnless you are comparing collision for objects with different dimensions, like a cube and a square, the area check should make sense in N dimensions.", "Score": 1, "Replies": []}]},{"Title": "I have some queries as I want to write a process scheduler for atmega328p", "Score": 6, "URL": "https://www.reddit.com/r/compsci/comments/18szqml/i_have_some_queries_as_i_want_to_write_a_process/", "CreatedAt": 1703785411.0, "Full Content": "I am planning to write a simple process scheduler for an atmega328p micro-controller. I want to write a process scheduler because my interest lies towards operating systems and computer architecture, and I thought that writing a simple process scheduler for an actual hardware would be a good hands on experience.\n\nSo I understand that there are different process scheduling algorithms and all that but I have some doubts regarding the actual implementation of process schedulers - \n\nFirst, what are the things I need to actually keep track of in my schedule table? Do I need to store the value of every register, program counter, direction of data register bus, etc? I found some process scheduling programs on github, but I can only see objects like function, whether task is active or not and, priority - in the structure of processes (or maybe I was looking at something wrong), and not individual register values, etc.\n\nSecond, does the process states only depends upon the processor internal memory units(like registers, program counter, etc)? If yes, then what if a program which is dependent on memory and makes several memory store and memory fetch calls, does the scheduler also have to store a copy of whole memory block(which is simply impractical) and if it doesn't then isn't there a chance of data hazard(for example process 1 and process 2 can both depend and alter a memory location because of which corruption can occur after process switch). ", "CntComments": 6, "Comments": [{"Comment": "nice", "Score": 2, "Replies": [{"Reply": "Same didn't understood but it sounds cool", "Reply Score": 1}]}, {"Comment": "You are actually looking to make a very very light weight OS/RTOS I take it? (No reason to have a process scheduler without processes). 8-bit AVR microcontrollers are not just processors, in fact their processors are pretty weak. Embedded programming lesson 1 is to not think of a microcontroller as a computer. A micro's \"power\" really comes from it's HW peripherals (and using them intelligently). It's very easy to hit a resource or performance block with a micro (not enough code space, RAM, speed, etc). AVR micros are usually bare metal (no OS) to do a fairly simple job. \n\nI have done a scheduled event architecture in an ARM micro (event, I wouldn't call it a true process). You can setup a timer interrupt and then execute events based on a multiple of the interrupt count (probably via modular division and compare). Just a hunch, but I don't think you can do much more than that with an 8bit AVR. Taking a step up to ARM (STM32, etc) or ESP32 allows a lot more (10x more).", "Score": 2, "Replies": [{"Reply": "yes I was also worried about the same. But still wanted to atleast try it out.", "Reply Score": 1}]}, {"Comment": "[deleted]", "Score": 2, "Replies": [{"Reply": "any specific book or paper you can suggest related to these particular questions?", "Reply Score": 1}]}, {"Comment": "Same didn't understood but it sounds cool", "Score": 1, "Replies": []}, {"Comment": "yes I was also worried about the same. But still wanted to atleast try it out.", "Score": 1, "Replies": []}, {"Comment": "any specific book or paper you can suggest related to these particular questions?", "Score": 1, "Replies": [{"Reply": "[deleted]", "Reply Score": 2}]}, {"Comment": "[deleted]", "Score": 2, "Replies": [{"Reply": "In addition to reading books, OP should take a look at xv6 implementation. It's a small OS that is easy to understand.\nTake a look a the [schedule function](https://github.com/mit-pdos/xv6-public/blob/eeb7b415dbcb12cc362d0783e41c3d1f44066b17/proc.c#L323), and see how it performs context switching (saving/restoring registers, page tables,etc)\n\nEdit:typo", "Reply Score": 3}]}, {"Comment": "In addition to reading books, OP should take a look at xv6 implementation. It's a small OS that is easy to understand.\nTake a look a the [schedule function](https://github.com/mit-pdos/xv6-public/blob/eeb7b415dbcb12cc362d0783e41c3d1f44066b17/proc.c#L323), and see how it performs context switching (saving/restoring registers, page tables,etc)\n\nEdit:typo", "Score": 3, "Replies": []}]},{"Title": "Contraction Clustering (RASTER): A very fast and parallelizable clustering algorithm \u00b7 Issue #27848 \u00b7 scikit-learn/scikit-learn", "Score": 15, "URL": "https://github.com/scikit-learn/scikit-learn/issues/27848", "CreatedAt": 1703590726.0, "Full Content": "", "CntComments": 2, "Comments": [{"Comment": "nice", "Score": 1, "Replies": []}]},{"Title": "Books or articles on Nice algorithms for doing Linear Algebra Operations", "Score": 22, "URL": "https://www.reddit.com/r/compsci/comments/18qeu53/books_or_articles_on_nice_algorithms_for_doing/", "CreatedAt": 1703493612.0, "Full Content": "I am trying to find books or surveys on algorithms which involves doing computational linear algebra.\nI found one book [Linear Algebra Methods in Combinatorics](https://people.cs.uchicago.edu/~laci/babai-frankl-book2022.pdf) which involve this but i wanna find more of these.", "CntComments": 6, "Comments": [{"Comment": "Golub & Van Loan, Matrix Computations.", "Score": 6, "Replies": [{"Reply": "I haven't read that book in particular, but +1 for Van Loan.", "Reply Score": 1}]}, {"Comment": "Numerical Methods for Large Eigenvalue Problems by Yousef Saad", "Score": 6, "Replies": [{"Reply": "He's got this book and another good one in PDF form on his website: https://www-users.cse.umn.edu/~saad/books.html", "Reply Score": 2}]}, {"Comment": "Coding the Matrix", "Score": 1, "Replies": []}, {"Comment": "I haven't read that book in particular, but +1 for Van Loan.", "Score": 1, "Replies": []}, {"Comment": "He's got this book and another good one in PDF form on his website: https://www-users.cse.umn.edu/~saad/books.html", "Score": 2, "Replies": []}]},{"Title": "Cyclic Machine that could be useful for simulating some properties of elementary particle", "Score": 0, "URL": "https://www.reddit.com/r/compsci/comments/18r3f19/cyclic_machine_that_could_be_useful_for/", "CreatedAt": 1703578616.0, "Full Content": "(Previous post was automatically deleted due to edit)\n\nHere is a description:  \n\nMachine consists of a list of instructions. Each instruction represents some direction in space: left, right, up, down, forward, back. Machine executes all it's states one after another and moves in corresponding direction of space the same discrete distance on each tick of time. After some state is executed, control is passed to the next state. It happens infinitely in a loop. After last state is executed, control is passed to the first state.  \n\nSo machine moves and moves until it's state is changed.  \n\nhttps://youtu.be/yXSO_N2tL-0?si=RhrsZe26qwl19rlX\n\nProperties and phenomena that this machine reproduces:  \n\n\\- E - total energy: total amount of states. Energy of one state is numerically equal to reduced Planck\u2019s constant and is the reason why action is discrete\n\n\\- inertia (first newton's law): Machine keeps on moving infinitely until it's state is updated and the more states in machine the more states to be updated and the more particle \"resists to acceleration\"  \n\nhttps://youtu.be/sO9TgfWO5c4?si=XSxcvTIZP1w4hdgd\n\n\\- limited speed: you can not move faster than straight  \n\n\\- discrete action (interaction): change in energy equals energy of one state  \n\n\\- reduced wave length: contribution of one discrete piece in motion  \n\nhttps://youtu.be/uaYC5s82iIE?si=8KSSpK-OtD9RljKt  \n\n\\- uncertainty and observer effect: interaction destroys the original particle so you can't know some properties of the particle after interaction. Because you can't control, which state will be taken from machine or passed to machine  \n\nhttps://youtu.be/mNjKbEcswI4  \n\n\\- momentum and conservation of momentum: amount of states that represents motion  \n\nhttps://youtu.be/IG7Rfsu4fK4?si=PDikrn45K26gsmoF  \n\n\\- rest mass: amount of states that represents cyclic motion. For example a pair of states left-right does not move machine anywhere, but takes ticks of time too be executed.  \n\n\\- speed: If you have particle RRRR, it moves with maximum possible speed right. If you have RRRRLLL, it moves right 1 moment of time out of 7. So it's average speed is 1/7 of maximum speed  \n\n\\- Photons generated by set of particles have more or less the same speed in any direction  \n\nhttps://youtu.be/RVrPr4NvddU  \n\nand more.  \n\nBell inequalities are not usable because of observer effect. Classical statistic can not be calculated because observation destroys the particle AND hidden variables. Therefor you can not use classical statistics in bell inequalities.  \n\nhttps://youtu.be/OX\\_0poP6\\_tM  \n\nEdit: some day we could test if it\u2019s just simulation or the true nature of reality. It can be the next huge thing.", "CntComments": 44, "Comments": [{"Comment": "It was deleted due to being crazytalk.", "Score": 15, "Replies": [{"Reply": "Or the greatest breakthrough of a century that you can't get.", "Reply Score": -14}]}, {"Comment": "I\u2019m amazed at how willing people are to engage borderline schizophrenics in conversation", "Score": 7, "Replies": [{"Reply": "Stop it", "Reply Score": -1}]}, {"Comment": "If this is anything more than insane ramblings you are going to have to try again, and be more specific. What information is in each state? How exactly do you move between them? Where do they come from? How do the instructions relate to the states? None of this makes any sense, and I have a PhD in computer science and a background in quantum physics.", "Score": 11, "Replies": [{"Reply": "Those states  are matter. Matter joins into lists to form cyclic machines - elementary particles. Matter are simple machines with energy numerically equal to Reduced Planck\u2019s constant. Those machines navigate particles one after another in infinite loop - according to first Newton\u2019s law.\n\nWhy do you think matter keeps on moving in reality? How particles store their energy value? Why photons move straight for billions of years? Why direction of their motion is so precise that we can see stars and galaxies?\n\nDo you imagine how much information should be in one photons for such precise motion?", "Reply Score": -1}]}, {"Comment": "Read at least a basic text on Lattice Gas Automata or Lattice Boltzmann Methods to see how naive your notion is.\n\nRelevant keywords: \"semi-detailed balance\" and \"momentum advection tensor\".\n\nAt least read up on D3Q19 to realize that your approach has broken rotational symmetry due to the naive discretization of momenta.", "Score": 5, "Replies": [{"Reply": "your notion is naive. My model is testable.\n\nBy the way. Speak about your own models.", "Reply Score": -5}]}, {"Comment": "It is not the next huge thing.", "Score": 3, "Replies": [{"Reply": "Yes it is", "Reply Score": 0}]}, {"Comment": "So you outlined a physics sim. It doesn\u2019t really do us much good as we don\u2019t have the processing power to run it, and as you designed it it\u2019s quite inefficient. Also what\u2019s this yap about the true nature of reality? We can\u2019t make claims on that yet because not only is there so much we don\u2019t know, but we don\u2019t know if there even is a nature of reality.", "Score": 1, "Replies": []}, {"Comment": "Or the greatest breakthrough of a century that you can't get.", "Score": -14, "Replies": [{"Reply": "Why don\u2019t you write a paper on it then?", "Reply Score": 9}]}, {"Comment": "Stop it", "Score": -1, "Replies": []}, {"Comment": "Those states  are matter. Matter joins into lists to form cyclic machines - elementary particles. Matter are simple machines with energy numerically equal to Reduced Planck\u2019s constant. Those machines navigate particles one after another in infinite loop - according to first Newton\u2019s law.\n\nWhy do you think matter keeps on moving in reality? How particles store their energy value? Why photons move straight for billions of years? Why direction of their motion is so precise that we can see stars and galaxies?\n\nDo you imagine how much information should be in one photons for such precise motion?", "Score": -1, "Replies": [{"Reply": "Ok you have not explained anything. If you want people to understand this you need to use set notation, an algorithmic description of how the machines work, something concrete. Less words more math. If you can\u2019t do that then it is just bullshit.", "Reply Score": 8}]}, {"Comment": "your notion is naive. My model is testable.\n\nBy the way. Speak about your own models.", "Score": -5, "Replies": [{"Reply": "So you're going to ignore the broken rotational symmetry? You're fine with diamond-shaped waves rather than circles?\n\n>Speak about your own models.\n\nThe ones I work with mostly stick to D3Q19, which is easily shown to have a correct MAT, and thus to preserve the symmetry despite the discretization. This has been known since the late 1980s.", "Reply Score": 3}]}, {"Comment": "Yes it is", "Score": 0, "Replies": [{"Reply": "Nope.", "Reply Score": 3}]}, {"Comment": "Why don\u2019t you write a paper on it then?", "Score": 9, "Replies": [{"Reply": "Where should I write it? there is no any \"algorithm of universe\" scientific journal yet.", "Reply Score": -8}]}, {"Comment": "Ok you have not explained anything. If you want people to understand this you need to use set notation, an algorithmic description of how the machines work, something concrete. Less words more math. If you can\u2019t do that then it is just bullshit.", "Score": 8, "Replies": [{"Reply": "It\u2019s at the beginning of the post.", "Reply Score": 1}]}, {"Comment": "So you're going to ignore the broken rotational symmetry? You're fine with diamond-shaped waves rather than circles?\n\n>Speak about your own models.\n\nThe ones I work with mostly stick to D3Q19, which is easily shown to have a correct MAT, and thus to preserve the symmetry despite the discretization. This has been known since the late 1980s.", "Score": 3, "Replies": [{"Reply": "check how normal distribution works in \"rhombus based universe\" and you will be surprised that it still looks like a sphere.\n\nAfter that think just a little bit.", "Reply Score": 1}, {"Reply": "also watch this video and be surprised even more\n\n[https://youtu.be/RVrPr4NvddU](https://youtu.be/RVrPr4NvddU)", "Reply Score": -1}]}, {"Comment": "Nope.", "Score": 3, "Replies": [{"Reply": "Yo think nobody will as you. Programmable matter is the future.", "Reply Score": 0}]}, {"Comment": "Where should I write it? there is no any \"algorithm of universe\" scientific journal yet.", "Score": -8, "Replies": [{"Reply": "It\u2019s not that what you\u2019re talking doesn\u2019t make sense - even if that\u2019s the case - but I don\u2019t think you\u2019re familiar with academia, or anything scientific in that matter.\n\nClaiming something is a \u201crevolutionary breakthrough\u201d in science when you don\u2019t have any barely organized research ready for peer-review is either trolling or delusional. Newton didn\u2019t need a \u201claws of the universe\u201d journal to publish his findings.", "Reply Score": 9}]}, {"Comment": "It\u2019s at the beginning of the post.", "Score": 1, "Replies": [{"Reply": "No it\u2019s not. You just used ambiguous words in your post. There is no formal description. You might think it is clear but that is because you have spent probably hundreds of hours thinking about this and/or you are having a manic episode. You need to define formally, with math, how this works.", "Reply Score": 6}]}, {"Comment": "check how normal distribution works in \"rhombus based universe\" and you will be surprised that it still looks like a sphere.\n\nAfter that think just a little bit.", "Score": 1, "Replies": []}, {"Comment": "also watch this video and be surprised even more\n\n[https://youtu.be/RVrPr4NvddU](https://youtu.be/RVrPr4NvddU)", "Score": -1, "Replies": [{"Reply": "I did watch it. So what's the form of the momentum advection tensor of your model?", "Reply Score": 3}]}, {"Comment": "Yo think nobody will as you. Programmable matter is the future.", "Score": 0, "Replies": [{"Reply": "Sorry, no.", "Reply Score": 3}]}, {"Comment": "It\u2019s not that what you\u2019re talking doesn\u2019t make sense - even if that\u2019s the case - but I don\u2019t think you\u2019re familiar with academia, or anything scientific in that matter.\n\nClaiming something is a \u201crevolutionary breakthrough\u201d in science when you don\u2019t have any barely organized research ready for peer-review is either trolling or delusional. Newton didn\u2019t need a \u201claws of the universe\u201d journal to publish his findings.", "Score": 9, "Replies": [{"Reply": "Yes, Newton just published a book. Without any peer review or academia.\n\nAnd I publish videos.", "Reply Score": -7}]}, {"Comment": "No it\u2019s not. You just used ambiguous words in your post. There is no formal description. You might think it is clear but that is because you have spent probably hundreds of hours thinking about this and/or you are having a manic episode. You need to define formally, with math, how this works.", "Score": 6, "Replies": [{"Reply": "That\u2019s why there are links to videos. I can not provide theory of everything in a post anyway.", "Reply Score": 1}]}, {"Comment": "I did watch it. So what's the form of the momentum advection tensor of your model?", "Score": 3, "Replies": [{"Reply": "The model is testable without such stupid questions. Everything will work fine just as it works fine in normal distribution.", "Reply Score": -4}, {"Reply": "You see.. guys.. you got used to do nothing but exchange formulas. That is not necessary right. You should speak about experiments, not about tensors. That what science is", "Reply Score": -3}]}, {"Comment": "Sorry, no.", "Score": 3, "Replies": [{"Reply": "Just go your way. Don\u2019t be sorry. Nobody cares", "Reply Score": 0}]}, {"Comment": "Yes, Newton just published a book. Without any peer review or academia.\n\nAnd I publish videos.", "Score": -7, "Replies": [{"Reply": "Comparing yourself with Newton does not help your case.", "Reply Score": 6}, {"Reply": "You know what, knock yourself out. I ran out of friendly advice.", "Reply Score": 1}]}, {"Comment": "That\u2019s why there are links to videos. I can not provide theory of everything in a post anyway.", "Score": 1, "Replies": [{"Reply": "I actually watched a bunch of the videos for some reason and if there is one that explains how the machines work I couldn\u2019t find it. They all just assume you know wtf the machines are before you start.", "Reply Score": 8}]}, {"Comment": "The model is testable without such stupid questions. Everything will work fine just as it works fine in normal distribution.", "Score": -4, "Replies": [{"Reply": "Lot of words for \"I just didn't derive it, but trust me bro\".\n\nGoodbye and good luck.", "Reply Score": 4}]}, {"Comment": "You see.. guys.. you got used to do nothing but exchange formulas. That is not necessary right. You should speak about experiments, not about tensors. That what science is", "Score": -3, "Replies": []}, {"Comment": "Just go your way. Don\u2019t be sorry. Nobody cares", "Score": 0, "Replies": [{"Reply": "You are the one offering meaningless, unpublished word salad claiming that it is the next big thing.\n\nIt is on you to make a proposition that is useful.\n\nSo, no.  Sorry, but no.", "Reply Score": 3}]}, {"Comment": "Comparing yourself with Newton does not help your case.", "Score": 6, "Replies": [{"Reply": "Being a blind denier does not help you either.", "Reply Score": -1}]}, {"Comment": "You know what, knock yourself out. I ran out of friendly advice.", "Score": 1, "Replies": []}, {"Comment": "I actually watched a bunch of the videos for some reason and if there is one that explains how the machines work I couldn\u2019t find it. They all just assume you know wtf the machines are before you start.", "Score": 8, "Replies": []}, {"Comment": "Lot of words for \"I just didn't derive it, but trust me bro\".\n\nGoodbye and good luck.", "Score": 4, "Replies": [{"Reply": "It\u2019s much more then your it will not work because 1980", "Reply Score": -2}]}, {"Comment": "You are the one offering meaningless, unpublished word salad claiming that it is the next big thing.\n\nIt is on you to make a proposition that is useful.\n\nSo, no.  Sorry, but no.", "Score": 3, "Replies": [{"Reply": "No startup is published, dummy.", "Reply Score": 0}]}, {"Comment": "Being a blind denier does not help you either.", "Score": -1, "Replies": []}, {"Comment": "It\u2019s much more then your it will not work because 1980", "Score": -2, "Replies": []}, {"Comment": "No startup is published, dummy.", "Score": 0, "Replies": [{"Reply": "And what you have said here is not meaningful. Not even close.", "Reply Score": 2}]}, {"Comment": "And what you have said here is not meaningful. Not even close.", "Score": 2, "Replies": [{"Reply": "We\u2019ll see without your baseless opinion.", "Reply Score": 1}]}, {"Comment": "We\u2019ll see without your baseless opinion.", "Score": 1, "Replies": [{"Reply": "Again, it is on you to convince us that your ramblings are worthwhile. Not the other way around.", "Reply Score": 2}]}, {"Comment": "Again, it is on you to convince us that your ramblings are worthwhile. Not the other way around.", "Score": 2, "Replies": [{"Reply": "Nobody needs to convince specifically you. Trolls will be there trolls.", "Reply Score": 1}]}, {"Comment": "Nobody needs to convince specifically you. Trolls will be there trolls.", "Score": 1, "Replies": [{"Reply": "Lol.", "Reply Score": 2}]}, {"Comment": "Lol.", "Score": 2, "Replies": []}]},{"Title": "Parsers and CAS (Computer Algebra System)", "Score": 41, "URL": "https://i.redd.it/jig59vk2768c1.gif", "CreatedAt": 1703392084.0, "Full Content": "", "CntComments": 1, "Comments": [{"Comment": "Recursive descent, operator precedence - Pratt Parser\r  \nReference: https://www.researchgate.net/publication/258629806\\_A\\_Formalization\\_and\\_Correctness\\_Proof\\_of\\_the\\_CGOL\\_Language\\_System\\_Pratt\\_Parser\r  \n\r  \nComputer Algebra System (CAS) based on Abstract Syntax Tree (AST) with built-in Automatic Simplification\r  \nReference:\r  \nBuchberger, Bruno, et al. Computer Algebra. Springer, 1983.\r  \nCohen, Joel S. Computer Algebra and Symbolic Computation. A K Peters/CRC Press, 19 July 2002.\r  \n\r  \nGithub repository: [https://github.com/silverfoxpt/SFMLFunction](https://github.com/silverfoxpt/SFMLFunction)", "Score": 1, "Replies": []}]},{"Title": "The lost compression algorithm", "Score": 151, "URL": "https://www.reddit.com/r/compsci/comments/18nyfq2/the_lost_compression_algorithm/", "CreatedAt": 1703195702.0, "Full Content": "Hi i am really sorry for my bad English\n\nin 2014 when i was a kid i heard a story about someone went to  Philips company in 90s and he had a crazy algorithm to encrypt the files for really high compress level i can't remember the numbers but i believe it was from 1gb to 1mb something like that anyways the end of the story he died and left  Philips with half of the code.\n\n&#x200B;\n\nnow i can't find that story can anybody help i am very interested\n\nthank you", "CntComments": 62, "Comments": [{"Comment": "Well, any domain-specific compression algorithm can probably compress things down arbitrarily tightly, but that just moves the problem elsewhere. No free lunch.", "Score": 112, "Replies": [{"Reply": "[deleted]", "Reply Score": 72}, {"Reply": "Here's an interesting question though: if you *could* cheat by pre-storing a gigabyte of information in the decompressor, how could you best exploit this to compress new messages? (and specifically new messages - so you can't just store the message in the decompressor)\n\nThe best answer I can think of is an LLM. It can learn any amount of statistics about text, and it will output token probabilities that you can use for entropy coding.", "Reply Score": 9}]}, {"Comment": "Story was quite known in NL due to the book of Eric Smith. Never talked to them about this personally, but there were a couple of contractors at the IT department I worked at, who were involved in The Fifth Force in this demoing period and thought they would become filthy rich.\n\nhttps://en.m.wikipedia.org/wiki/Sloot_Digital_Coding_System", "Score": 28, "Replies": [{"Reply": "> The Sloot Digital Coding System was an alleged data sharing technique that its inventor claimed could store a complete digital movie file in 8 kilobytes of data\n\nlol that\u2019s less than the movie's dialogue as text. That\u2019s 4 screens of an 80x25 terminal.", "Reply Score": 30}]}, {"Comment": "Isn't this almost the plot of Silicon Valley?\n\nRegardless it sounds like a myth. There is a lot of theoretical stuff behind compression. For example \"If some messages come out shorter, at least one must come out longer due to the pigeonhole principle. In practical use, this is generally not a problem, because one is usually only interested in compressing certain types of messages, such as a document in English, as opposed to gibberish text, or digital photographs rather than noise, and it is unimportant if a compression algorithm makes some unlikely or uninteresting sequences larger.\" There's zip bombs that expand from a few kb to petabytes, but it's all junk data.", "Score": 32, "Replies": [{"Reply": "Looks like the story is real, but the algorithm was a myth - or even a fraud.", "Reply Score": 18}, {"Reply": "Right? I just learned about this compression algorithm myth, and as soon as I read \u201cPieper\u201d in the wikipedia article, Silicon Valley came to mind.", "Reply Score": 13}, {"Reply": "Middle out, of course!", "Reply Score": 9}, {"Reply": "Was about to mention middle out compression with a heavy dose of humour. Glad someone beat me to it.", "Reply Score": 4}]}, {"Comment": "https://corecursive.com/sloot-digital-coding-system/", "Score": 30, "Replies": [{"Reply": "And for those that don't want to listen to an hour-long podcast: https://en.wikipedia.org/wiki/Sloot_Digital_Coding_System", "Reply Score": 45}, {"Reply": "Thank you so much", "Reply Score": 3}]}, {"Comment": "He was assassinated by Big Hard Drive.", "Score": 16, "Replies": [{"Reply": "Them boys don't mess around", "Reply Score": 1}]}, {"Comment": "Seems an urban legend to me. If there's any truth on it, the supposed algorithm was almost surely a lossy one.\n\nThere are two forms of compression: lossy and lossless. \n\nLossy compression is used on images, videos and sounds, to reduce size by reducing quality. Some information is lost: one can't recover the original image or sound.\n\nLossless compression does what it says: from the compressed file, you can get back the original file, without loss. But it comes with a price: not every file will be smaller when compressed, some even will be bigger.\n\nThe reason for it comes from information theory. Suppose that a file is made of n bits: there are 2^n possible files with n bits. A perfect compression algorithm would always map each file to a smaller file, say, with k < n bits. But there are: 1 file with 0 bits, 2 files with 1 bit, 2^2 files with 2 bits, ... until 2^(n - 1) files with n - 1 bits, for a grand total of (2^n) - 1 files; since we're trying to fit 2^n files into (2^n - 1) files, one file won't be compressed. And that's the theoretical best.\n\nReference:  \nhttps://en.m.wikipedia.org/wiki/Data_compression", "Score": 19, "Replies": [{"Reply": "[deleted]", "Reply Score": 23}]}, {"Comment": "So they're saying a full feature film in 8k. A 100 minute movie at 24 frames per second would have 158400 individual frames. I'll be generous and say it's 8Ki instead of 8K. That's 65536 *bits*. So on average, they're storing roughly 2.5 frames *per bit*.\n\nNo wonder the guy had a heart attack. The bullshit caught up with him.", "Score": 10, "Replies": []}, {"Comment": "He probably left with the code to decompress it. It is probably a joke. It is even possible to compress 1gb into a byte, but computer scientists are still searching for the  algorithm to decompress the data. \n\nMore seriously, all depends on the content of the 1gb file. If the contained data is highly redundant, it is possible the get high compression ratio, but at the cost that files without redundant data will come out slightly bigger from the compression algorithm. For instance it would result from the addition of a simple flag stating that this file could not be compressed.", "Score": 9, "Replies": [{"Reply": "You can \"decompress\" the data as well. The \"decompressor\" contains up to 256 movies in their entirety and the byte contains the number of the movie to show. Sloot's demo may have worked like that.", "Reply Score": 3}]}, {"Comment": "Is this the inspiration for the plot of Silicon Valley", "Score": 3, "Replies": []}, {"Comment": "Could be an oblique reference somehow to the CD-i marketing disaster and the Bell System monopoly breakup.", "Score": 3, "Replies": []}, {"Comment": "Pretty much a hoax. What was unbelievable yet happened at Philips was PASC.", "Score": 2, "Replies": []}, {"Comment": "I'll look into it. Company I worked with for IT and Security (and just left last week) was big into Phillips partnerships. \nInteresting asf would go great into my dApp I'm building", "Score": 2, "Replies": []}, {"Comment": "I remember some app in the DOS days that claimed to do \"wavelet\" compression. it seemed to work miracles but all it did is store the data on un-indexed sectors of the hard drive, so it could recover the data but only until something wrote over it", "Score": 1, "Replies": []}, {"Comment": "There are clear theoretical limits on how much compression is possible. If you cut the file somewhere in the middle, and both of the possible next bits have a probability of 50%, then the file is not compressible. (Assuming it applies to most cuts) \n\nCompression is only possible if for enough cuts, the probability for the next bit is not divided 50/50.\n\nIf the file is from a certain application domain (for example sound or image file), one may consider lossy compression. This is a separate science  because it depends on how the file is perceived by the receiver.", "Score": 1, "Replies": []}, {"Comment": "Isn't this what Pied Piper from Silicon Valley TV series did?", "Score": 1, "Replies": []}, {"Comment": "I can get infinite compression to /dev/null ... but not doing so well on the decompression side of that ... or maybe the decompression is just really slow ... haven't gotten the first bit back ... yet.", "Score": 1, "Replies": []}, {"Comment": "the end of this story is in fiction section of any library. Aka the things that never existed.\n\nThe are 2 types of compressions. One with the loss of information and another without. \n\nExample of compression with loss of information is JPEG. When we throw out some pixels that are not visible to eyes. Thus we can compress files but we loosing some insignificant information. \n\nWithout loss example is  zip compressions and it already achieves almost maximum compression level (yes, there is mathematical proof). Hence any new algorithm will not do it much better like 1gb to 1 mb.", "Score": 1, "Replies": []}, {"Comment": "You can do stuff like that with zstd compression, if you use a dictionary", "Score": 1, "Replies": []}, {"Comment": "[deleted]", "Score": 72, "Replies": [{"Reply": "    alias compress=\"ln -s\"", "Reply Score": 5}, {"Reply": "Source?", "Reply Score": 1}]}, {"Comment": "Here's an interesting question though: if you *could* cheat by pre-storing a gigabyte of information in the decompressor, how could you best exploit this to compress new messages? (and specifically new messages - so you can't just store the message in the decompressor)\n\nThe best answer I can think of is an LLM. It can learn any amount of statistics about text, and it will output token probabilities that you can use for entropy coding.", "Score": 9, "Replies": [{"Reply": "It would really depend on what kind of information you're dealing with.\n\nAn easy example to think about this would be any sort of thing where you deal with diffs: filesystems and databases and source version control systems and bioinformatics where you store what is different from a fixed default state, maybe.\n\nSo I'm most familiar with the bioinformatics context but basically what they do is they store a 'default genome'; and then at particular offsets within that long string you might say that this particular nucleotide G is instead a C and so on and so forth-- this lets you compress down what would have otherwise been a few gigabasepairs down by a few orders of magnitude by only storing the differences.\n\nYou can think of something similar with git for instance, where someone's change is really just adding a certain line in between two others for instance given the previous history -- instead of reuploading all of the SLOCs.", "Reply Score": 6}, {"Reply": "That's just called a large dictionary.  Check out zstd with a custom dictionary as you'd use it on protobufs.\n\nI think the LLMs would count as lossy decompression, as the result is unlikely a perfect reproduction.", "Reply Score": 1}]}, {"Comment": "> The Sloot Digital Coding System was an alleged data sharing technique that its inventor claimed could store a complete digital movie file in 8 kilobytes of data\n\nlol that\u2019s less than the movie's dialogue as text. That\u2019s 4 screens of an 80x25 terminal.", "Score": 30, "Replies": [{"Reply": "That's obviously video compression snake oil.", "Reply Score": 19}]}, {"Comment": "Looks like the story is real, but the algorithm was a myth - or even a fraud.", "Score": 18, "Replies": []}, {"Comment": "Right? I just learned about this compression algorithm myth, and as soon as I read \u201cPieper\u201d in the wikipedia article, Silicon Valley came to mind.", "Score": 13, "Replies": []}, {"Comment": "Middle out, of course!", "Score": 9, "Replies": []}, {"Comment": "Was about to mention middle out compression with a heavy dose of humour. Glad someone beat me to it.", "Score": 4, "Replies": []}, {"Comment": "And for those that don't want to listen to an hour-long podcast: https://en.wikipedia.org/wiki/Sloot_Digital_Coding_System", "Score": 45, "Replies": [{"Reply": "so... It's MIDI, but for movies.", "Reply Score": 11}]}, {"Comment": "Thank you so much", "Score": 3, "Replies": [{"Reply": "Its a great podcast, give the other episodes a listen too!", "Reply Score": 3}]}, {"Comment": "Them boys don't mess around", "Score": 1, "Replies": []}, {"Comment": "[deleted]", "Score": 23, "Replies": [{"Reply": "Well, theoretically one could say, calculate the average luminance of a frame and round it to either pure black or pure white that could be represented by a single bit, and call it a lossy compression algorithm. The missing half of the frames can be interpolated from the stored datapoints. IMO the viewing experience would lack too much of the source material's nuances for this to be a commercial success.", "Reply Score": 5}]}, {"Comment": "You can \"decompress\" the data as well. The \"decompressor\" contains up to 256 movies in their entirety and the byte contains the number of the movie to show. Sloot's demo may have worked like that.", "Score": 3, "Replies": []}, {"Comment": "    alias compress=\"ln -s\"", "Score": 5, "Replies": []}, {"Comment": "Source?", "Score": 1, "Replies": []}, {"Comment": "It would really depend on what kind of information you're dealing with.\n\nAn easy example to think about this would be any sort of thing where you deal with diffs: filesystems and databases and source version control systems and bioinformatics where you store what is different from a fixed default state, maybe.\n\nSo I'm most familiar with the bioinformatics context but basically what they do is they store a 'default genome'; and then at particular offsets within that long string you might say that this particular nucleotide G is instead a C and so on and so forth-- this lets you compress down what would have otherwise been a few gigabasepairs down by a few orders of magnitude by only storing the differences.\n\nYou can think of something similar with git for instance, where someone's change is really just adding a certain line in between two others for instance given the previous history -- instead of reuploading all of the SLOCs.", "Score": 6, "Replies": []}, {"Comment": "That's just called a large dictionary.  Check out zstd with a custom dictionary as you'd use it on protobufs.\n\nI think the LLMs would count as lossy decompression, as the result is unlikely a perfect reproduction.", "Score": 1, "Replies": [{"Reply": "Because of the way entropy coding works, this would be lossless. Worse predictions just mean more bits per token.", "Reply Score": 1}]}, {"Comment": "That's obviously video compression snake oil.", "Score": 19, "Replies": [{"Reply": "Though, I could easily build a video coding system that can perfectly compress 256 movies into only 1 byte of data. The codec is a rather large binary, though. ;)", "Reply Score": 10}]}, {"Comment": "so... It's MIDI, but for movies.", "Score": 11, "Replies": []}, {"Comment": "Its a great podcast, give the other episodes a listen too!", "Score": 3, "Replies": [{"Reply": "i will thank you, may i ask why i'm getting downvoting ?", "Reply Score": 1}]}, {"Comment": "Well, theoretically one could say, calculate the average luminance of a frame and round it to either pure black or pure white that could be represented by a single bit, and call it a lossy compression algorithm. The missing half of the frames can be interpolated from the stored datapoints. IMO the viewing experience would lack too much of the source material's nuances for this to be a commercial success.", "Score": 5, "Replies": []}, {"Comment": "Because of the way entropy coding works, this would be lossless. Worse predictions just mean more bits per token.", "Score": 1, "Replies": [{"Reply": "What kind of context do you think you'd give an LLM to do that?", "Reply Score": 1}]}, {"Comment": "Though, I could easily build a video coding system that can perfectly compress 256 movies into only 1 byte of data. The codec is a rather large binary, though. ;)", "Score": 10, "Replies": [{"Reply": "> The codec is a rather large binary, though. ;)\n\nOr an ML model.\n\nThe password is a prompt..", "Reply Score": 6}]}, {"Comment": "i will thank you, may i ask why i'm getting downvoting ?", "Score": 1, "Replies": [{"Reply": "I am assuming because people dont know it is a real story. There is a fair amount of crackpot posts on here.", "Reply Score": 6}, {"Reply": "People can downvote (and upvote) for any reason and they don't have to give you the reason. Asking why you got a downvote is usually pretty useless.", "Reply Score": 1}]}, {"Comment": "What kind of context do you think you'd give an LLM to do that?", "Score": 1, "Replies": [{"Reply": "The previous <x> characters of the file. \n\n[Researchers have already done this.](https://arxiv.org/pdf/2309.10668.pdf)", "Reply Score": 1}]}, {"Comment": "> The codec is a rather large binary, though. ;)\n\nOr an ML model.\n\nThe password is a prompt..", "Score": 6, "Replies": [{"Reply": "I read something like this in Asimov's (the scifi monthly magazine)", "Reply Score": 2}]}, {"Comment": "I am assuming because people dont know it is a real story. There is a fair amount of crackpot posts on here.", "Score": 6, "Replies": [{"Reply": "I mean the story is real, yes. The alleged system is bunk though.", "Reply Score": 8}, {"Reply": "I can't understand reddit users. everyone rude here", "Reply Score": 7}]}, {"Comment": "People can downvote (and upvote) for any reason and they don't have to give you the reason. Asking why you got a downvote is usually pretty useless.", "Score": 1, "Replies": []}, {"Comment": "The previous <x> characters of the file. \n\n[Researchers have already done this.](https://arxiv.org/pdf/2309.10668.pdf)", "Score": 1, "Replies": [{"Reply": "Oh that's whacky!  Thanks!", "Reply Score": 1}]}, {"Comment": "I read something like this in Asimov's (the scifi monthly magazine)", "Score": 2, "Replies": [{"Reply": "Link?", "Reply Score": 2}]}, {"Comment": "I mean the story is real, yes. The alleged system is bunk though.", "Score": 8, "Replies": []}, {"Comment": "I can't understand reddit users. everyone rude here", "Score": 7, "Replies": [{"Reply": "I can't quite tell if that's a pun. Considering your username.\n\nBut a bit more seriously, a lot of people on Reddit can be unforgiving about other people not knowing things they think are basic knowledge.", "Reply Score": 4}]}, {"Comment": "Oh that's whacky!  Thanks!", "Score": 1, "Replies": []}, {"Comment": "Link?", "Score": 2, "Replies": [{"Reply": "I'm trying to dig it up, but I read it in print, c2002. The premise is decent especially with the context of our modern chat bots. Humans begin to pickup interstellar communications but they are all at the speed of light. Most are signals from long dead civilizations that send out beacons like \"we were here once.\"  But one signal when assembled becomes very analagous to chatgpt. So imagine an alien race beaming us the weights of an AI like that, and it begins to talk to us. Apparently its not the only alien organization to broadcast AIs like this, but it is more sophisticated that all the others.", "Reply Score": 2}]}, {"Comment": "I can't quite tell if that's a pun. Considering your username.\n\nBut a bit more seriously, a lot of people on Reddit can be unforgiving about other people not knowing things they think are basic knowledge.", "Score": 4, "Replies": [{"Reply": "Yeah i think you're right, but what do you mean by pun my username is random by reddit and there's nothing to do with my name \n\nthat's why i deleted all social media apps like instagram. you can't understand people", "Reply Score": -2}]}, {"Comment": "I'm trying to dig it up, but I read it in print, c2002. The premise is decent especially with the context of our modern chat bots. Humans begin to pickup interstellar communications but they are all at the speed of light. Most are signals from long dead civilizations that send out beacons like \"we were here once.\"  But one signal when assembled becomes very analagous to chatgpt. So imagine an alien race beaming us the weights of an AI like that, and it begins to talk to us. Apparently its not the only alien organization to broadcast AIs like this, but it is more sophisticated that all the others.", "Score": 2, "Replies": [{"Reply": "Sounds cool! But what was the content of this magazine? Stories written by Asimov himself or by others with similar style/genre?", "Reply Score": 1}]}, {"Comment": "Yeah i think you're right, but what do you mean by pun my username is random by reddit and there's nothing to do with my name \n\nthat's why i deleted all social media apps like instagram. you can't understand people", "Score": -2, "Replies": [{"Reply": "It's just that your username contains the word 'rude' and you described people on here as rude. It could have been interpreted as a pun or a joke. So, I wasn't entirely sure.\n\nBy the way, you do get to pick your own username as an alternative to the one that Reddit gives you. (But you only get to change it once.)", "Reply Score": 2}]}, {"Comment": "Sounds cool! But what was the content of this magazine? Stories written by Asimov himself or by others with similar style/genre?", "Score": 1, "Replies": [{"Reply": "Asimov's is a Magazine with short stories from a variety of authors, pro and otherwise, publishers select what makes it", "Reply Score": 1}]}, {"Comment": "It's just that your username contains the word 'rude' and you described people on here as rude. It could have been interpreted as a pun or a joke. So, I wasn't entirely sure.\n\nBy the way, you do get to pick your own username as an alternative to the one that Reddit gives you. (But you only get to change it once.)", "Score": 2, "Replies": [{"Reply": "I will change it buddy thx for advice\n\nand still getting downvoting lol", "Reply Score": 1}, {"Reply": "Damn i can't change my username even tho i created the account using gmail", "Reply Score": 1}]}, {"Comment": "Asimov's is a Magazine with short stories from a variety of authors, pro and otherwise, publishers select what makes it", "Score": 1, "Replies": []}, {"Comment": "I will change it buddy thx for advice\n\nand still getting downvoting lol", "Score": 1, "Replies": []}, {"Comment": "Damn i can't change my username even tho i created the account using gmail", "Score": 1, "Replies": [{"Reply": "That's strange, with me it kept asking me to choose an alternative username.", "Reply Score": 1}]}, {"Comment": "That's strange, with me it kept asking me to choose an alternative username.", "Score": 1, "Replies": [{"Reply": "i think that's because they asked me before and i pressed keep username\n\nthis is so stupid", "Reply Score": 1}]}, {"Comment": "i think that's because they asked me before and i pressed keep username\n\nthis is so stupid", "Score": 1, "Replies": [{"Reply": "Oh no. Maybe it's possible to ask Reddit support?", "Reply Score": 1}, {"Reply": "Sorry mate, you're out of luck..\nhttps://support.reddithelp.com/hc/en-us/articles/204579479-Can-I-change-my-username-", "Reply Score": 1}]}, {"Comment": "Oh no. Maybe it's possible to ask Reddit support?", "Score": 1, "Replies": []}, {"Comment": "Sorry mate, you're out of luck..\nhttps://support.reddithelp.com/hc/en-us/articles/204579479-Can-I-change-my-username-", "Score": 1, "Replies": []}]},{"Title": "Sending addresses via the address bus", "Score": 0, "URL": "https://www.reddit.com/r/compsci/comments/18oghgj/sending_addresses_via_the_address_bus/", "CreatedAt": 1703256236.0, "Full Content": "If we had an address bus that was 8 bits wide, then you'd assume it would be able to access 2^8 (256) memory locations, but the average person would want to access more locations than this. How do we get around this? A couple of obvious ways I can think of would be sending the address over multiple clock cycles or sending a \"row\" and a \"column\" address to access 256\u00d7256 locations. I was wondering what the standard way of accessing more locations is and if there are any more cool ways to do it.\n\nThanks in advance!", "CntComments": 5, "Comments": [{"Comment": "\u201cSending a row and column address\u201d is one version of \u201csending the address over multiple clock cycles\u201d.  In all cases you need some way of identifying the start - and in some cases the end - of an address cycle, which essentially treats the N-1 preceding address words as data words written into a separate \u201ccontrol\u201d space, addressed either explicitly or implicitly by the signal(s) that you use. Bank switching is another take on the same thing.\n\nSome other approaches give you \u201cimplicit\u201d address bits; e.g. a Harvard architecture will have one or more signals indicating the type of access (instruction, data, etc) that are effectively address bits.\n\nThere are also content-addressable stores, where a data tag is presented as part of the address cycle; the referenced location in this case will be derived from the tag (this is how some cache memories work).\n\nSeparately, I think more than a few 8051 programmers would take issue at \u201can average person would want more than this\u201d. 8)", "Score": 5, "Replies": []}, {"Comment": "Check out bank switching in NES cartridge mapper emulations. Writing to certain locations makes the hardware mapper behave differently. So you can't access it all at once, but you can access more than would be permitted within the constraints of the address space and default mappings.", "Score": 3, "Replies": []}, {"Comment": "The [Intel 8085](https://en.wikipedia.org/wiki/Intel_8085) had a multiplexed address bus exactly like this - the \\`ALE\\` pin was enabled when the address bus was holding the high byte, and disabled when the low byte was present. It had a few peripheral chips which understood this protocol as well, so as long as you weren't building anything too complicated, you could get away with an 8-bit address bus + 1 address latch pin.", "Score": 2, "Replies": []}, {"Comment": "PCs even had to do this, though with 16-bit registers in order to access more memory than 2\\*\\*16 bytes. In real mode (a mode that modern Intel x86 CPUs still have for backwards compatibility and booting), it has a mechanism called segmentation, where you'd have segment registers that would act as a base address for various operations.\n\nThis is where the distinction between \"near calls\" and \"far calls\" come from; far calls modify both segment and offset registers.", "Score": 2, "Replies": []}, {"Comment": "The standard method is to just have a wider bus. If you have a CPU, capable of accessing more memory - you might as well give it more address pins.\n\nAnother common method is multiplexing data lines as an additional address lines. This method makes sense, if the memory is at least two times slower than the CPU - the memory cannot respond with data in the first cycle anyway.\n\nRow-column is used specifically for DRAM. It is important to note - for DRAM rows and columns are not just two random parts of the address, they are fundamentally different. Selecting a new row is a lengthy operation, while selecting a column is not. You can access multiple columns after selecting a row. Only rows need update cycles.\n\nUsing multiple address cycles in non-DRAM way is actually rather rare - it only makes sense if pin count is more important than performance. And if pin count is important - you can just go serial, with only 3 or 4 wires for everything.\n\nNote, that all methods so far must be built in into the CPU. They cannot be used to extend address of a CPU beyond what it supports. **Bank switching**, on the other hand, can be made with an external circuitry:\n\n* topmost address lines are provided by an external IO device (most likely - a simple latch). Several such devices can exist - in that case, top few bits of the bus address select which one is used\n* to change those topmost bits, the program have to do an IO write to the device", "Score": 1, "Replies": []}]},{"Title": "Is the problem of \"finding the output given the algorithm halts\" not computable?", "Score": 20, "URL": "https://www.reddit.com/r/compsci/comments/18m2y94/is_the_problem_of_finding_the_output_given_the/", "CreatedAt": 1702995544.0, "Full Content": "Originally posted here [https://cs.stackexchange.com/q/164611](https://cs.stackexchange.com/q/164611)\n\nFor simplicity, let's assume all Turing machines print 0 or 1 on the tape. Consider an algorithm $A$ (which is guaranteed to halt), which, given any Turing machine $T$ as the input, always outputs a single digit $x\\in \\{0,1\\}$, such that x satisfies the condition that if $T$ halts, then the last symbol printed by $T$ is $x.$ \nWhen $T$ does not halt, it does not matter what the output of $A$ is, as long as it halts and outputs 0 or 1.\n\nIs it the case that algorithm $A$ does not exist (and thus the problem is undecidable)?\n\nEDIT: assume that the input of T is bundled with T, so we do not need to care about input.\n\nCLARIFICATION: I have clearly stated that A must halt even when T does not.", "CntComments": 44, "Comments": [{"Comment": "Construct M with the following behavior:\n\n1. Run A(M) => x.\n2. Return (1-x).\n\nThe general theorem for this type of question is https://en.m.wikipedia.org/wiki/Rice's_theorem.", "Score": 24, "Replies": [{"Reply": "Great answer!", "Reply Score": 2}, {"Reply": ">Construct M with the following behavior:\n\nM is putting itself into A?", "Reply Score": 1}]}, {"Comment": "You can show that such an algorithm A cannot exist due to the [Time Hierarchy Theorem](https://en.wikipedia.org/wiki/Time_hierarchy_theorem). Here's a slightly handwavey proof:\n\nIf algorithm A existed (and always halts) then it would run in time f(|T|) for some time-constructible function f.\n\nWe now pick a way harder decision problem P that requires far more time than f(n) to be solved (but is still decidable). Let's say P requires time f(exp(n))+exp(n).\n\nGiven an input I of problem P of size n, we construct a TM T that decides P for input I and outputs 0/1 accordingly. Since P is decidable, there exists a TM T' that decides it, and T' is of constant size. To construct T we just need to give the constant-size description of T' plus the input I, so if T is encoded in any remotely reasonable way the size of |T| = poly(n).\n\nNow running algorithm A on input T would solve P on input I in time f(poly(n)), but by our choice of P it cannot be decided in time f(poly(n)).", "Score": 10, "Replies": []}, {"Comment": "Wait, what is the input of T?\n\nIn any case, it is very much computable. You could just simulate T and print its output.", "Score": 15, "Replies": [{"Reply": ">Wait, what is the input of T?\n\nLet's assume that the input is bundled with T. Then we do not need input.", "Reply Score": 2}, {"Reply": ">In any case, it is very much computable. You could just simulate T and print its output.\n\nThis is not right - I want A to guarantee to halt. That's what an \"algorithm\" means.", "Reply Score": -6}, {"Reply": "You're being thrown off by the title of the post but missed the description.  OP wants to output what T outputs if T halts, but output something arbitrary if T does not halt.  Simulation doesn't work in that case since you need to know whether T halts.  What OP wants is actually undecidable.  In a different comment, /u/ellipticaltable sketched a proof of that.", "Reply Score": 1}]}, {"Comment": "Without getting into the weeds of a formal proof, consider the following.\n\nIf A runs the machine T then it will not halt if T does not Halt.\nSo A must determine the output of T without running it or it must determine whether T will halt or not before running it.\n\nThe second is the halting problem.\nThe first is asking \"can I determine the output of T without running it, and get either a non-sense result or a known result if I can't run it.\" The known result for not able to run it is again the halting problem, and the non-sense result becomes all that's left to explore.\n\nSo, looking at a different field for a second, an algorithm that takes a program and computes it's result without running it is an optimizing compiler.\n\nSo the real meat of your question is, \"can we create a compilable turing-complete language where non-terminating problems only result from undefined behavior?\"\n\nI do not know the answer to that definitively, but that language cannot allow for loops that continue until a condition is met unless that condition is provably met, and that _feels enough like_ the halting problem for me to say:\n\nTL;DR almost certainly no", "Score": 6, "Replies": []}, {"Comment": "Like `tail`?", "Score": 3, "Replies": [{"Reply": "So it\u2019s been edited now. Here\u2019s the original question. \n\n> For simplicity, let's assume all Turing machines print 0 or 1 on the tape. Consider an algorithm A, which, given any Turing machine T as the input, outputs x\u2208{0,1}, satisfying the condition that if T halts, then the last symbol printed by T is x. When T does not halt, it does not matter what the output of A is.\n> Is it the case that algorithm A does not exist (and thus the problem is undecidable)?\n\nThe current edit seems to ask whether A can detect that T won\u2019t halt, which is just the halting problem.", "Reply Score": 5}]}, {"Comment": "Isn't this just the halting problem? Write an algorithm that uses A to decide if the input halts. If it does halt, loop forever. If it doesn't halt, output a value and terminate. Then feed *that* algorithm into A. It becomes an impossible contradiction; if A says the algorithm will halt then it doesn't, and vice versa.", "Score": 5, "Replies": []}, {"Comment": "Edit: It seems I misinterpreted the question.\n\n~~Given a description of any Turing Machine T, we construct a representation of the following Turing Machine U: Simulate T, If T has halted, pad an additional 1 at the end of the tape. (1)~~\n\n~~We can always append a 1 at the end of the tape of any TM using procedure (1)~~\n\n~~Since it doesn't matter what A output when T doesn't halt, we choose it to output 0 by convention.~~\n\n~~Suppose M is a TM that computes A, then~~\n\n~~We construct a Turing Machine R as follows:~~\n\n~~R: On input T, Compute a description of U using T as per (1). Run M on using the description of U as input. Output what M outputs.~~\n\n~~Clearly, R is a TM that decides the halting problem.~~\n\n~~Thus, any such algorithm A must be uncomputable. The problem must be undecidable~~.", "Score": 1, "Replies": [{"Reply": "> Since it doesn't matter what A output when T doesn't halt, we choose it to output 0 by convention.\n\n(What I'm presuming) the point of OPs question is is that we cannot presume anything about the output of A if it is fed a T that does not halt.", "Reply Score": 2}, {"Reply": "Thank you for the answer, but this does not seem to work, because if A does NOT always output 0 when T does not halt, then you cannot obtain this result.", "Reply Score": 1}]}, {"Comment": "Algorithm A does exist, one implementation would be to just simulate T and if T halts, A prints its output. However since T may not halt and no algorithm A exists that can find that out (this would be solving the Halting problem) beforehand and A wouldn't terminate given a T that doesn't terminate. So the problem is that the assumption \"A halts\" doesn't hold.\n\nSo the correct answer here is up to interpretation given the vagueness of the question. I'd answer like this:\n\n> If A does exist, one implementation would be just simulating T. However A doesn't halt if T doesn't halt, making it [undecidable](https://en.wikipedia.org/wiki/Undecidable_problem). This is just a convoluted way of stating the [halting problem](https://en.wikipedia.org/wiki/Halting_problem). Since A however is given to halt but doesn't always halt, the assumption doesn't hold, and such an A cannot exist.\n\nEDIT: The algorithm guys arrived and seemingly instantly agreed on a set of default assumptions. On top of that I misinterpreted the question a bit, though the answer still isn't really wrong, you cannot get the final output of all programs that won't halt, which seems to me is needed to satisfy A.", "Score": 1, "Replies": [{"Reply": ">However A doesn't halt if T doesn't halt, making it   \n>  \n>undecidable  \n>  \n>.\n\nI have now clearly stated that A must halt even when T does not.", "Reply Score": 1}, {"Reply": "Please tell me what is vague so that I can clarify.", "Reply Score": 0}]}, {"Comment": "Let me see if I understand the question: You're asking about whether there is an algorithm that takes in a TM+input, and returns the output of the TM if it halts and gibberish if it does not?", "Score": 1, "Replies": []}, {"Comment": "Rephrased, to make sure I understand:\n\nA is a halting turing machine that, given some other TM 'T' as input, determines whether or not T prints 'x' as it's last symbol before halting.\n\nI call this type of turing machine a 'behavior decider' (e.g. 'T prints 111' or 'T moves left 3 times in a row'), and they're usually undecidable. Let's try to prove it.\n\nSuppose A is decidable. Given some pair of Turing machine M and input string w ( <M,w> ), let's hardcode w into M to create M_w which has the same behavior but runs on empty input. \n\nNow modify M_w such that, if M_w would accept it instead prints 'x' and halts. Call this machine T_w. You may also need to do an additional modification here to guarantee that T_w never accidentally prints 'x' in normal operation, which you could do by simply swapping all instances of 'x' with 'y' or some other symbol.\n\nNow run this T_w through A and A will *decide* whether or not T_w's last printed symbol before halting is 'x'. By extension, this T_w is *deciding* whether or not M accepts w, which is known to be undecidable (the universal language is undecidable). Therefore the assumption that A is decidable is false.\n\nTL;DR if you can detect whether or not a TM prints x before halting, then you can just rig up any undecidable turing machine to print x before accepting and suddenly, thanks to A, it's decidable. So clearly A can't exist.", "Score": 1, "Replies": []}, {"Comment": "Consider the following program P:\n\nx := A(P) // yes! This same program P I'm writing right now\n\nreturn 1 - x\n\nIf we assume A is computable, then P obviously is computable as well\n\nWhat does P return? *What does A say that P returns?*\n\nIs something off?", "Score": 1, "Replies": []}, {"Comment": "Great answer!", "Score": 2, "Replies": []}, {"Comment": ">Construct M with the following behavior:\n\nM is putting itself into A?", "Score": 1, "Replies": [{"Reply": "Yup. It simulates A running on itself.\n\nhttps://en.m.wikipedia.org/wiki/Kleene%27s_recursion_theorem is why we can always assume that a program has access to its own code. Playing around with https://en.m.wikipedia.org/wiki/Quine_(computing) is a good way to gain intuition for it.", "Reply Score": 8}]}, {"Comment": ">Wait, what is the input of T?\n\nLet's assume that the input is bundled with T. Then we do not need input.", "Score": 2, "Replies": [{"Reply": "Alright, then just simply simulate it - just do the steps in A as if it was T. Then whatever it outputs will be the output T would have outputted.", "Reply Score": 4}]}, {"Comment": ">In any case, it is very much computable. You could just simulate T and print its output.\n\nThis is not right - I want A to guarantee to halt. That's what an \"algorithm\" means.", "Score": -6, "Replies": [{"Reply": "But isn\u2019t it already given that T halts? Isn\u2019t that literally in the title? Finding the ouput given the algorithm halts?\n\nEdit: ahhh wait, I might have misunderstood. You want A to print T\u2019s output as if it was T, but still halt otherwise? If yes, this very much feels uncomputable, but I do not have a proof right now.", "Reply Score": 8}]}, {"Comment": "You're being thrown off by the title of the post but missed the description.  OP wants to output what T outputs if T halts, but output something arbitrary if T does not halt.  Simulation doesn't work in that case since you need to know whether T halts.  What OP wants is actually undecidable.  In a different comment, /u/ellipticaltable sketched a proof of that.", "Score": 1, "Replies": [{"Reply": "Yeah, if you check I already \u201cendorsed\u201d his comment as a reply :)", "Reply Score": 1}]}, {"Comment": "So it\u2019s been edited now. Here\u2019s the original question. \n\n> For simplicity, let's assume all Turing machines print 0 or 1 on the tape. Consider an algorithm A, which, given any Turing machine T as the input, outputs x\u2208{0,1}, satisfying the condition that if T halts, then the last symbol printed by T is x. When T does not halt, it does not matter what the output of A is.\n> Is it the case that algorithm A does not exist (and thus the problem is undecidable)?\n\nThe current edit seems to ask whether A can detect that T won\u2019t halt, which is just the halting problem.", "Score": 5, "Replies": [{"Reply": "All I have edited is to add the clarification \"(which is guaranteed to halt)\". This does not change the meaning of the original post at all. It in no ways imply the halting problem in an obvious way!", "Reply Score": -2}]}, {"Comment": "> Since it doesn't matter what A output when T doesn't halt, we choose it to output 0 by convention.\n\n(What I'm presuming) the point of OPs question is is that we cannot presume anything about the output of A if it is fed a T that does not halt.", "Score": 2, "Replies": []}, {"Comment": "Thank you for the answer, but this does not seem to work, because if A does NOT always output 0 when T does not halt, then you cannot obtain this result.", "Score": 1, "Replies": []}, {"Comment": ">However A doesn't halt if T doesn't halt, making it   \n>  \n>undecidable  \n>  \n>.\n\nI have now clearly stated that A must halt even when T does not.", "Score": 1, "Replies": []}, {"Comment": "Please tell me what is vague so that I can clarify.", "Score": 0, "Replies": [{"Reply": "Questions I had are:\n\n- Does A have to be implementable on a Touring Machine?\n- Could A compute a heuristic instead of a result?\n- Does the output assumption regarding T even matter?\n- \"return True\" does exist and decides the problem given the specification of the question - it just isn't exactly useful.", "Reply Score": 1}]}, {"Comment": "Yup. It simulates A running on itself.\n\nhttps://en.m.wikipedia.org/wiki/Kleene%27s_recursion_theorem is why we can always assume that a program has access to its own code. Playing around with https://en.m.wikipedia.org/wiki/Quine_(computing) is a good way to gain intuition for it.", "Score": 8, "Replies": [{"Reply": "Never studied recursion theorem, but even if we're looking at plain old turing machines there seems to be no logical leap in passing the code of a program as input to another program or itself. The code is just a string of input, just like any other string.", "Reply Score": 1}]}, {"Comment": "Alright, then just simply simulate it - just do the steps in A as if it was T. Then whatever it outputs will be the output T would have outputted.", "Score": 4, "Replies": []}, {"Comment": "But isn\u2019t it already given that T halts? Isn\u2019t that literally in the title? Finding the ouput given the algorithm halts?\n\nEdit: ahhh wait, I might have misunderstood. You want A to print T\u2019s output as if it was T, but still halt otherwise? If yes, this very much feels uncomputable, but I do not have a proof right now.", "Score": 8, "Replies": [{"Reply": "Maybe I need more clarification. I said that A must provide an output for any input T **first**. Then I say that, out of all the input, the inputs which halts will lead to a correct output. So my second sentence does not override my first sentence!", "Reply Score": 4}]}, {"Comment": "Yeah, if you check I already \u201cendorsed\u201d his comment as a reply :)", "Score": 1, "Replies": [{"Reply": "Ah, yes you did. I hadn't noticed that.  Sorry for the spam then :-)", "Reply Score": 1}]}, {"Comment": "All I have edited is to add the clarification \"(which is guaranteed to halt)\". This does not change the meaning of the original post at all. It in no ways imply the halting problem in an obvious way!", "Score": -2, "Replies": [{"Reply": "That completely changes the problem since you added the constraint that A must halt. Now it\u2019s the halting problem instead of `tail`.", "Reply Score": 3}]}, {"Comment": "Questions I had are:\n\n- Does A have to be implementable on a Touring Machine?\n- Could A compute a heuristic instead of a result?\n- Does the output assumption regarding T even matter?\n- \"return True\" does exist and decides the problem given the specification of the question - it just isn't exactly useful.", "Score": 1, "Replies": [{"Reply": ">Does A have to be implementable on a Touring Machine?\n\nI have assumed the convention that an \"algorithm\" can run on a Turing machine. (Of course, you might see other conventions at other places.)", "Reply Score": 2}, {"Reply": ">Could A compute a heuristic instead of a result?\n\nWhat is a heuristic?", "Reply Score": 1}, {"Reply": ">\"return True\" does exist and decides the problem given the specification of the question - it just isn't exactly useful.\n\nWhat is \"return True\"?", "Reply Score": 1}]}, {"Comment": "Never studied recursion theorem, but even if we're looking at plain old turing machines there seems to be no logical leap in passing the code of a program as input to another program or itself. The code is just a string of input, just like any other string.", "Score": 1, "Replies": [{"Reply": "The tricky bit isn't passing some program to another program. As you note, it's just a string. The tricky bit is passing *your own code* to another program. That requires a bit of care.", "Reply Score": 3}]}, {"Comment": "Maybe I need more clarification. I said that A must provide an output for any input T **first**. Then I say that, out of all the input, the inputs which halts will lead to a correct output. So my second sentence does not override my first sentence!", "Score": 4, "Replies": [{"Reply": "Then no such algorithm exists. To decide *anything* about the behavior of a Turing-machine, you have to run it. The only way to know what it outputs in case it halts is to run it and since your algorithm is supposed to always terminate, it can\u2019t run the machine.", "Reply Score": 5}]}, {"Comment": "Ah, yes you did. I hadn't noticed that.  Sorry for the spam then :-)", "Score": 1, "Replies": []}, {"Comment": "That completely changes the problem since you added the constraint that A must halt. Now it\u2019s the halting problem instead of `tail`.", "Score": 3, "Replies": [{"Reply": "By the time I say \"A will outputs x\u2208{0,1}\", it is 100% clear that A must halt. **Otherwise A does not have output!** But apologies for not being too obvious.", "Reply Score": -1}]}, {"Comment": ">Does A have to be implementable on a Touring Machine?\n\nI have assumed the convention that an \"algorithm\" can run on a Turing machine. (Of course, you might see other conventions at other places.)", "Score": 2, "Replies": []}, {"Comment": ">Could A compute a heuristic instead of a result?\n\nWhat is a heuristic?", "Score": 1, "Replies": [{"Reply": "It's a nice way of saying \"guess\". E.g. a neural network (or a human) can guess if a program halts by \"looking at it\". Meaning that A would not always be correct.", "Reply Score": 1}]}, {"Comment": ">\"return True\" does exist and decides the problem given the specification of the question - it just isn't exactly useful.\n\nWhat is \"return True\"?", "Score": 1, "Replies": [{"Reply": "Oh sorry, my brain instantly thought halting problem and I didn't get that A should replicate Ts final output if it can.\n\nOkay so here's my 2nd take:\n\nIf T is simulated and halts, does A know somehow? If not, can A just print what T last printed when it guesses T halted(heuristic)? If it guesses wrong AND T doesn't halt, it'd still be correct.", "Reply Score": 1}]}, {"Comment": "The tricky bit isn't passing some program to another program. As you note, it's just a string. The tricky bit is passing *your own code* to another program. That requires a bit of care.", "Score": 3, "Replies": [{"Reply": "Ah right I get it now that makes sense thanks", "Reply Score": 2}]}, {"Comment": "Then no such algorithm exists. To decide *anything* about the behavior of a Turing-machine, you have to run it. The only way to know what it outputs in case it halts is to run it and since your algorithm is supposed to always terminate, it can\u2019t run the machine.", "Score": 5, "Replies": []}, {"Comment": "By the time I say \"A will outputs x\u2208{0,1}\", it is 100% clear that A must halt. **Otherwise A does not have output!** But apologies for not being too obvious.", "Score": -1, "Replies": [{"Reply": "And you also say\n> When $T$ does not halt, it does not matter what the output of $A$ is.\n\nI think it may help to formulate the question better.", "Reply Score": 2}]}, {"Comment": "It's a nice way of saying \"guess\". E.g. a neural network (or a human) can guess if a program halts by \"looking at it\". Meaning that A would not always be correct.", "Score": 1, "Replies": [{"Reply": "I mean the case where A is always correct. (The case where A is not always correct open the way to so many other interesting problems.)", "Reply Score": 1}]}, {"Comment": "Oh sorry, my brain instantly thought halting problem and I didn't get that A should replicate Ts final output if it can.\n\nOkay so here's my 2nd take:\n\nIf T is simulated and halts, does A know somehow? If not, can A just print what T last printed when it guesses T halted(heuristic)? If it guesses wrong AND T doesn't halt, it'd still be correct.", "Score": 1, "Replies": []}, {"Comment": "Ah right I get it now that makes sense thanks", "Score": 2, "Replies": []}, {"Comment": "And you also say\n> When $T$ does not halt, it does not matter what the output of $A$ is.\n\nI think it may help to formulate the question better.", "Score": 2, "Replies": [{"Reply": "Sure. Thanks for the suggestion.", "Reply Score": 1}]}, {"Comment": "I mean the case where A is always correct. (The case where A is not always correct open the way to so many other interesting problems.)", "Score": 1, "Replies": []}, {"Comment": "Sure. Thanks for the suggestion.", "Score": 1, "Replies": []}]},{"Title": "is Diffie-Hellman symmetric or asymmetric?", "Score": 13, "URL": "https://www.reddit.com/r/compsci/comments/18lyl3f/is_diffiehellman_symmetric_or_asymmetric/", "CreatedAt": 1702980772.0, "Full Content": " [Difference Between Diffie-Hellman and RSA - GeeksforGeeks](https://www.geeksforgeeks.org/difference-between-diffie-hellman-and-rsa/) \n\n>Symmetric vs. Asymmetric: Diffie-Hellman is a symmetric-key algorithm, while RSA is an asymmetric-key algorithm. This means that Diffie-Hellman uses the same key for encryption and decryption, while RSA uses different keys for encryption and decryption.\n\naccording to this\\^ it is symmetric? but i'm pretty sure it is not. So is this page just wrong or am i just stupid?", "CntComments": 14, "Comments": [{"Comment": "DH isn\u2019t an encryption algorithm- it\u2019s a key exchange algorithm (similar to the same idea as asymmetric encryption) that allows both parties to agree on an identical key without A having to give a key to B which would\u2019ve allowed for the possibility of interception. So while neither parties CHOOSE the key explicitly, both calculate the same key in the end and so can use that with symmetric encryption.", "Score": 31, "Replies": []}, {"Comment": "Asymmetric algorithms use two different keys. One for encryption and one for decryption. These keys have to be generated as a pair. \n\nSymmetric algorithms use one key which is the same for both encryption and decryption.\n\nDiffie-Hellman is a process by which two parties can independently create the same single key. This key would then be used with a symmetric algorithm.\n\nEdit: I just read the article linked in the post and I would take everything it says with a big grain of salt. It contradicts itself, makes a couple of nonsensical points, and just generally reads like it was written by a bad AI. I'm sure even ChatGPT could do a much better job.", "Score": 6, "Replies": []}, {"Comment": "Both are asymmetric. The difference is that (textbook) RSA is explicitly asymmetric and DH requires random key exchange on top of PSK. Which makes it asymmetric.", "Score": 3, "Replies": [{"Reply": "I don\u2019t get what the PSK has to do with this. With DH you connect with a random party on the internet ( because DH does not verify that party, from our POV  it is like chat roulette), but then no other can spy on your communication.", "Reply Score": 7}]}, {"Comment": "Geeksforgeeks has had a huge problem with people using ChatGPT to generate articles and this is a great example of that problem.\n\nThe Diffie-Hellman key exchange method allows for asymmetrical encryption schemes. \n\nSymmetrical encryption is pretty simple when it comes to a key, it\u2019s just one variable k. The public key system involves cyclic groups, prime numbers, and all this other stuff that\u2019s like a whole chapter in the Intro to Modern Cryptography textbook, which you should pick up and read if you want to actually understand cryptography instead of reading G4G articles", "Score": 3, "Replies": [{"Reply": "How does DH allow asymmetrical encryption schemes (it can only encrypt)?", "Reply Score": 0}]}, {"Comment": "If you want to compare, you should rather use El Gamal instead of DH (DH allows generating the key, while El Gamal is the encryption algorithm itself, which is asymmetric)", "Score": 1, "Replies": []}, {"Comment": "I don\u2019t get what the PSK has to do with this. With DH you connect with a random party on the internet ( because DH does not verify that party, from our POV  it is like chat roulette), but then no other can spy on your communication.", "Score": 7, "Replies": [{"Reply": "DH is used to agree on a key, typically to be used for subsequent symmetric encryption. But I\u2019d also say that DH isn\u2019t asymmetric.", "Reply Score": 6}]}, {"Comment": "How does DH allow asymmetrical encryption schemes (it can only encrypt)?", "Score": 0, "Replies": [{"Reply": "Via el Gamal: for a given generator g of the group G, p large prime:\nAlice picks sk, then pk = g^sk mod p (DH)\nBob wants to encrypt m, picks some random k and send (C_1, C_2)=(g^k , m*pk^k )\nAlice decrypt: m = C_2/C_1^sk\n\ne: ideally |G| is prime", "Reply Score": 2}, {"Reply": "DH is a system for creating a public key and a shared key between two people communicating. What you said about \u201cit can only encrypt\u201d doesn\u2019t make a lot of sense.\n\nThe public key is used for encryption\nThe shared key (private key) is used for decryption \n\nIt itself is not the set of algorithms making up any scheme but it is the mechanism for which we can create asymmetric cryptography schemes.", "Reply Score": 2}]}, {"Comment": "DH is used to agree on a key, typically to be used for subsequent symmetric encryption. But I\u2019d also say that DH isn\u2019t asymmetric.", "Score": 6, "Replies": [{"Reply": "AFAIK Yes, once both parties are authenticated and that the session key is known by both caller and callee, the encryption scheme is symmetric until the end of the session (for practical performance reasons if I remember well).", "Reply Score": 1}, {"Reply": "Yeah, after a night of sleep, I remember.", "Reply Score": 1}]}, {"Comment": "Via el Gamal: for a given generator g of the group G, p large prime:\nAlice picks sk, then pk = g^sk mod p (DH)\nBob wants to encrypt m, picks some random k and send (C_1, C_2)=(g^k , m*pk^k )\nAlice decrypt: m = C_2/C_1^sk\n\ne: ideally |G| is prime", "Score": 2, "Replies": [{"Reply": "So you need to extend DH in order for it to be usable beyond its purpose of agreeing on a shared secret.  \n\n\nSince most asymmetric crypto systems dont require DH at all I would not define this as its primary purpose (and again, DH on itself cannot achieve this anyway).", "Reply Score": 1}]}, {"Comment": "DH is a system for creating a public key and a shared key between two people communicating. What you said about \u201cit can only encrypt\u201d doesn\u2019t make a lot of sense.\n\nThe public key is used for encryption\nThe shared key (private key) is used for decryption \n\nIt itself is not the set of algorithms making up any scheme but it is the mechanism for which we can create asymmetric cryptography schemes.", "Score": 2, "Replies": [{"Reply": "The only values encrypted by DH are the secrets (a, b) of each party. And the whole purpose of DH is for them to never be known by anyone besides the ower.\n\nIts important to be precise. As you said, the idea of DH can be extended to create an asymmetric crypto system, but DH itself is none.", "Reply Score": 1}]}, {"Comment": "AFAIK Yes, once both parties are authenticated and that the session key is known by both caller and callee, the encryption scheme is symmetric until the end of the session (for practical performance reasons if I remember well).", "Score": 1, "Replies": []}, {"Comment": "Yeah, after a night of sleep, I remember.", "Score": 1, "Replies": []}, {"Comment": "So you need to extend DH in order for it to be usable beyond its purpose of agreeing on a shared secret.  \n\n\nSince most asymmetric crypto systems dont require DH at all I would not define this as its primary purpose (and again, DH on itself cannot achieve this anyway).", "Score": 1, "Replies": []}, {"Comment": "The only values encrypted by DH are the secrets (a, b) of each party. And the whole purpose of DH is for them to never be known by anyone besides the ower.\n\nIts important to be precise. As you said, the idea of DH can be extended to create an asymmetric crypto system, but DH itself is none.", "Score": 1, "Replies": []}]},{"Title": "Indexing and searching for a polygon regardless of it's orientation / scale", "Score": 12, "URL": "https://www.reddit.com/r/compsci/comments/18klgt8/indexing_and_searching_for_a_polygon_regardless/", "CreatedAt": 1702831806.0, "Full Content": "It's been a while since I've worked on a real compsci problem, but since I've been enjoying the Advent of Code so much this year I came up with my own problem which I've been trying to tackle this weekend.\n\nGiven any polygon (as a set of vertices) I want to be able to query a collection of previously indexed polygons for the most similar match, in a way that does not depend on the polygon being in a specific orientation or scale. \n\n[Here's my current approach](https://github.com/Timmoth/PolyMatcher)   \nIdentification:  \n\\- Sort the vertices in ascending order by their distance to the centroid. This order will be the same for any orientation / scale.\n\n\\- Calculate the angle between each pair of points and the centroid. (If there are an odd number of vertices, pair the last vertex with the first)\n\n\\- Return an array of floating point angles in radians.\n\nComparison:\n\n\\- Calculate the Euclidean distance between two arrays of floating point angles in radians.\n\n\\- The lower the Euclidean distance the better the match (between 0.0 and 1.0)  \n\n\nI'm still unsure if my solution works for all scenarios, and am currently trying to think of edge cases and add tests for them. At the moment its all in memory, but I hope to learn a bit more about databases by integrating it into one.  \n\n\nI'd love to know if there are any algorithms or areas of computer science you think I should research that might be applicable to this problem.  \nOr if you have any feedback for my solution i'd really like to hear it! ", "CntComments": 9, "Comments": [{"Comment": "Apologies, I'm a mathematician.\n\nI think this comparing two n-gons modulo translations and rotations and scaling can be done in nlogn time using fast convolutions (i.e. fast Fourier transforms).\n\nRepresent an n-gon by an n-dimensional vector v whose entries v1,...,vn are complex numbers (the vertices of the polygon). Without loss of generality, we may assume that all polygons average to 0 (i.e. the origin is the centroid). For two n-gons u,v, we'd like to know if they're multiples of one another. Obviously, you could compute the entrywise quotient u./v, but this is not what one should do.\n\nInstead, compute the inner product <u,v>. This is because |<u,v>| = ||u|| ||v|| if and only if u and v are collinear.\n\nWe now need to handle rotations. The first thing is to replace u by u/u[0], this has the effect of rotating (and rescaling) so that u[0]=1.\n\nWe also want to normalize v in a similar manner, but we also need to possibly shift the entries of v in a circular or periodic manner, because we don't know which entry of v should be sent to 1. To that end, for a shift s=0,...,n-1, define a new vector h[s] = <u[t],v[t-s]>. The quantity t-s should be interpreted mod n. Note that the vector h can be computed in O(n^2 ) operations.\n\nThen, |h[s]| = ||u|| ||v|| if and only if u and v are parallel, after a rotation by s.\n\nThe final step is to observe that h is given by a convolution, so it can be computed in O(nlogn) operations via fast Fourier transforms.\n\nIf you want to estimate the distance, use the \"polarization identity\" ||u-v||^2 = ||u||^2 + ||v||^2 - 2\u211c<u,v>, which is helpful since h[s] = <u[t],v[t-s]>.", "Score": 5, "Replies": []}, {"Comment": "I'm not sure the angles between points from centroid after sorting the points by distance from centroid makes sense. It seems like you'd be losing important information. Also, I think you should be comparing the distance from the centroid to the points, not just the angles, because distances have shape information that angles alone don't convey. \n\nWhy don't you just rotate the polygon into some canonical orientation (such as putting the furthest vertex from centroid up) and then resizing the shape such that the top point's distance from centroid is 1? You can then just compare the list of points.\n\nAlso, do you want to compare polygons with different numbers of vertices somehow?", "Score": 3, "Replies": [{"Reply": "Thanks for the input!   \n\n\nI've adjusted the code to take the distance into account too which I think helps.   \n\n\nI had considered that but I was getting hung up on finding a property which would always result in the same orientation - for instance if it was based on the furthest vertex there could be a shape with two vertices the same maximum distance from the centroid.\n\nI hadn't thought of comparing polygons with different vertices, but I'll definitely move onto that next!", "Reply Score": 1}]}, {"Comment": "I took a computer vision class when I was at university back in 2003. I remember my professor talking about her research work to deal with object recognition independent of transformation, though it was more advanced than the work we actually did in the class.\n\nI found [this slide deck from her that covers the approach](https://www.cse.psu.edu/~rtc12/CSE486/lecture32.pdf). Note that the technique is generally meant for object recognition in bitmap images, which is not the same problem that you're trying to solve. But maybe her technique will be useful to you.", "Score": 2, "Replies": [{"Reply": "Thanks so much for this! I'm just having a read at the bit on Dimensionality Reduction. It's definitely given me some much needed direction.", "Reply Score": 1}]}, {"Comment": "You might be interested in [Hu Moments](https://learnopencv.com/shape-matching-using-hu-moments-c-python/)", "Score": 1, "Replies": []}, {"Comment": "I'd be tempted to calculate a bunch of metrics:\n\nCircular aberration:  if a circle of the same area is drawn on the centroid, what percent of the polygon is outside the circle.\n\nMajor Axis to Minor Axis Ratio:\n\nMaximal Perpendicular Axis Ratio:\n\nSquare your normalized distances, and recalculate the whole list.\n\nSuperimpose a mirror (and /or rotated) polygon on it and recalculate the list again, add in deltas for the mirroring/rotation.  This helps classify symmetry issues.\n\nThese will help comparing  polygons of different vertex counts. \n\nMy issue with your approach is:\nIf you make a polygon that is identical in shape and perimeter, but uses excess vertices, I suspect that your approach will not classify the polygons as the same.\n\nFeel free to disabuse me of my ignorance.", "Score": 1, "Replies": []}, {"Comment": "Thanks for the input!   \n\n\nI've adjusted the code to take the distance into account too which I think helps.   \n\n\nI had considered that but I was getting hung up on finding a property which would always result in the same orientation - for instance if it was based on the furthest vertex there could be a shape with two vertices the same maximum distance from the centroid.\n\nI hadn't thought of comparing polygons with different vertices, but I'll definitely move onto that next!", "Score": 1, "Replies": [{"Reply": ">I was getting hung up on finding a property which would always result in the same orientation\n\nSo I didn't want to mention this before as I thought it might confuse you but a simple way to guarantee a canonical orientation is the following. \n\nStart by putting all the vertex distances/angles from the centroid into an array, starting from any vertex, in the order of the vertices. The problem you have is how to find a canonical vertex to start from, but we'll determine that after having an array in hand.\n\nNext, you take your array of numbers and you rotate it, that is, you shift all the numbers 1 step to the right and move the last number to the front. Rotate the array repeatedly until it comes back to its starting order.\n\nEach array you produce after rotating is still describing the same polygon, just starting from a different vertex. You now pick the array that comes first if you had to sort all the rotated arrays in lexicographic order, that is, the way you'd sort strings (sort by first element, then, from those with the same first element, sort by second element, and so on). This will always give you an array starting from the same vertex regardless of how it's rotated.\n\nYou can also reflect the polygon (reverse the array) and include those rotations in the list of arrays to have a representation that is reflection invariant as well apart from rotation invariant.\n\nNote that the way you're performing the search is to get an exact match rather than a fuzzy one (get the most similar looking polygon) because two canonically orientated polygons might not give you the most overlap (I imagine that overlap, also called intersection, is the best similarity measure to use). If you want to get the most similar polygon rather than an exact match then you'd need to perform these array rotations on one of the polugons during comparison and pick the rotation that gives maximum similarity to the other polygon. I'm not sure of there's a way to do this without having to perform rotations during comparison.", "Reply Score": 1}]}, {"Comment": "Thanks so much for this! I'm just having a read at the bit on Dimensionality Reduction. It's definitely given me some much needed direction.", "Score": 1, "Replies": [{"Reply": "Happy to help! Funny how some nugget of knowledge from 20 years ago might come in handy.\n\nI remember when she was describing her research, I had one of those \"aha!\" moments. It was definitely one of my favorite classes, but I suspect that most of it is now obsolete by ML-based approaches to image recognition.", "Reply Score": 1}]}, {"Comment": ">I was getting hung up on finding a property which would always result in the same orientation\n\nSo I didn't want to mention this before as I thought it might confuse you but a simple way to guarantee a canonical orientation is the following. \n\nStart by putting all the vertex distances/angles from the centroid into an array, starting from any vertex, in the order of the vertices. The problem you have is how to find a canonical vertex to start from, but we'll determine that after having an array in hand.\n\nNext, you take your array of numbers and you rotate it, that is, you shift all the numbers 1 step to the right and move the last number to the front. Rotate the array repeatedly until it comes back to its starting order.\n\nEach array you produce after rotating is still describing the same polygon, just starting from a different vertex. You now pick the array that comes first if you had to sort all the rotated arrays in lexicographic order, that is, the way you'd sort strings (sort by first element, then, from those with the same first element, sort by second element, and so on). This will always give you an array starting from the same vertex regardless of how it's rotated.\n\nYou can also reflect the polygon (reverse the array) and include those rotations in the list of arrays to have a representation that is reflection invariant as well apart from rotation invariant.\n\nNote that the way you're performing the search is to get an exact match rather than a fuzzy one (get the most similar looking polygon) because two canonically orientated polygons might not give you the most overlap (I imagine that overlap, also called intersection, is the best similarity measure to use). If you want to get the most similar polygon rather than an exact match then you'd need to perform these array rotations on one of the polugons during comparison and pick the rotation that gives maximum similarity to the other polygon. I'm not sure of there's a way to do this without having to perform rotations during comparison.", "Score": 1, "Replies": []}, {"Comment": "Happy to help! Funny how some nugget of knowledge from 20 years ago might come in handy.\n\nI remember when she was describing her research, I had one of those \"aha!\" moments. It was definitely one of my favorite classes, but I suspect that most of it is now obsolete by ML-based approaches to image recognition.", "Score": 1, "Replies": []}]},{"Title": "What typical beginners problems become obsolete with new programming languages?", "Score": 0, "URL": "https://www.reddit.com/r/compsci/comments/18km00x/what_typical_beginners_problems_become_obsolete/", "CreatedAt": 1702833214.0, "Full Content": "People advertising for new languages, especially \"functional\" ones often boast \"see how easy it is to write quicksort in it\" and show the version with two new arrays filtered out of original and then sorted recursively. True mammoths of programming despise this generally as the method is not exactly quicksort since it doesn't work \"in-place\".\n\nI'm trying to collect what other problems become completely spoiled, e.g.:\n\n* reversing string - in languages like C or Pascal you need to do it in-place, walking from both ends with two pointers and swapping characters - in most nowadays languages strings are immutable, reversing functions are often built-in (or it is `[::-1]` in case of Python) so it takes more time to explain to students it is a problem at all\n* rotating string - again, in-place it could be done with whimsical applying of two reverts, but in modern languages it's just concatenation of two substring calls\n* modular arithmetics - calculating long chain of additions and multiplications and taking modulo at the end (like [this](https://www.codeabbey.com/index/task_view/modular-calculator)) - normally this makes student to think about overflow and understand applying of modulo after every other operation - but in Python specifically numbers are silently switching to infinite arithmetics - other languages nowadays have built-in bigints and so on.\n* rotating square matrix 90 degrees (useless for matrix but useful for images) is performed in-place by mirroring against main diagonal and then flipping horizontally or vertically (advanced version of rotate string, perhaps) - though again with modern languages people rarily understand the idea of doing anything \"in-place\" (and with modern garbage collectors and typical ram sizes it is not that bad, of course).\n\nThus I would be thankful if you can help adding more examples to this list.", "CntComments": 13, "Comments": [{"Comment": "First: There is no definitive \"official\" definition of quicksort that requires it to be in-place, so these despises are wrong.\n\nAlso, I highly doubt that \"modern language people\" do not understand or do not care for in-place computation. Memory is not an issue for most applications, but copying large data structures again and again is. I do not see such tendencies in the communities of Scala, Rust, or Kotlin. Purely functional languages also allow in place computation, but advanced functional programming is just not the content of a typical introductory example.", "Score": 12, "Replies": []}, {"Comment": "I have never heard of anyone saying that functional programming is good because you can easily write quick sort with it, nor heard of anyone complaining that/if it would be.\n\nAlso, not everything in software engineering is about doing things \"cool\" and as fast as possible. Unless you are working on embedded hardware, the ability to work fast is usually more important than for your code to run fast.\n\n>in languages like C or Pascal you need to do it in-place, walking from both ends with two pointers and swapping characters - in most nowadays languages strings are immutable, reversing functions are often built-in (or it is \\[::-1\\] in case of Python) so it takes more time to explain to students it is a problem at all\n\nFirst, no, in C you don't need to do it in-place; I can write you a C function that constructs a new char array that is a reversed copy of another one, while leaving the origin unchanged.\n\nSecond, I'd much rather explain students why \"You have to copy the entire string\" can be too slow in some cases rather than why as an engineer in a team on a large project, having immutable Strings is probably a good thing.\n\n>calculating long chain of additions and multiplications and taking modulo at the end (like this) - normally this makes student to think about overflow and understand applying of modulo after every other operation - but in Python specifically numbers are silently switching to infinite arithmetics - other languages nowadays have built-in bigints and so on.\n\nThat's such backwards logic. Why do you even want your students to use C? If they were working in Assembly, they'd really have to think about bytes and registers and understand how branch prediction works. How about we go even further back and let them plug together transistors so they understand how electrons flow when they're trying to build a hash table?\n\n&#x200B;\n\nAlso, libraries have been a thing since forever, not just since mainstream languages have functional elements built in.", "Score": 10, "Replies": [{"Reply": "The flow of electrons is only tangentially related to hash tables. It's really the electromagnetic fields performing the hashing/storage.", "Reply Score": 1}]}, {"Comment": "Huh?", "Score": 4, "Replies": []}, {"Comment": "The point about modular arithmetic doesn't hold like you think. Correctness might not matter, but keeping results small by reducing at each step is important for speed and sometimes space.", "Score": 2, "Replies": []}, {"Comment": "To answer the title only: null pointer exceptions, and unchecked exceptions in general, for example.", "Score": 2, "Replies": []}, {"Comment": "Beginners problems don't become obsolete with new languages as they are the only way to learn what higher level languages are doing for you.   Now you might argue that higher level languages mean you don't need to understand such stuff but I'd say you are wrong!!!!   Beginners benefit massively by actually implementing a few data structures and algorithms themselves.   Those long standing problems for student are not spoiled by new languages, if anything new students learn to value the capabilities of these new languages and still have the background to implement special solutions when standard libraries fail them.", "Score": 2, "Replies": []}, {"Comment": "* My favorite example for an elegant recursive algorithm (which hence meshes well with functional programming) would not be quicksort, but mergesort, since mergesort is hard to write in-place anyways, and you only really need operations that can be implemented neatly functionally (sorted merge, splitting a list), while guaranteeing asymptotically optimal linearithmic time complexity (very much unlike quicksort, and the number one technique to make quicksort fast, choosing random pivots, is very inconvenient in a pure functional language).\n* \"True mammoths of programming\" do not despise missing constant factor optimizations, especially if they get other benefits for it (such as a simpler / safer / quicker implementation).\n* Reversing is not much of a problem, and very often you do not want to mutate the original object. Besides, writing a functional reverse is a nice exercise too.\n* Rotating strings seems like a very artificial problem. In terms of asymptotic time complexity, there is again no issue with the substring calls, and again you will often not want to do operations in-place, and don't care about the savings in auxiliary memory usage.\n* Modular arithmetics are still needed. Using bigints means some integer operations are exact, but performance becomes a concern as integers grow. Especially cryptographic algorithms usually work with modulo arithmetics (on bigints). Apart from that, you very often don't want modulo arithmetic; overflow is most often just a bug.\n* A functional transposition is again a good exercise, and again as efficient, asymptotically.\n\nTL;DR: Usually, memory usage (and many constant factors, sometimes even the asymptotic time complexities) are simply a very minor concern. The top priority is producing a correct and maintainable program quickly. Low-level optimizations have their place, but are not a \"mandatory\" part of programming; being able to brush details like memory management aside (by using a language with a GC) lets you focus on the more important stuff, and if you do happen to need that performance, you can always go lower level again. (For some sub-areas like systems programming, you even have to.)", "Score": 2, "Replies": []}, {"Comment": "Programming languages don't obsolete beginner problems, ecosystems do.\n\nWhat's hard in programming is doing something like making an Http call.  A new programming language is unlikely to have a good http client, more mature languages will have several.", "Score": 1, "Replies": []}, {"Comment": "the point of the problems are not the solutions. learning to solve quadratic equations did not become obsolete with the invention of wolfram alpha.", "Score": 1, "Replies": []}, {"Comment": "Most other languages:\n\n    found = false\n\n    while !found {\n\n      if lookForThingIn(slice[i]) {\n\n        mutate(slice[i])\n\n        found = true\n\n      }\n\n    }\n\n    if !found { createAndAddThingTo(slice) }\n\nPython:\n\n    for i, thing in slice:\n  \n      if thing.isWhatWeWant:\n   \n        mutate(thing)\n\n        break\n\n    else:\n\n      createAndAddThingTo(slice)\n\n(coding standards / formatting or whatever, but you get the idea)", "Score": 1, "Replies": []}, {"Comment": "The flow of electrons is only tangentially related to hash tables. It's really the electromagnetic fields performing the hashing/storage.", "Score": 1, "Replies": []}]},{"Title": "Undecidable problems for cryptography?", "Score": 16, "URL": "https://www.reddit.com/r/compsci/comments/18ibf4k/undecidable_problems_for_cryptography/", "CreatedAt": 1702568580.0, "Full Content": "I wonder if it's possible to build cryptographic protocols based on security provided by undecidable problems such as the halting problem, which are uncomputable from definition. \nIf there's a way we are able to do this(probably build one way functions from an uncomputable function), wouldn't it imply ultimate security even in the PQC era? \n\nPlease excuse my naivety if this seems absurd\nThanks", "CntComments": 19, "Comments": [{"Comment": "The idea of NP-complete problems is that it is hard to find a solution but easy to check one once it is provided. This asymmetry is what makes it useful for cryptography. With undecidable problems you cannot easily check a solution: the solution to the halting problem is just yes/no, but to check it you essentially have to solve the problem again.", "Score": 27, "Replies": [{"Reply": "Current asymmetrical crypto relies on e.g. integer factorization which isn\u2019t known to be, and is believed not to be, NP complete.", "Reply Score": 5}, {"Reply": "Interestingly enough that\u2019s not entirely true anymore if you consider quantum computing because there\u2019s a quantum interactive proof for RE, so you could in fact check a solution to to the halting problem without solving the halting problem", "Reply Score": -1}, {"Reply": "The ability to check the solution is not that relevant unless you want pragmatic things such as the computer not freezing forever when you enter the wrong password. You could devise an algorithm that terminates and outputs a specific message if and only if the correct key is provided\u2013and does not halt for any wrong key.", "Reply Score": -1}]}, {"Comment": "I don't think it's absurd, but there are practical reasons why you might not. To elaborate, let's come up with sensible (?) restrictions we have on the algorithm:\n\n* The successful decryption happens in *reasonable* (polynomial, maybe even linear) time and space.\n* The key is given to the algorithm and is bound in size by N.\n\nThis means that cracking the message could look something like this: you run 2^N instances of decrypt with all possible keys in parallel for P(N) steps. This is EXPTIME and this is no harder than methods based on non-undecidable methods. More than that: you get a set of keys that didn't give a result in time and they cannot be candidates for the real key. If instead all finished, you'd have 2^N candidate solutions and no a priori way to distinguish between them. So you are better off having them all finish.", "Score": 4, "Replies": []}, {"Comment": "In a way white-box cryptography relies on undecidability for its security. The problem it relies on being intractable to solve is \"does this program and this other program do the same thing?\", which is undecidable in general.\n\nThe problem is that the \"in general\" is doing a lot of heavy lifting there, and no one has (to my knowledge) come up with a way to make a specific useful set of programs that are provably computationally hard to deobfuscate.", "Score": 4, "Replies": []}, {"Comment": "Not really but you can use restricted versions of undecidable problems. For example Time bounded Kolmogorov Complexity has been used to design key exchange.", "Score": 5, "Replies": []}, {"Comment": "It\u2019s not absurd, it is similar to known techniques.\n\nYou could create an equivalent condition of zero advantage by sending hardware generated random numbers between two endpoints.\n\nIt\u2019s a good place to start because it\u2019s export-friendly and can\u2019t get you into any trouble.\n\nIt doesn\u2019t matter how many times you encrypt that type of data with an export-grade cipher, the adversary gains no advantage because the one correct version of the output has no meaning.", "Score": 2, "Replies": []}, {"Comment": "crypto problems have to be hard on average. np is just hard somewhere. undecidable is not quite either: it\u2019s saying something is not computable. both np and crypto are computable (because, well, you need to compute the solution)\n\nhope this helps", "Score": 2, "Replies": []}, {"Comment": "The halting problem has 2 possible answers, yes and no. So you just check them both. Even if you do a hundred halting problems in a row, now you have the problem of figuring out how *anyone* can verify it. Cryptography isnt useful without verification. So im not sure if this is possible or how youd do it.\n\nYou could do \"hard\" problems. Like in theory create a long unsorted list pseudorandomly generated from a key, then sort it. Verifying the order of a sort is linear time, sorting it is N*logN. A bad example because the time complexity isnt high enough to make it truly assymetrical, but this could give you an idea.", "Score": 1, "Replies": []}, {"Comment": "Current asymmetrical crypto relies on e.g. integer factorization which isn\u2019t known to be, and is believed not to be, NP complete.", "Score": 5, "Replies": []}, {"Comment": "Interestingly enough that\u2019s not entirely true anymore if you consider quantum computing because there\u2019s a quantum interactive proof for RE, so you could in fact check a solution to to the halting problem without solving the halting problem", "Score": -1, "Replies": [{"Reply": "That\u2019s not what MIP* means. For interactive proofs one does not assume the prover meets any notion of efficiency. They are \u201call knowing\u201d. The interactive proof can be seen as the (computationally bounded) verifier interrogating these all knowing provers. The quantum part here discusses how the all knowing provers are allowed to coordinate. Rather than individually interrogating the provers (and perhaps catching their lies) the quantum provers get a certain degree of entanglement that they can use to (weakly) communicate/coordinate.\n\nThis is to say that provers in MIP*  aren\u2019t implementable using quantum computing.", "Reply Score": 4}]}, {"Comment": "The ability to check the solution is not that relevant unless you want pragmatic things such as the computer not freezing forever when you enter the wrong password. You could devise an algorithm that terminates and outputs a specific message if and only if the correct key is provided\u2013and does not halt for any wrong key.", "Score": -1, "Replies": [{"Reply": "But that is only in theory. In practice, you just wait a couple seconds and then assume it is not going to halt, because you know that for the correct answer it should not take that long.", "Reply Score": 2}]}, {"Comment": "That\u2019s not what MIP* means. For interactive proofs one does not assume the prover meets any notion of efficiency. They are \u201call knowing\u201d. The interactive proof can be seen as the (computationally bounded) verifier interrogating these all knowing provers. The quantum part here discusses how the all knowing provers are allowed to coordinate. Rather than individually interrogating the provers (and perhaps catching their lies) the quantum provers get a certain degree of entanglement that they can use to (weakly) communicate/coordinate.\n\nThis is to say that provers in MIP*  aren\u2019t implementable using quantum computing.", "Score": 4, "Replies": [{"Reply": "I know that, so could you explain part of what I actually said is incorrect? \n\nMy comment claims that the *verifiers* (which ARE computationally bounded) can *verify* a solution in MIP*. So you can check the solution is correct without actually solving the halting problem.\n\nThe comment above me is saying that a verifier essentially needs to solve the halting problem in order to know if the solution is YES/NO, but a quantum verifier in fact does not. It only needs to be given a proof.", "Reply Score": 1}]}, {"Comment": "But that is only in theory. In practice, you just wait a couple seconds and then assume it is not going to halt, because you know that for the correct answer it should not take that long.", "Score": 2, "Replies": [{"Reply": "That's true, but that still makes it an interesting idea: If every attempt to crack the message takes a second, it's infeasible to brute force. Though that isn't restricted to undecidable problems and taking a variable time to decode depending on the key leaves timing-based side channels wide open.", "Reply Score": -1}]}, {"Comment": "I know that, so could you explain part of what I actually said is incorrect? \n\nMy comment claims that the *verifiers* (which ARE computationally bounded) can *verify* a solution in MIP*. So you can check the solution is correct without actually solving the halting problem.\n\nThe comment above me is saying that a verifier essentially needs to solve the halting problem in order to know if the solution is YES/NO, but a quantum verifier in fact does not. It only needs to be given a proof.", "Score": 1, "Replies": [{"Reply": "The verifier in MIP\\* is classical. The provers are all powerful, but have entangled qubits to communicate / \"make their stories consistent\" when they are trying to lie to the (classical) verifier.\n\nThere is no \"proof\" as a well-defined object (e.g. a string, or quantum state). Instead, the entire \\*interaction\\* is a proof. To be used as a cryptographic protocol, *someone* would have to play the role of the (multiple interactive) provers. How might we instantiate those parties?\n\nThe answer is that we couldn't. Appealing to quantum entanglement wouldn't magically let us solve the halting problem, and what MIP\\* = RE should be interpreted as saying is that the computational model of MIP\\* is unrealistic, and not that we can now magically solve the halting problem because quantum.", "Reply Score": 2}]}, {"Comment": "That's true, but that still makes it an interesting idea: If every attempt to crack the message takes a second, it's infeasible to brute force. Though that isn't restricted to undecidable problems and taking a variable time to decode depending on the key leaves timing-based side channels wide open.", "Score": -1, "Replies": [{"Reply": "I don't think this is a new idea, is it? We would usually call this a \"work factor\" in crypto and it's a parameter in most modern password hashing schemes (e.g. PBKDF-2)", "Reply Score": 5}]}, {"Comment": "The verifier in MIP\\* is classical. The provers are all powerful, but have entangled qubits to communicate / \"make their stories consistent\" when they are trying to lie to the (classical) verifier.\n\nThere is no \"proof\" as a well-defined object (e.g. a string, or quantum state). Instead, the entire \\*interaction\\* is a proof. To be used as a cryptographic protocol, *someone* would have to play the role of the (multiple interactive) provers. How might we instantiate those parties?\n\nThe answer is that we couldn't. Appealing to quantum entanglement wouldn't magically let us solve the halting problem, and what MIP\\* = RE should be interpreted as saying is that the computational model of MIP\\* is unrealistic, and not that we can now magically solve the halting problem because quantum.", "Score": 2, "Replies": [{"Reply": "I\u2019m not sure you understood my comment. I\u2019ve NEVER suggested that we can solve the halting problem at all, nor am I suggesting that we can in general act as provers for the halting problem. All I\u2019m saying that there exists the asymmetry of solving being computationally much harder than verifying. As you said in your comment, we CAN verify an instance of the halting problem using an MIP* protocol, we CANNOT solve the problem ourselves. However, if two provers claim to know the solution, we can verify that the solution is true. Obviously this is NOT SOLVING, but nevertheless we obtain the solution if the provers happen to be correct.\n\nI\u2019ve also never claimed that this is a cryptographic protocol, I\u2019m only responding to a single comment. If we try to come up with one, but obviously it wouldn\u2019t work like a typical protocol since you need two parties to generate one key.\n\nStill, \n\n> How might we instantiate these provers?\n\nYou wouldn\u2019t. Even for protocols using problems in NP, you don\u2019t need powerful provers because you don\u2019t expect to solve the actual NP problem, you just need an instance with a known solution. The prover *can* in practice be computationally bounded even if the problem instance is hard. ie you never factor a number in the RSA protocol, you just choose the primes. The \u201cprover\u201d doesn\u2019t ever solve FACTOR(n), but you hope that an attack breaking RSA must. You don\u2019t need an NP oracle to do cryptography with a problem in NP.\n\nSimilarly, the prover(s) for a RE cryptography protocol shouldn\u2019t need to be all powerful, you just need an instance that is known to either halt or not halt, and then the provers can convince the verifiers that this answer is correct. The key generation protocol and decryption should *not* use the solution to HALT as an oracle.", "Reply Score": 0}]}, {"Comment": "I don't think this is a new idea, is it? We would usually call this a \"work factor\" in crypto and it's a parameter in most modern password hashing schemes (e.g. PBKDF-2)", "Score": 5, "Replies": []}, {"Comment": "I\u2019m not sure you understood my comment. I\u2019ve NEVER suggested that we can solve the halting problem at all, nor am I suggesting that we can in general act as provers for the halting problem. All I\u2019m saying that there exists the asymmetry of solving being computationally much harder than verifying. As you said in your comment, we CAN verify an instance of the halting problem using an MIP* protocol, we CANNOT solve the problem ourselves. However, if two provers claim to know the solution, we can verify that the solution is true. Obviously this is NOT SOLVING, but nevertheless we obtain the solution if the provers happen to be correct.\n\nI\u2019ve also never claimed that this is a cryptographic protocol, I\u2019m only responding to a single comment. If we try to come up with one, but obviously it wouldn\u2019t work like a typical protocol since you need two parties to generate one key.\n\nStill, \n\n> How might we instantiate these provers?\n\nYou wouldn\u2019t. Even for protocols using problems in NP, you don\u2019t need powerful provers because you don\u2019t expect to solve the actual NP problem, you just need an instance with a known solution. The prover *can* in practice be computationally bounded even if the problem instance is hard. ie you never factor a number in the RSA protocol, you just choose the primes. The \u201cprover\u201d doesn\u2019t ever solve FACTOR(n), but you hope that an attack breaking RSA must. You don\u2019t need an NP oracle to do cryptography with a problem in NP.\n\nSimilarly, the prover(s) for a RE cryptography protocol shouldn\u2019t need to be all powerful, you just need an instance that is known to either halt or not halt, and then the provers can convince the verifiers that this answer is correct. The key generation protocol and decryption should *not* use the solution to HALT as an oracle.", "Score": 0, "Replies": [{"Reply": "\\> You wouldn\u2019t. Even for protocols using problems in NP, you don\u2019t need powerful provers because you don\u2019t expect to solve the actual NP problem, you just need an instance with a known solution.\n\nmy entire point is that this reasoning does not scale past NP. What is a \"known solution\" for a PSPACE-complete problem? It isn't a (polynomially-size) witness unless you believe NP = PSPACE.\n\nGiven there is no analogue of a witness, the provers *must* be super-polynomial time (unless you think whatever class under consideration, e.g. MIP\\*, is equal to P). This is because if there is an efficient prover, the verifier (on input their instance) can just simulate the prover. By soundness of the IP if this leads to an accepting transcript they can decide that their instance is in the language.\n\n\\> you just need an instance that is known to either halt or not halt, and then the provers can convince the verifiers that this answer is correct.\n\nwhat does it mean for an instance to be \"known to halt\" or \"known to not halt\"? For NP you can formalize this with witnesses. But provided NP != PSPACE, you *cannot* do this for problems in PSPACE (= IP). Similarly, if NP != NEXP (which is true by time hierarchy) then you *cannot* do this for problems in NEXP = MIP.\n\nThat all being said, you've been wrong about numerous things, and combative, so I'm not particularly interested in discussing things further.", "Reply Score": 2}]}, {"Comment": "\\> You wouldn\u2019t. Even for protocols using problems in NP, you don\u2019t need powerful provers because you don\u2019t expect to solve the actual NP problem, you just need an instance with a known solution.\n\nmy entire point is that this reasoning does not scale past NP. What is a \"known solution\" for a PSPACE-complete problem? It isn't a (polynomially-size) witness unless you believe NP = PSPACE.\n\nGiven there is no analogue of a witness, the provers *must* be super-polynomial time (unless you think whatever class under consideration, e.g. MIP\\*, is equal to P). This is because if there is an efficient prover, the verifier (on input their instance) can just simulate the prover. By soundness of the IP if this leads to an accepting transcript they can decide that their instance is in the language.\n\n\\> you just need an instance that is known to either halt or not halt, and then the provers can convince the verifiers that this answer is correct.\n\nwhat does it mean for an instance to be \"known to halt\" or \"known to not halt\"? For NP you can formalize this with witnesses. But provided NP != PSPACE, you *cannot* do this for problems in PSPACE (= IP). Similarly, if NP != NEXP (which is true by time hierarchy) then you *cannot* do this for problems in NEXP = MIP.\n\nThat all being said, you've been wrong about numerous things, and combative, so I'm not particularly interested in discussing things further.", "Score": 2, "Replies": []}]},{"Title": "Prime Distribution Conjecture", "Score": 4, "URL": "https://www.reddit.com/r/compsci/comments/18id6ap/prime_distribution_conjecture/", "CreatedAt": 1702573327.0, "Full Content": "Hi all, I would like to test - and learn from your responses to a conjecture on prime distribution I came up with:\n\n**Sequence Definition**\n\n1. **Starting Point**: *a*\\_0 \u200b= *n*, for any integer *n* \\> 3.\n2. **For each subsequent element** *a*\\_*i*\\+1**\u200b:**\n\n* **If** ***a***\\_***i***\u200b **is a prime number**: *a*\\_*i+1\u200b = a*\\_*i* \u200b\u2212 *p*, where *p* is the maximum prime number less than *a*\\_*i*\u200b.\n* **If** ***a***\\_***i***\u200b **is not a prime number**: *a*\\_*i+1 \u200b= a*\\_*i* \u200b+ *n*(*A*), where *A* is the set of prime numbers less than *a*\\_*i*\u200b, and *n*(*A*) is the number of elements in *A*.\n\n**Conjecture**\n\nFor every initial *a*\\_0\u200b, there exists some *N* in N(atural Numbers) such that a\\_*N* \u200b= 2.\n\nLooking forward to your comments!\n\nThanks!", "CntComments": 4, "Comments": [{"Comment": "Wat?\n\n>*ai+1\u200b = ai*\u00a0\u200b\u2212\u00a0*p*, where\u00a0*p*\u00a0is the maximum prime number less than\u00a0*ai*\u200b.\n\nai +1  (whether that means a\\*i or a\u1d62)= ai - a positive number?  How is ai + 1 = ai - something positive ?", "Score": 1, "Replies": [{"Reply": "Excuse me, I've edited the post -- the \\_ denotes subscript.", "Reply Score": 1}]}, {"Comment": "Excuse me, I've edited the post -- the \\_ denotes subscript.", "Score": 1, "Replies": []}]},{"Title": "Christos Papadimitriou is a famous theoretical computer scientist. In 2014, he said about modern CS education that \u201cthe courses have become so complicated that I doubt I could pass them!\u201d", "Score": 132, "URL": "https://www.pnas.org/doi/10.1073/pnas.1405579111", "CreatedAt": 1702484167.0, "Full Content": "", "CntComments": 38, "Comments": [{"Comment": "How is every top level comment negative, wtf", "Score": 15, "Replies": []}, {"Comment": "It's just the accumulation of the art and science.  As the science gets more advanced there's so much more that's \"default knowledge\" just to get up to speed.   There's hundreds of PhDs adding to the \"basic knowledge\" every year when they rewrite textbooks.  It's stuff that took the current senior engineers decades to learn as it was developed and a student has to catch up in four years.\n\nThat's why Degrees fracture into subcategories.  There's too much to learn it all in four or even 8 years, so students focus on one narrow part of a subject to specialize in so get can accumulate enough practice and experience to advance the field.", "Score": 14, "Replies": []}, {"Comment": "Meanwhile, my hardest CS courses were based on his work on combinatorial optimization. He's talking out of his ass this one time.", "Score": 12, "Replies": [{"Reply": "Still have his combinatorial optimization book.", "Reply Score": 3}]}, {"Comment": "I really like this profile, and also the accompanying paper that Christos put out alongside his election to the NAS: [Algorithms, complexity, and the sciences](https://www.pnas.org/doi/abs/10.1073/pnas.1416954111). It meant a lot to me both because Christos is advocating for an [algorithmic view of nature](https://egtheory.wordpress.com/2019/03/23/algorithmic-x/) (which would greatly expand the scope of theoretical computer sciecne and take us away from worrying about primarily tech problems) and because it was perhaps first time that my own work on algorithmic biology was endorsed/mentioned!", "Score": 3, "Replies": []}, {"Comment": "I think it's due to imperative programming practices dominating CS curricula. Imperative coding is fundamentally bit twiddling. We're so far into the weeds of managing and manipulating state that we don't really advance our theories of computation when data and controll are all bit manipulation.  \n\nIf we want to advance our knowleged of CS, it needs to be firmly grounded in mathematics and FP to express said mathematics. The simple notion of recursion is a total mess in the imperative world but it's simply expressed in FP. If by stroke of accident that CS was based on FP, we would be much better off for it. And we would also understand a lot more about imperative programming, its trade offs, how to manipulate state safely, etc.", "Score": -11, "Replies": [{"Reply": "I *strongly* doubt that\u2019s what he was saying here.\n\nImperative programming is barely a focus in a CS curriculum. We typically teach a couple of introductory courses in programming, usually in OO/imperative-style languages, and then move on. There\u2019s often a survey course of other programming models, but programming is simply not what 90% of a CS curriculum is about. We use it, but it\u2019s not the material a course is covering.\n\nAlso, while I tend to agree that explicit state management as required by imperative programming is really hard, it\u2019s only hard at scale. We don\u2019t ask students to build million SLoC software and maintain it for a decade. We ask them to write small standalone systems from scratch to demonstrate mastery of specific topics. The field might be better off if more students did that in functional languages, but my experience as a teacher tells me **that** is what would make the courses harder. Almost every student finds FP harder than imperative code when getting toy stuff to work.", "Reply Score": 29}, {"Reply": "I think the biggest problem for the future of CS is not imperative vs functional, but rather dealing with parallelism. \n\nOur computers are now *ludicrously* fast, but only in parallel; clock speeds have capped out. Today's high-end CPUs have hundreds of cores, GPUs have tens of thousands, and this trend looks like it's going to continue. \n\nBut writing efficient parallel programs is hard. Most programs are still single-threaded. We have fast computers and no clue what to do with them, like a new version of Dijkstra's [software crisis](https://en.wikipedia.org/wiki/Software_crisis) from the 70s:\n\n>The major cause of the software crisis is that the machines have become several orders of magnitude more powerful! To put it quite bluntly: as long as there were no machines, programming was no problem at all; when we had a few weak computers, programming became a mild problem, and now we have gigantic computers, programming has become an equally gigantic problem.\n\nI think the solution here is machine learning, which creates computer programs through optimization rather than construction. Neural networks are trivially parallelizable, and gradient descent allows you to efficiently search the space of possible programs to create programs that you have no clue how to construct manually. \n\nThe programs you create this way are very different from traditional programs - they're very large, data-driven, and somewhat messy - and CS doesn't have good tools for understanding them yet. I believe this is what the next era of CS will be focused on.", "Reply Score": 7}, {"Reply": "I\u2019ll preface everything by saying I actually used to believe this. \n\nBut in my current opinion computer science is fundamentally about sequenced atomic operations, about state machines. You don\u2019t have an actual model for computation with just a lambda calculus variation, you need an evaluation strategy too, and the best mathematical model we have for formalizing what that is is a Turing machine back again. Classical computation is something that takes place as a sequence of steps in discrete time, such that complexity is something that can be reasoned about, such that temporal invariants are something that can be reasoned about, such that nontermination can be expressed, and so forth. Functional programming is nice, and provided you\u2019re either working with a strict language or are an expert at the inner workings of your compiler and are working with a lazy language it can be a useful and productive abstraction over producing the final normalized finite sequence of instructions you\u2019re interested in producing directly, but it\u2019s just that, an abstraction.", "Reply Score": 0}]}, {"Comment": "A course with high complexity can help weed out the students who are just there for the degree from those who are actually interested in the subject.", "Score": -10, "Replies": [{"Reply": "That's a perverse incentive. Most jobs who say they require the degree don't actually need what's taught in it. Even people who want to do PhDs because they're super interested in a CS subject are never interested in all of it.", "Reply Score": 9}]}, {"Comment": "Well, he\u2019s a theorist, so everything he does can be done without a computer, whereas most undergrad CS classes are preparing students for a career in software engineering with tangible, vocational skills.\n\nEdit: Cormen never said such things.", "Score": -44, "Replies": [{"Reply": "It seems so obvious that what we need is a pre-computing undergrad that is common to both CS and Software Engineering (\u201cSEng.), then students should either go directly into vocational jobs, seek higher engineering education/credentials, or enter post-grad CS, which will be about the science, not the industry.\n\nBut that ship has sailed, the industry likes to hire \u201cComputer Science\u201d graduates, then ask them to write an address book in React and demonstrate their ability to estimate how many [round manhole covers][1] fit in a Dreaamliner.\n\n[1]: http://sellsbrothers.com/12395", "Reply Score": 25}, {"Reply": "> whereas most undergrad CS classes are preparing students for a career in software engineering with tangible, vocational skills.\n\nAlbeit poorly lol", "Reply Score": 2}]}, {"Comment": "Back in the 1990s there was a pervasive rumor on campus that mathematics was the most difficult major.\n\nToday,  CS is the most difficult major on campus -- by far.  THe courseload is outrageous.   Programming under a deadline is utter torture.  People who don't know this is going on in their university are essentially clueless.", "Score": -12, "Replies": [{"Reply": "Nah, I take maths and comp sci subjects and it's not even close. Maths is way more intense", "Reply Score": 10}, {"Reply": "I definitely struggled more with maths than CS, and I quite like maths. I signed up for a pure maths class and noped out a quarter of the way through.", "Reply Score": 1}]}, {"Comment": "Still have his combinatorial optimization book.", "Score": 3, "Replies": []}, {"Comment": "I *strongly* doubt that\u2019s what he was saying here.\n\nImperative programming is barely a focus in a CS curriculum. We typically teach a couple of introductory courses in programming, usually in OO/imperative-style languages, and then move on. There\u2019s often a survey course of other programming models, but programming is simply not what 90% of a CS curriculum is about. We use it, but it\u2019s not the material a course is covering.\n\nAlso, while I tend to agree that explicit state management as required by imperative programming is really hard, it\u2019s only hard at scale. We don\u2019t ask students to build million SLoC software and maintain it for a decade. We ask them to write small standalone systems from scratch to demonstrate mastery of specific topics. The field might be better off if more students did that in functional languages, but my experience as a teacher tells me **that** is what would make the courses harder. Almost every student finds FP harder than imperative code when getting toy stuff to work.", "Score": 29, "Replies": [{"Reply": "> Almost every student finds FP harder than imperative code\n\nOnly when introduced to an imperative style first. At schools that start with functional languages (Racket, Haskell, OCaml, and SML are common choices), fresh students have no such difficulty, and they also seem better able to move to imperative styles from functional. Functional programming follows more naturally from the mathematical background most students will have by the end of high school, since state doesn't exist there.\n\nSome people are researching this exact scenario. Look at Shriram Krishnamurthi's work at Brown, for instance.", "Reply Score": 2}]}, {"Comment": "I think the biggest problem for the future of CS is not imperative vs functional, but rather dealing with parallelism. \n\nOur computers are now *ludicrously* fast, but only in parallel; clock speeds have capped out. Today's high-end CPUs have hundreds of cores, GPUs have tens of thousands, and this trend looks like it's going to continue. \n\nBut writing efficient parallel programs is hard. Most programs are still single-threaded. We have fast computers and no clue what to do with them, like a new version of Dijkstra's [software crisis](https://en.wikipedia.org/wiki/Software_crisis) from the 70s:\n\n>The major cause of the software crisis is that the machines have become several orders of magnitude more powerful! To put it quite bluntly: as long as there were no machines, programming was no problem at all; when we had a few weak computers, programming became a mild problem, and now we have gigantic computers, programming has become an equally gigantic problem.\n\nI think the solution here is machine learning, which creates computer programs through optimization rather than construction. Neural networks are trivially parallelizable, and gradient descent allows you to efficiently search the space of possible programs to create programs that you have no clue how to construct manually. \n\nThe programs you create this way are very different from traditional programs - they're very large, data-driven, and somewhat messy - and CS doesn't have good tools for understanding them yet. I believe this is what the next era of CS will be focused on.", "Score": 7, "Replies": [{"Reply": "Parallelism and concurrency are much better understood when modeled within a type system. It's much better understood because they tend to be algebraic in nature. We know the power of applied mathematics why not do the same to CS.", "Reply Score": -2}]}, {"Comment": "I\u2019ll preface everything by saying I actually used to believe this. \n\nBut in my current opinion computer science is fundamentally about sequenced atomic operations, about state machines. You don\u2019t have an actual model for computation with just a lambda calculus variation, you need an evaluation strategy too, and the best mathematical model we have for formalizing what that is is a Turing machine back again. Classical computation is something that takes place as a sequence of steps in discrete time, such that complexity is something that can be reasoned about, such that temporal invariants are something that can be reasoned about, such that nontermination can be expressed, and so forth. Functional programming is nice, and provided you\u2019re either working with a strict language or are an expert at the inner workings of your compiler and are working with a lazy language it can be a useful and productive abstraction over producing the final normalized finite sequence of instructions you\u2019re interested in producing directly, but it\u2019s just that, an abstraction.", "Score": 0, "Replies": []}, {"Comment": "That's a perverse incentive. Most jobs who say they require the degree don't actually need what's taught in it. Even people who want to do PhDs because they're super interested in a CS subject are never interested in all of it.", "Score": 9, "Replies": [{"Reply": "To your latter point: even if you're going to do a PhD in CS, you should have a decent background in the areas outside your specialization. Few things worse than an expert who doesn't know the rest of their field at all.", "Reply Score": 1}, {"Reply": "You could narrow down the syllabus spectrum, but from my experience, highly complicated workloads will eventually help you adapt to a higher abstraction layer,they'll all be useful someday, even the ones you thought were useless.", "Reply Score": 0}]}, {"Comment": "It seems so obvious that what we need is a pre-computing undergrad that is common to both CS and Software Engineering (\u201cSEng.), then students should either go directly into vocational jobs, seek higher engineering education/credentials, or enter post-grad CS, which will be about the science, not the industry.\n\nBut that ship has sailed, the industry likes to hire \u201cComputer Science\u201d graduates, then ask them to write an address book in React and demonstrate their ability to estimate how many [round manhole covers][1] fit in a Dreaamliner.\n\n[1]: http://sellsbrothers.com/12395", "Score": 25, "Replies": [{"Reply": "There\u2019s a middle ground, which is currently what ABET enforces (in the US). \n\nSo few undergrads go on to graduate school, and magnitudes less complete their Doctorate, that many schools across the country don\u2019t even have a requirement for PhDs to teach undergrad classes anymore. \n\nThis is further compounded by the fact that a \u201cgood\u201d undergrad can make $40k+ over a PhD degreed teaching faculty, ensuring a dwindling supply of \u201cgood\u201d professors.", "Reply Score": 6}]}, {"Comment": "> whereas most undergrad CS classes are preparing students for a career in software engineering with tangible, vocational skills.\n\nAlbeit poorly lol", "Score": 2, "Replies": [{"Reply": "Explain?", "Reply Score": 1}]}, {"Comment": "Nah, I take maths and comp sci subjects and it's not even close. Maths is way more intense", "Score": 10, "Replies": []}, {"Comment": "I definitely struggled more with maths than CS, and I quite like maths. I signed up for a pure maths class and noped out a quarter of the way through.", "Score": 1, "Replies": []}, {"Comment": "> Almost every student finds FP harder than imperative code\n\nOnly when introduced to an imperative style first. At schools that start with functional languages (Racket, Haskell, OCaml, and SML are common choices), fresh students have no such difficulty, and they also seem better able to move to imperative styles from functional. Functional programming follows more naturally from the mathematical background most students will have by the end of high school, since state doesn't exist there.\n\nSome people are researching this exact scenario. Look at Shriram Krishnamurthi's work at Brown, for instance.", "Score": 2, "Replies": [{"Reply": "I think we're drifting afield a bit here. The original comment was that what Dr. Papadimitrious meant by \"the classes are getting so complex that I'm not sure I could pass them\" was that he would be somehow unable to deal with the complexity of imperative code needed in those classes. All I'm really saying here is, \"no, that's certainly not what he meant\".\n\nIt's pretty clear it was a tongue-in-cheek comment anyway, but my point was just that the fact that we teach C++ or Java instead of Scheme is not why anyone finds an upper-division CS course to be difficult, and it doesn't actually make the classes more difficult in general. \n\nI'm sure you're right that if they learn FP first, some things get easier. I think some things will get harder as well though. Notably things like data structures can be much more challenging. But overall, a machine learning course is challenging because statistics is hard, not because it's harder to write a for loop than recursion.\n\n> Functional programming follows more naturally from the mathematical background most students will have by the end of high school, since state doesn't exist there.\n\nI'd also say that there are loads of students whose mathematical sophistication coming out of high school is sketchy enough that appealing to the connection to mathematics is actually harder than just teaching them state manipulation. Teaching them state manipulation might be a long-term disservice -- not taking a position either way on that one.\n\nI'd be in favor of teaching more functional programming, even as first languages. I'm willing to believe it would improve things. I'm just disagreeing with the idea that not doing so is why CS is hard for students.", "Reply Score": 2}]}, {"Comment": "Parallelism and concurrency are much better understood when modeled within a type system. It's much better understood because they tend to be algebraic in nature. We know the power of applied mathematics why not do the same to CS.", "Score": -2, "Replies": [{"Reply": "Because we've been trying the mathematics approach for the last half-century, and there are still problems (open-world robotics, language understanding, image recognition) that are too complex for it to solve. \n\nThese programs created through optimization don't use structures like types or functions, and they're more like what evolution produces than what a mathematician would. But they do much better at these complex, open-ended problems than anything we've built by hand.", "Reply Score": 2}]}, {"Comment": "To your latter point: even if you're going to do a PhD in CS, you should have a decent background in the areas outside your specialization. Few things worse than an expert who doesn't know the rest of their field at all.", "Score": 1, "Replies": [{"Reply": "For a decent background, definitely, not as a pleb filter", "Reply Score": 2}]}, {"Comment": "You could narrow down the syllabus spectrum, but from my experience, highly complicated workloads will eventually help you adapt to a higher abstraction layer,they'll all be useful someday, even the ones you thought were useless.", "Score": 0, "Replies": [{"Reply": "There's a limit to this. You might end up weeding out a Papadimitrou just cause he can't wrap his head around network layers.", "Reply Score": 6}]}, {"Comment": "There\u2019s a middle ground, which is currently what ABET enforces (in the US). \n\nSo few undergrads go on to graduate school, and magnitudes less complete their Doctorate, that many schools across the country don\u2019t even have a requirement for PhDs to teach undergrad classes anymore. \n\nThis is further compounded by the fact that a \u201cgood\u201d undergrad can make $40k+ over a PhD degreed teaching faculty, ensuring a dwindling supply of \u201cgood\u201d professors.", "Score": 6, "Replies": [{"Reply": "There isn\u2019t a dwindling supply of good professors. We produce hundreds of PhDs for every faculty job, and that ratio keeps increasing. Yes, CS is an area where industry is best able to siphon off talent, but if 450 people are applying for every job, some of them are still going to be great.", "Reply Score": -2}]}, {"Comment": "Explain?", "Score": 1, "Replies": [{"Reply": "Many new CS grads take a few years to become good programmers.\n\nBut this may be less about college and more about being 22 and inexperienced.", "Reply Score": 0}]}, {"Comment": "I think we're drifting afield a bit here. The original comment was that what Dr. Papadimitrious meant by \"the classes are getting so complex that I'm not sure I could pass them\" was that he would be somehow unable to deal with the complexity of imperative code needed in those classes. All I'm really saying here is, \"no, that's certainly not what he meant\".\n\nIt's pretty clear it was a tongue-in-cheek comment anyway, but my point was just that the fact that we teach C++ or Java instead of Scheme is not why anyone finds an upper-division CS course to be difficult, and it doesn't actually make the classes more difficult in general. \n\nI'm sure you're right that if they learn FP first, some things get easier. I think some things will get harder as well though. Notably things like data structures can be much more challenging. But overall, a machine learning course is challenging because statistics is hard, not because it's harder to write a for loop than recursion.\n\n> Functional programming follows more naturally from the mathematical background most students will have by the end of high school, since state doesn't exist there.\n\nI'd also say that there are loads of students whose mathematical sophistication coming out of high school is sketchy enough that appealing to the connection to mathematics is actually harder than just teaching them state manipulation. Teaching them state manipulation might be a long-term disservice -- not taking a position either way on that one.\n\nI'd be in favor of teaching more functional programming, even as first languages. I'm willing to believe it would improve things. I'm just disagreeing with the idea that not doing so is why CS is hard for students.", "Score": 2, "Replies": []}, {"Comment": "Because we've been trying the mathematics approach for the last half-century, and there are still problems (open-world robotics, language understanding, image recognition) that are too complex for it to solve. \n\nThese programs created through optimization don't use structures like types or functions, and they're more like what evolution produces than what a mathematician would. But they do much better at these complex, open-ended problems than anything we've built by hand.", "Score": 2, "Replies": [{"Reply": "We're no longer talking about parallelism. That's fine. \n\nWithout math techniques there would no way to train ML systems.", "Reply Score": 1}]}, {"Comment": "For a decent background, definitely, not as a pleb filter", "Score": 2, "Replies": []}, {"Comment": "There's a limit to this. You might end up weeding out a Papadimitrou just cause he can't wrap his head around network layers.", "Score": 6, "Replies": [{"Reply": "Well you do have a valid point, but i'd like to think that there's no upper limit", "Reply Score": 0}]}, {"Comment": "There isn\u2019t a dwindling supply of good professors. We produce hundreds of PhDs for every faculty job, and that ratio keeps increasing. Yes, CS is an area where industry is best able to siphon off talent, but if 450 people are applying for every job, some of them are still going to be great.", "Score": -2, "Replies": [{"Reply": "For every good professor I interview, I have ~200-300 qualified applicants.  Domestic PhD applications are down nationwide. \n\nMost pass after I offer them $80k for 4/4. \nThe ones that survive all have side gigs (myself included). \n\nBLS estimates that jobs requiring CS PhDs is increasing at about 15%, but graduation rates have remained stable at ~2k/year.", "Reply Score": 6}]}, {"Comment": "Many new CS grads take a few years to become good programmers.\n\nBut this may be less about college and more about being 22 and inexperienced.", "Score": 0, "Replies": []}, {"Comment": "We're no longer talking about parallelism. That's fine. \n\nWithout math techniques there would no way to train ML systems.", "Score": 1, "Replies": [{"Reply": "Well, parallelism makes this practical. Imagine training a single program with a million serial steps - most likely, your random initial weights would produce garbage and your trainer would have nothing to work with. \n\nNow imagine 100 steps (layers) but each is doing 10,000 computations at once. You can still get useful information out the other end even if some parts of the program fail. And your program is implicitly trying many things at once, producing a richer training signal. \n\n>Without mathematics techniques there would no way to train ML systems.\n\nThis is true; the optimizer we're using is a traditional program built using traditional techniques. \n\nBut the programs it finds are very different. Traditional software development is about stacking abstractions on top of abstractions on top of abstractions; optimization abandons all that and just flips switches until it finds settings that work.", "Reply Score": 1}]}, {"Comment": "Well you do have a valid point, but i'd like to think that there's no upper limit", "Score": 0, "Replies": [{"Reply": "The point of uni is educating, not weeding out. If there's no upper limit on scope and complexity, you'll weed out people who don't have the resources to commit everything to get the degree. No time for jobs to pay for your classes. No time for special interests in CS topics that aren't touched until PhD.", "Reply Score": 5}]}, {"Comment": "For every good professor I interview, I have ~200-300 qualified applicants.  Domestic PhD applications are down nationwide. \n\nMost pass after I offer them $80k for 4/4. \nThe ones that survive all have side gigs (myself included). \n\nBLS estimates that jobs requiring CS PhDs is increasing at about 15%, but graduation rates have remained stable at ~2k/year.", "Score": 6, "Replies": [{"Reply": "> Most pass after I offer them $80k for 4/4. The ones that survive all have side gigs (myself included).\n\nI\u2019m sort of confused who this set of people are. I believe you, but who are these people who are leaving graduate school and making it through the terrible application process only to pass on the offer that any one of us knew was coming?\n\nIf you aren\u2019t taking $80k a year, why are you even in the candidate pool? It\u2019s not like there aren\u2019t lucrative options available. I don\u2019t blame anyone for taking one \u2014 it\u2019s what I did too. But I didn\u2019t need to send out 300 applications and interview for 8 months to eventually be like, \"wait, so you\u2019re not going to pay me $300k?\"", "Reply Score": 1}]}, {"Comment": "Well, parallelism makes this practical. Imagine training a single program with a million serial steps - most likely, your random initial weights would produce garbage and your trainer would have nothing to work with. \n\nNow imagine 100 steps (layers) but each is doing 10,000 computations at once. You can still get useful information out the other end even if some parts of the program fail. And your program is implicitly trying many things at once, producing a richer training signal. \n\n>Without mathematics techniques there would no way to train ML systems.\n\nThis is true; the optimizer we're using is a traditional program built using traditional techniques. \n\nBut the programs it finds are very different. Traditional software development is about stacking abstractions on top of abstractions on top of abstractions; optimization abandons all that and just flips switches until it finds settings that work.", "Score": 1, "Replies": []}, {"Comment": "The point of uni is educating, not weeding out. If there's no upper limit on scope and complexity, you'll weed out people who don't have the resources to commit everything to get the degree. No time for jobs to pay for your classes. No time for special interests in CS topics that aren't touched until PhD.", "Score": 5, "Replies": []}, {"Comment": "> Most pass after I offer them $80k for 4/4. The ones that survive all have side gigs (myself included).\n\nI\u2019m sort of confused who this set of people are. I believe you, but who are these people who are leaving graduate school and making it through the terrible application process only to pass on the offer that any one of us knew was coming?\n\nIf you aren\u2019t taking $80k a year, why are you even in the candidate pool? It\u2019s not like there aren\u2019t lucrative options available. I don\u2019t blame anyone for taking one \u2014 it\u2019s what I did too. But I didn\u2019t need to send out 300 applications and interview for 8 months to eventually be like, \"wait, so you\u2019re not going to pay me $300k?\"", "Score": 1, "Replies": [{"Reply": "The people that accept are in (mostly) three categories:\n1. They love the area (north Texas) or have family here. \n2. They love teaching / want to escape the tenure treadmill. \n3. They desperately need a visa to stay in the country (unfortunate). \n\nWe are seeing less applications over the past two years, but it could be anything\u2026 not a lot want to teach, Texas has been driving away educators, etc. \n\nThat being said, as chair of the hiring committee I\u2019ve only hired 12 full time faculty in a CS department of ~70, so who accepts might just be anecdotal.\n\nEdit: Oh and I\u2019m not allowed to give the salary range until they get an offer letter.  It\u2019s my preference to just straight up post what we offer for Asst./Assc., but dept. chair explicitly forbids it.", "Reply Score": 2}]}, {"Comment": "The people that accept are in (mostly) three categories:\n1. They love the area (north Texas) or have family here. \n2. They love teaching / want to escape the tenure treadmill. \n3. They desperately need a visa to stay in the country (unfortunate). \n\nWe are seeing less applications over the past two years, but it could be anything\u2026 not a lot want to teach, Texas has been driving away educators, etc. \n\nThat being said, as chair of the hiring committee I\u2019ve only hired 12 full time faculty in a CS department of ~70, so who accepts might just be anecdotal.\n\nEdit: Oh and I\u2019m not allowed to give the salary range until they get an offer letter.  It\u2019s my preference to just straight up post what we offer for Asst./Assc., but dept. chair explicitly forbids it.", "Score": 2, "Replies": [{"Reply": "Sorry, I think I wasn\u2019t clear in my rant there. My rhetorical question wasn\u2019t about the people who accept. I\u2019m saying \"how do the people who turn you down not already know that the offer will be $80k?\"\n\nWhen I was still in academia, I definitely knew what I\u2019d be offered (more or less) before I even sent an application. We know academic salaries are low outside of exceptional cases. Many are public as the state universities have to disclose them. I could look at big groups of people hired recently, compare their CVs to mine, and then by name see what they made. It seems crazy to me to invest the amount of time and energy needed to get an offer and then reject it for what they could have just found on the internet eight months ago.", "Reply Score": 1}]}, {"Comment": "Sorry, I think I wasn\u2019t clear in my rant there. My rhetorical question wasn\u2019t about the people who accept. I\u2019m saying \"how do the people who turn you down not already know that the offer will be $80k?\"\n\nWhen I was still in academia, I definitely knew what I\u2019d be offered (more or less) before I even sent an application. We know academic salaries are low outside of exceptional cases. Many are public as the state universities have to disclose them. I could look at big groups of people hired recently, compare their CVs to mine, and then by name see what they made. It seems crazy to me to invest the amount of time and energy needed to get an offer and then reject it for what they could have just found on the internet eight months ago.", "Score": 1, "Replies": [{"Reply": "Many just don't do due diligence even though as a public employee, our salaries are \"open\".  \n\n\nA bit of it is probably lag in the market (housing) too... 8 years ago you could buy a 2000sq ft 3 bedroom/2 bath in our city for $150k.  \nNow there isn't a house under $300k, but the stigma (?) of north Texas being \"cheap to live\" is still there.", "Reply Score": 2}]}, {"Comment": "Many just don't do due diligence even though as a public employee, our salaries are \"open\".  \n\n\nA bit of it is probably lag in the market (housing) too... 8 years ago you could buy a 2000sq ft 3 bedroom/2 bath in our city for $150k.  \nNow there isn't a house under $300k, but the stigma (?) of north Texas being \"cheap to live\" is still there.", "Score": 2, "Replies": []}]},{"Title": "What are some relatively unknown CS books which are gems?", "Score": 49, "URL": "https://www.reddit.com/r/compsci/comments/18gaetx/what_are_some_relatively_unknown_cs_books_which/", "CreatedAt": 1702345482.0, "Full Content": "", "CntComments": 45, "Comments": [{"Comment": "[*Probabilistic Machine Learning Books 1 and 2*](https://probml.github.io/pml-book/) by Kevin Murphy (both of the new ones) are, combined, perhaps the most ambitious yet successful attempt I've seen to put all of machine learning and AI in a single mathematical framework. Many pearls of wisdom in the sense of intuitive, conceptual ideas that don't require tons of algebra. (Some examples: how does negative entropy make sense if entropy means bits? Why is Kullback-Liebler divergence used instead of other ways of comparing probability distributions? Why do VAEs often make blurry images?)", "Score": 16, "Replies": [{"Reply": "Thanks!", "Reply Score": 0}, {"Reply": "Just wanted to say there many classic statistics texts which put machine learning in a single mathematical framework", "Reply Score": 1}]}, {"Comment": "My favorite that I think actually qualifies as \"unknown\" is \"The Art of the Metaobject Protocol\".\n\nIt builds a full and complex object system (as in object oriented programming) from the ground up in Common Lisp. Maybe a niche audience for it, but if you know a bit of Lisp, it's a great little book on what (one flavor of) OOP really is \"under the hood\".", "Score": 11, "Replies": [{"Reply": "\u201cLet over Lambda\u201d (Doug Hoyle) also a great read if you are into Lisp or just like reading about it.", "Reply Score": 3}]}, {"Comment": "I believe it is known but this 100 page paper changed a lot including current fashion GPT. \nhttps://en.wikipedia.org/wiki/Syntactic_Structures?wprov=sfti1", "Score": 9, "Replies": []}, {"Comment": "I really love [\"PROGRAM = PROOF\"](https://www.lix.polytechnique.fr/Labo/Samuel.Mimram/teaching/INF551/course.pdf) by Samuel Mimram. It's an introduction to theorem proving with dependent types, using the programming language Agda. Some of the later chapters (focused on Homotopy Type Theory) are a little difficult to follow, but most of the book is very approachable to anyone with some knowledge of typed functional programming.", "Score": 8, "Replies": []}, {"Comment": "Soul of a New Machine by Tracy Kidder.  Not exactly CS, but great insight into early computing.", "Score": 7, "Replies": [{"Reply": "Also watch the series \u201chalt and catch fire.\u201d", "Reply Score": 4}]}, {"Comment": "You might be interested in https://paperswelove.org/\n\nIt's a community around reading/sharing comp sci papers.", "Score": 15, "Replies": [{"Reply": "Thanks ill look into it", "Reply Score": 1}]}, {"Comment": "Another one that may not be unknown is *The Elements of Computing Systems* by Noam Nisan and Shimon Schocken. It is the book version of a [great site](https://www.nand2tetris.org) for getting a real sense of what a computer is.", "Score": 7, "Replies": []}, {"Comment": "\"Programming Pearls\" is a pretty amazing book.  It has quite a few real world examples of problems and solutions.  Most of them will be from before most of the audience was born.  They all predate my computing experience.  Yet the lessons that Programming Pearls teaches can be applied today.  This book is as much about thinking smart as it is about programming itself.\n\n&#x200B;\n\nThe chapter that introduces \"back of the envelope calculations\" is worth the entire read for me.  Highly recommended.\n\n&#x200B;\n\nhttps://www.goodreads.com/en/book/show/52084", "Score": 6, "Replies": [{"Reply": "Pearls of Functional Algorithm Design in a similar theme:\n\n[https://dai.fmph.uniba.sk/courses/FPRO/bird\\_pearls.pdf](https://dai.fmph.uniba.sk/courses/FPRO/bird_pearls.pdf)\n\nReally interesting approach to problem solving.", "Reply Score": 1}]}, {"Comment": "The Fortran Coloring Book.", "Score": 3, "Replies": []}, {"Comment": "Lecture on computation by Feynman", "Score": 3, "Replies": []}, {"Comment": "*Compiler Design In C* by Allen Holub. An out-of-print classic, but available free online from the author at https://holub.com/compiler/", "Score": 3, "Replies": []}, {"Comment": "The Algorithmic Beauty of Plants.\n\nGet PDF as the print copies are rare and expensive, but a real gem of CS meets nature!", "Score": 5, "Replies": []}, {"Comment": "[Computer Systems: A Programmer's Perspective](https://csapp.cs.cmu.edu/)\n\nMay not be that much unknown.\n\nBest for 2nd level introductory book for computer system.\n\nMust be consumed after completing C language and before starting OS.", "Score": 4, "Replies": [{"Reply": "Gotta say this IS the best book to build fundamentals", "Reply Score": 3}]}, {"Comment": "If you want a cross between computer science and sci-fi, Alien Information Theory by Andrew Gallimore is a rather fun mind-bending read. Although far fetched, it does help show demonstrate abstraction", "Score": 2, "Replies": []}, {"Comment": "Pragmatic Programmer, more of a style of thinking or how to apply different methodologies (from what I\u2019ve heard) than picking up K&R, which is also a fantastic book. Taught me C in high school", "Score": 2, "Replies": []}, {"Comment": "I\u2019m not sure it\u2019s unknown, but definitely a gem: [To Mock a Mockingbird](https://en.wikipedia.org/wiki/To_Mock_a_Mockingbird):\n\n> To Mock a Mockingbird and Other Logic Puzzles: Including an Amazing Adventure in Combinatory Logic (1985, ISBN 0-19-280142-2) is a book by the mathematician and logician Raymond Smullyan. It contains many nontrivial recreational puzzles of the sort for which Smullyan is well known. It is also a gentle and humorous introduction to combinatory logic and the associated metamathematics, built on an elaborate ornithological metaphor.", "Score": 2, "Replies": []}, {"Comment": "The Art of Computer Programming by Knuth. Rare that I meet software engineers or programmers in today's world that actually know and appreciate those volumes of art.", "Score": 3, "Replies": [{"Reply": "Ive the ebook but never really started reading it.\n\nShould it be used as a reference book? Can you give some tips on how to read it?", "Reply Score": 3}, {"Reply": "Literally the most well known books on computer programming. \n\nProbably the least read, however. \ud83d\ude02. Sorry I can\u2019t execute MIX in my head.", "Reply Score": 2}]}, {"Comment": "GEB (Godel Escher Bach). It's a hard read", "Score": 3, "Replies": [{"Reply": "You think GEB is relatively unknown?\n\nIt is so famous, that even non-CS people often know about it. For CS, it was literally recommended to us by our professor in the first semester as \"one of the most well-known for fun CS-related books\".\n\nIt is great, but it is not relatively unknown for sure.", "Reply Score": 14}]}, {"Comment": "Well known within the Learning Theory community but the K&V book from the 90s is still great.", "Score": 1, "Replies": [{"Reply": "What's the full title?", "Reply Score": 3}]}, {"Comment": "I\u2019m not sure if it would qualify as unknown, but I prefer [*Computer Networks: A Systems Approach*](https://book.systemsapproach.org) by Peterson and Davie to Kurose & Ross\u2019s *Computer Networking: A Top-Down Approach* (which is still a very good book).", "Score": 1, "Replies": []}, {"Comment": "Thanks!", "Score": 0, "Replies": []}, {"Comment": "Just wanted to say there many classic statistics texts which put machine learning in a single mathematical framework", "Score": 1, "Replies": [{"Reply": "I've definitely never read anything else that has the comprehensiveness of the Murphy books. Any suggestions?", "Reply Score": 1}]}, {"Comment": "\u201cLet over Lambda\u201d (Doug Hoyle) also a great read if you are into Lisp or just like reading about it.", "Score": 3, "Replies": []}, {"Comment": "Also watch the series \u201chalt and catch fire.\u201d", "Score": 4, "Replies": [{"Reply": "The greatest show ever!!!", "Reply Score": 1}]}, {"Comment": "Thanks ill look into it", "Score": 1, "Replies": []}, {"Comment": "Pearls of Functional Algorithm Design in a similar theme:\n\n[https://dai.fmph.uniba.sk/courses/FPRO/bird\\_pearls.pdf](https://dai.fmph.uniba.sk/courses/FPRO/bird_pearls.pdf)\n\nReally interesting approach to problem solving.", "Score": 1, "Replies": []}, {"Comment": "Gotta say this IS the best book to build fundamentals", "Score": 3, "Replies": []}, {"Comment": "Ive the ebook but never really started reading it.\n\nShould it be used as a reference book? Can you give some tips on how to read it?", "Score": 3, "Replies": [{"Reply": "It should be used for exploration imo. I've never gotten through the whole series (and do not intend to in my lifetime) but few works are as fundamental yet thought provoking as the ideas therein. It's a clean break away from the idea that mfs are inspired by Leetcode solely for the purpose of looking for a lackluster tech job. TAoCP reminds me that there was a purpose.", "Reply Score": 5}]}, {"Comment": "Literally the most well known books on computer programming. \n\nProbably the least read, however. \ud83d\ude02. Sorry I can\u2019t execute MIX in my head.", "Score": 2, "Replies": [{"Reply": "Literally NOT the most well known books on computer programming. In the US, most curriculums do not cater to synthesizing material directly from TAoCP at the collegiate level. Graduate schools have their priorities on their topics and overall don't waste time thinking as abstractly. Secondary schools would not touch it with a ten foot pole when there are vastly more approachable mediums. There isn't a surge of boot camps with ambitions to have their enrollees understand this material. So where exactly is everyone getting notice about this body of work?\n\nWhile the combined works are the most fundamental and comprehensive in modern computing, it doesn't mean that people (especially CS people or anyone involved in IT by any stretch) know what it is at scale.", "Reply Score": 0}]}, {"Comment": "You think GEB is relatively unknown?\n\nIt is so famous, that even non-CS people often know about it. For CS, it was literally recommended to us by our professor in the first semester as \"one of the most well-known for fun CS-related books\".\n\nIt is great, but it is not relatively unknown for sure.", "Score": 14, "Replies": [{"Reply": "I'd even argue it's one of, if not **the** most well known CS-ish book out there", "Reply Score": 4}, {"Reply": "It literally won a Pulitzer.", "Reply Score": 6}, {"Reply": "Yeah you are probably right \ud83d\udc4d", "Reply Score": 1}, {"Reply": "My university library has 4 copies and none are due back for another week lol", "Reply Score": 1}]}, {"Comment": "What's the full title?", "Score": 3, "Replies": [{"Reply": "An Introduction to Computational Learning Theory by Kearns and Vazirani", "Reply Score": 7}]}, {"Comment": "I've definitely never read anything else that has the comprehensiveness of the Murphy books. Any suggestions?", "Score": 1, "Replies": [{"Reply": "In terms of comprehensiveness you may be right. I had in mind the books on statistical learning theory by Vapnik, which dates from the birth of neural networks and machine learning.", "Reply Score": 2}]}, {"Comment": "The greatest show ever!!!", "Score": 1, "Replies": []}, {"Comment": "It should be used for exploration imo. I've never gotten through the whole series (and do not intend to in my lifetime) but few works are as fundamental yet thought provoking as the ideas therein. It's a clean break away from the idea that mfs are inspired by Leetcode solely for the purpose of looking for a lackluster tech job. TAoCP reminds me that there was a purpose.", "Score": 5, "Replies": []}, {"Comment": "Literally NOT the most well known books on computer programming. In the US, most curriculums do not cater to synthesizing material directly from TAoCP at the collegiate level. Graduate schools have their priorities on their topics and overall don't waste time thinking as abstractly. Secondary schools would not touch it with a ten foot pole when there are vastly more approachable mediums. There isn't a surge of boot camps with ambitions to have their enrollees understand this material. So where exactly is everyone getting notice about this body of work?\n\nWhile the combined works are the most fundamental and comprehensive in modern computing, it doesn't mean that people (especially CS people or anyone involved in IT by any stretch) know what it is at scale.", "Score": 0, "Replies": []}, {"Comment": "I'd even argue it's one of, if not **the** most well known CS-ish book out there", "Score": 4, "Replies": []}, {"Comment": "It literally won a Pulitzer.", "Score": 6, "Replies": []}, {"Comment": "Yeah you are probably right \ud83d\udc4d", "Score": 1, "Replies": []}, {"Comment": "My university library has 4 copies and none are due back for another week lol", "Score": 1, "Replies": []}, {"Comment": "An Introduction to Computational Learning Theory by Kearns and Vazirani", "Score": 7, "Replies": []}, {"Comment": "In terms of comprehensiveness you may be right. I had in mind the books on statistical learning theory by Vapnik, which dates from the birth of neural networks and machine learning.", "Score": 2, "Replies": [{"Reply": "If we're talking about machine learning up to circa 2012, there are definitely a few standard texts that cover all of the work up until then pretty well. What the Murphy texts do that I haven't seen is then going on to cover diffusion models, autoencoders, transformers, GANs, score-based models, etc., in the same mathematical probabilistic framework as the rest of the text. That's really nice as someone working in those areas: it feels like you can easily get up to speed on the basics in a new area by going through text with the same notation as you've gotten used to cutting your teeth on the linear models presented earlier.", "Reply Score": 1}]}, {"Comment": "If we're talking about machine learning up to circa 2012, there are definitely a few standard texts that cover all of the work up until then pretty well. What the Murphy texts do that I haven't seen is then going on to cover diffusion models, autoencoders, transformers, GANs, score-based models, etc., in the same mathematical probabilistic framework as the rest of the text. That's really nice as someone working in those areas: it feels like you can easily get up to speed on the basics in a new area by going through text with the same notation as you've gotten used to cutting your teeth on the linear models presented earlier.", "Score": 1, "Replies": []}]},{"Title": "Technical Considerations for GUI Toolkits [Discussion]", "Score": 4, "URL": "https://www.reddit.com/r/compsci/comments/18g4cgy/technical_considerations_for_gui_toolkits/", "CreatedAt": 1702329088.0, "Full Content": "I  didn't find popular enough sub to talk about gui's, so ill just talk here. A while ago i make myself a list of possible ways to make a GUI  (reddit-specific markdown fucked up the formatting, but its still  readable enough):\n\n\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\nTypes of tools for creating a gui (and how those tools approximately work):\n\n1)Utilize the native *graphical interface API*, and depending on the platform, they have specific layers to interface:\n\n* Wayland, X11, for Linux\n* [GDI](https://learn.microsoft.com/en-us/windows/win32/gdi/windows-gdi) for windows\n* [Quartz](https://en.wikipedia.org/wiki/Quartz_(graphics_layer)) for macOS  \nExample - GTK uses [wayland](https://gitlab.gnome.org/GNOME/gtk/-/blob/main/docs/reference/gdk/wayland.md) ([source code](https://gitlab.gnome.org/GNOME/gtk/-/tree/main/gdk/win32)) [X11](https://gitlab.gnome.org/GNOME/gtk/-/blob/main/docs/reference/gdk/x11.md) ([source code](https://gitlab.gnome.org/GNOME/gtk/-/tree/main/gdk/x11)) GDI ([source code](https://gitlab.gnome.org/GNOME/gtk/-/tree/main/gdk/win32)) Quartz ([source code](https://gitlab.gnome.org/GNOME/gtk/-/tree/main/gdk/macos)) [How to use wayland display server](https://bugaevc.gitbooks.io/writing-wayland-clients/content/black-square/the-wayland-client-library.html) (TODO missing \"animation\" section)\n\n2)Utilize opengl *or other low level graphics api's* with window context, use GPU to render widgets\n\n* Window context manager - [glfw](https://github.com/glfw/glfw), [sdl](https://www.libsdl.org/)  \n\n   * contexts and surfaces, reading input, handling events  \nExample: ImGui, NanoVG, Nuklear, raylib Why? Mainly used for game development, but also good for gui's. *(i haven't seen any examples that uses this method that are used for developing general-use graphical user interfaces.)*\n\n3)Utilize multiple established gui toolkit libraries to make one common gui toolkit library  \nExample: [iup](https://www.tecgraf.puc-rio.br/iup/)  (need more examples) Why? Mainly for styling. Previously mentioned  examples like GTK might support multiple graphical interface API's to  support all platforms, but it still follows single style rule set. [GTK uses gnome rule set](https://developer.gnome.org/hig/guidelines.html), windows uses its own rule set, etc.\n\n4)Enslaved web pages\n\n* Electron Why? html and css is easier to use\n\n\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\nCurrently I'm confused, where is downside of option **2** over option **1**?  is it performance loss? I tried raylib and i found it easier, simpler  for creating gui's than any of compile-to-native gui toolkits, like gtk,  qt. I could just learn tools like opengl and/or option **1** libraries (like raylib), that way, not only do i have a tool for game dev, but also for gui dev.  \nI'm considering using opengl/raylib for custom-made text/code editor, is that a bad idea?  \nPeople  who are experienced in gui dev, i want to hear your thoughts on this  question, and also on correctness of the list above. Thank you.", "CntComments": 12, "Comments": [{"Comment": "I have 35 years of experience developing sophisticated graphics applications (Autodesk Maya 1.0) and GUI toolkits (ILOG Views). I have used plenty of languages (Pascal, C++, tcl/tkl, Java Swing, Java JWT, HTML...) and approaches (native, Qt, my own library, OpenGL, HTML Canvas, SVG...).\n\nAs of today, I'd recommend using Electron. The reason: portability, ease of transforming into a web application (which is more mainstream nowadays), and finally, the most important: durability. \n\nOS and toolkits evolve, and it's very frustrating when 10 years after having written a program you have to jump through hoops to rebuild it as the platform is obsolete and ill-maintained.", "Score": -1, "Replies": [{"Reply": "People love to hate electron for performance, and it is true that it has a lot of overhead.\n\nBut you can't beat HTML/CSS for ease of development and portability.", "Reply Score": 2}, {"Reply": "Thanks for your reply.   \nI kind of want to stay away from electron, not because i hate the web tech, but because i think it would restrict me - i could only write applications that consume more computing resources than needed, meaning i cant write ui programs for low-resource computers or an embedded devices. Also i really like snappy ui's using electron kinda limits that, i think.   \nUsing native (non-web) gui toolkits are pain-in-the-ass, but i think its worth..?", "Reply Score": 1}]}, {"Comment": "\\>where is downside of option 2 over option 1? \n\nIn 1) some features can be offloaded from the toolkit to the native platform or third-parties applications. \n\n\\>I'm considering using opengl/raylib for custom-made text/code editor, is that a bad idea?\n\nIn general, there is a ton of feature-rich editors with history, so I would suggest to write a plugin for an existing editor.", "Score": 0, "Replies": [{"Reply": "\\> In 1) some features can be offloaded from the toolkit to the native platform or third-parties applications.\n\nPlease elaborate, what do you mean by \"offloading features\", cause in both options you can copy features by just copying and pasting code.\n\n\\> In general, there is a ton of feature-rich editors with history, so I would suggest to write a plugin for an existing editor.\n\nthey all suck in their own little ways, i only use emacs because its good enough, others just dont cut for me", "Reply Score": 1}]}, {"Comment": "On Linux you can also use the framebuffer/DRM directly, or via libraries like DirectFB(2), along with virtually any vector graphics API (Cairo, Skia, Evas, etc), without having to depend on a display server like Wayland or X11.", "Score": 1, "Replies": []}, {"Comment": "The difference between two and one are in what your requirements and goals are. Even though libraries in the second often have \"gui\" in the name these are choices you go for when you know ahead of time that what you want is NOT a GUI framework. You use frameworks and platform APIs for GUIs when you know what you need are all the chunks of code dedicated to interacting with users. When you use libraries in the second category you don't get that code and would have to write it yourself. I see a lot of commentary in online spaces related to things like emulation development where people pick libraries that don't support their needs and end up extremely demotivated with regards to their projects. You pick GUI libraries when you know you need one, when you know you need things like internationalization and handling for commonplace user interactions. When you know you have need to support specific types of interfaces on specific hardware or operating systems, eg a lot of people used WPF and similar system libraries when they needed to support windows-like interfaces like the commonplace dockable multi-window layouts. If all you need to do is display some time sensitive data without much interaction then the second category becomes preferable to GUI frameworks -- there's no benefit to things like [platform-independent embedded binary management](https://doc.qt.io/qt-6/resources.html) and so forth if your application doesn't need to load and interact with those types of resources in your program.\n\n> I'm considering using opengl/raylib for custom-made text/code editor, is that a bad idea?\n\nIt's a very bad idea.", "Score": 1, "Replies": []}, {"Comment": "People love to hate electron for performance, and it is true that it has a lot of overhead.\n\nBut you can't beat HTML/CSS for ease of development and portability.", "Score": 2, "Replies": [{"Reply": "> and portability\n\nYou can't be serious. It's the opposite, I can run Qt interfaces on a LOT of hardware, and I can't port your electron crap at all.", "Reply Score": 1}]}, {"Comment": "Thanks for your reply.   \nI kind of want to stay away from electron, not because i hate the web tech, but because i think it would restrict me - i could only write applications that consume more computing resources than needed, meaning i cant write ui programs for low-resource computers or an embedded devices. Also i really like snappy ui's using electron kinda limits that, i think.   \nUsing native (non-web) gui toolkits are pain-in-the-ass, but i think its worth..?", "Score": 1, "Replies": [{"Reply": "Resource usage, even on embedded devices, is a non-issue for GUI programming. The resource usage is tied to the amount of computation and amount of data you have to manipulate, not to the complexity of your GUI.\n\nWhen I started GUI programming, I had to do with 512x342 pixels on the screen and 1Mb of RAM. And that was plenty enough for a graphical editor. Sure couldn't do 3D, though. But even on a phone nowadays, you have 3D accelerators that will swallow 20M polygons/s.\n\nAlso, GUI programming is definitely a bitch. Not from a algorithmic standpoint, but from a structural standpoint. \n\n1- read about design patterns and particularly MVC (model view controller)\n\n2- you're better off with electron and JS/Typescript to start with, but focus on the structure/services first.", "Reply Score": -1}]}, {"Comment": "\\> In 1) some features can be offloaded from the toolkit to the native platform or third-parties applications.\n\nPlease elaborate, what do you mean by \"offloading features\", cause in both options you can copy features by just copying and pasting code.\n\n\\> In general, there is a ton of feature-rich editors with history, so I would suggest to write a plugin for an existing editor.\n\nthey all suck in their own little ways, i only use emacs because its good enough, others just dont cut for me", "Score": 1, "Replies": [{"Reply": ">Please elaborate, what do you mean by \"offloading features\"\n\nA toolkit using Win32API or X11 can use system text rendering. It comes with its own small goodies for, say, copy-pasting or font handling. An openGL-backed toolkit has to do it on its own. \n\nX11 was developent as a network-oriented protocol, so using X11 you get fairly cheap remote execution capabilities. And so on. \n\n\\>they all suck in their own little ways, i only use emacs because its good enough, others just dont cut for me\n\nI very much doubt you can do better on your own in acceptable amount of time. So, why not make a plugin for emacs?", "Reply Score": 1}]}, {"Comment": "> and portability\n\nYou can't be serious. It's the opposite, I can run Qt interfaces on a LOT of hardware, and I can't port your electron crap at all.", "Score": 1, "Replies": []}, {"Comment": "Resource usage, even on embedded devices, is a non-issue for GUI programming. The resource usage is tied to the amount of computation and amount of data you have to manipulate, not to the complexity of your GUI.\n\nWhen I started GUI programming, I had to do with 512x342 pixels on the screen and 1Mb of RAM. And that was plenty enough for a graphical editor. Sure couldn't do 3D, though. But even on a phone nowadays, you have 3D accelerators that will swallow 20M polygons/s.\n\nAlso, GUI programming is definitely a bitch. Not from a algorithmic standpoint, but from a structural standpoint. \n\n1- read about design patterns and particularly MVC (model view controller)\n\n2- you're better off with electron and JS/Typescript to start with, but focus on the structure/services first.", "Score": -1, "Replies": []}, {"Comment": ">Please elaborate, what do you mean by \"offloading features\"\n\nA toolkit using Win32API or X11 can use system text rendering. It comes with its own small goodies for, say, copy-pasting or font handling. An openGL-backed toolkit has to do it on its own. \n\nX11 was developent as a network-oriented protocol, so using X11 you get fairly cheap remote execution capabilities. And so on. \n\n\\>they all suck in their own little ways, i only use emacs because its good enough, others just dont cut for me\n\nI very much doubt you can do better on your own in acceptable amount of time. So, why not make a plugin for emacs?", "Score": 1, "Replies": [{"Reply": "\\> X11 was developent as a network-oriented protocol, so using X11 you get fairly cheap remote execution capabilities. And so on.  \nthat's the kind of answer i was looking for, thanks\n\n\\> I very much doubt you can do better on your own in acceptable amount of time.  \nSame, but i'd say its worth the try", "Reply Score": 1}]}, {"Comment": "\\> X11 was developent as a network-oriented protocol, so using X11 you get fairly cheap remote execution capabilities. And so on.  \nthat's the kind of answer i was looking for, thanks\n\n\\> I very much doubt you can do better on your own in acceptable amount of time.  \nSame, but i'd say its worth the try", "Score": 1, "Replies": []}]},{"Title": "Could GDB's \"checkpoint\" functionality be used to add Universal Undo to any program?", "Score": 7, "URL": "https://www.reddit.com/r/compsci/comments/18g0opr/could_gdbs_checkpoint_functionality_be_used_to/", "CreatedAt": 1702320178.0, "Full Content": "Hey everyone. I have this crazy idea and I'm wondering if it has any chance.\n\nCould GDB's (or any decent debugger) \"checkpoint\" feature be used to effectively add an \"undo\" feature to literally any program?\n\nThis could be implemented as a wrapper around the program, and every minute or so, it runs GDB to attach to the process and save a checkpoint, plus a screenshot of the UI. The user might also be able to save a checkpoint on demand with a hotkey, let's say.\n\nAt any point, the user can check the available checkpoints, and visually decide which state of the application to \"undo\" or \"check back\" into.\n\nFor simplicity, let's assume that we only care about programs with no network access and with only a single process. But if this works well enough, some FAANG could through a few million dollars at it and figure out the rest. And only Linux support because let's face it, these sort of crazy ideas like containers and stuff are done there first.\n\nNaturally, a problem would be external files used or created by the program. These could be copied and associated to a checkpoint to be restored (maybe only storing diff information on successive checkpoints).\n\nThis would include a generic GUI for the \"restore checkpoint\" feature, which would work on any program.\n\nIndeed, this is similar to checkpoints in video game emulators.\n\nHas something like this been done? Is it feasible? I understand that this is doable on virtual machines, but then the performance cost and storage of checkpoints could be huge.\n\nEdit: Looks like [CRIU](https://en.wikipedia.org/wiki/CRIU) already implements part of this?", "CntComments": 15, "Comments": [{"Comment": "Time traveling debuggers used this idea to debug backwards. The idea was abandoned because multithreaded  makes it really hard.", "Score": 13, "Replies": []}, {"Comment": "Not all changes in program state are meaningful to the user. For instance, scrolling down a page will like change the state of the program, as will opening then closing a dialog, but a user selecting an \"Undo\" action (in, say, Word) will expect the last edit to be undone, rather than visual changes to happen backwards.", "Score": 12, "Replies": [{"Reply": "sure, but short of reading minds to figure out what's useful to the user, the proposed tool could be good enough for apps without a dedicated \"undo\".", "Reply Score": -9}]}, {"Comment": "The information density of the checkpoints would be very low, and you would need to capture an indeterminate quantity of information about the state of the rest of the universe if you wanted to be able to reproduce the transition from one to the next.\n\nGenerally, reverse-debug techniques focus on capturing the minimum amount of information needed to recreate what is lost between one checkpoint (instruction, function call, etc.) and the next, so that it\u2019s possible to rewind time by replacing current state with what was lost across each transition (basically reverting the diff between the two).\n\nThings generally \u201cgo wrong\u201d pretty quickly, so a snapshot that\u2019s a minute old is typically going to be useless for debug purposes. Being able to go back a few milliseconds is quite often more than enough.", "Score": 10, "Replies": [{"Reply": "But checkpoints in GDB are implemented by storing a copy of the process, I think, meaning they should always be restorable no matter how much time passed. They are unrelated to the hardware-assisted reverse-debugging feature which I think you're talking about.", "Reply Score": -1}]}, {"Comment": "Sure. You're entering a realm that a few people have tried to enter but none have succeeded in.\n\nThe [Mesa](https://en.wikipedia.org/wiki/Mesa_(programming_language)) desktop environment at Xerox did something similar to what you are asking about. They decided to use their debugger (capable of watching program state and externalizing it) as a wrapper to build a GUI system for their programs. It allowed them to not have to change the underlying program, and the debugger was not that much slower so it made sense. \n\nBuilding programs so that their state can be exported or altered is a huge pain.", "Score": 5, "Replies": []}, {"Comment": "CRIU doesn't implement parts of this. At least, though, it should make it clear you don't need GDB to do this\n\nThe crucial difference is that your goal is to go back in time, which is much harder.\n\nIf you suspend the process, copy the state elsewhere, restore the state, and continue running you don't have to deal with a lot of pesky problems like tracking what the contents of a file were (just what they are now and that the file is open, etc).\n\nTrying to go back in time means that networking is out of the picture, because the other side may now have an inconsistent state. Saying \"no network access\" is pretty darn limiting overall and really limits the utility. \n\nYou're assuming that file state can be rolled back, but that's not really safe, either. If you wrote file F in program A, closed it, opened program B and overwrote part of file F, and then rolled back program A, what happens to file F? There was no network access, it was just two programs running on the same computer.\n\nYou have to handle things like timers and whatnot in some way, in fact, anything working on wall clock time is going to have some weird shit going on. If something implements a timer by storing and subtracting timestamps it'll get all jacked up, too. Was the application timing how long the operation took or the actual progress of real-world time from beginning to end? A generic snapshotting application can't know.\n\nAn emulator of an NES or something is super fucking simple. It doesn't know/care about real-world time. It has a particular state, executes an instruction, changes that state. If you put it back into state N-1, and don't change any inputs, it'll get right back to state N.\n\nA virtual machine avoids problems like competing file modifications because the snapshot is just the state of the entire machine (disk included) at some point in time. It is implicit in the operation that anything that happened after the time of the snapshot is getting blown away.\n\nWhat application are you using that doesn't have a more user-friendly version of undo for which this functionality would be useful?", "Score": 4, "Replies": []}, {"Comment": "you want to read this https://doc.pypy.org/en/latest/stm.html - if you dig around you can even find the commits/branches corresponding to that work (somewhere on their gitlab).", "Score": 2, "Replies": []}, {"Comment": "Not really.\n\nThe real problem is that you'd have to capture all of the state of the program--including external file states, even files the program may no longer be reading or writing--at each 'check point', and have some way to roll all that state back.\n\nIn a sense, you'd have to capture the state *of the entire computer* (including all external files) in order to make this scheme work. (Because your checkpoint saving program has no way to know what external files are relevant to the execution of the program.)\n\n----\n\nGenerally 'undo' is implemented by the program's 'data model' saving 'change state' objects. That is, for every change to the data model of the program, an inverse change state object is crated, and stored on a stack. \"Undo\" basically applies the undo change to the model.\n\n(For example, a text editor may capture typing the letter 'X' as the instruction \"insert 'X' at position 5\"; undo simply adds a corresponding \"delete position 5\" to an undo stack. Undoing then executes the top instruction in the undo stack--and ideally stores the original insertion into a 'redo' stack.)\n\nIt puts more effort on the programmer (because you have to consider your data model as something that can be changed by a small set of commands, each of which has an inverse command associated with it), but you don't have to then save the entire state of your computer to make it work.", "Score": 2, "Replies": []}, {"Comment": "The answer is no. You can't unwrite a byte to a piece of hardware, e.g. (Or in layman's terms: if you hit \"print\", and a sheet of paper spits out of the printer, you can't hit \"undo\" and have the printer take that sheet back, remove its ink, etc.)\n\nThe idea of \"reversible computing\" is old, but it's never really cleared that hurdle.", "Score": 2, "Replies": []}, {"Comment": "We already have an universal undo: The Command Pattern :-)", "Score": 2, "Replies": []}, {"Comment": "I used CRIU extenssively in my project. While the project is amazing and it is even possible to restore TCP/IP sockets it is not as quick as one would like.\n\nWe used to snapshot containers and resume them on a different machine by restoring the state. It does a great job but when we attempted to perioeically snapshot containers we couldnt so it without pausing execution(makes sense if you want  a constant state) and then resuming it. \n\nThe time it took to do this introduced a signifficant delay. I recently did some testing of performance, which largly depends on the programs memory usage if intereates I can share.", "Score": 2, "Replies": [{"Reply": "that's amazing, thanks for your very relevant info", "Reply Score": 1}]}, {"Comment": "> But if this works well enough, some FAANG could through a few million dollars at it and figure out the rest.  \n\nI guess but honestly building in an undo as part of the product requirements, is so much easier than trying to hack a debugger to do it. \n\nYour idea works like you said... for very simple single process applications, where state is contained entirely to program memory. I.e. its great a development and debugging tool, but not great as a production application feature.", "Score": 1, "Replies": []}, {"Comment": "Run debug builds?", "Score": 1, "Replies": []}, {"Comment": "sure, but short of reading minds to figure out what's useful to the user, the proposed tool could be good enough for apps without a dedicated \"undo\".", "Score": -9, "Replies": []}, {"Comment": "But checkpoints in GDB are implemented by storing a copy of the process, I think, meaning they should always be restorable no matter how much time passed. They are unrelated to the hardware-assisted reverse-debugging feature which I think you're talking about.", "Score": -1, "Replies": []}, {"Comment": "that's amazing, thanks for your very relevant info", "Score": 1, "Replies": []}]},{"Title": "Optimal Splitting of Feature-Target Pairs within Input Size Constraints", "Score": 5, "URL": "https://www.reddit.com/r/compsci/comments/18fwr9y/optimal_splitting_of_featuretarget_pairs_within/", "CreatedAt": 1702307621.0, "Full Content": " \n\nHi there, I'm dealing with two lists, one comprising features and the other containing targets. The task at hand involves processing all possible feature-target pairs. Ideally, I'd combine these two lists into a single input for processing. However, due to an input size constraint, I'm required to split them and merge the outputs afterward. I experimented with taking all the features in all the chunks and adding as many unprocessed targets as possible to each chunk. Unfortunately, this approach doesn't accommodate scenarios where the features alone surpass the input size constraint.\n\nIs there an algorithm or an optimized approach that can intelligently split these feature-target pairs, ensuring eventual processing of all pairs while minimizing the number of required calls?", "CntComments": 5, "Comments": [{"Comment": "Do you know what your size constraints are? Like can you fit a K fraction of input at once or less, are you wanted a general solution or something you can just use one and done?", "Score": 1, "Replies": [{"Reply": "The constraint is constant but the size of the elements varies so I do not have a rule of how many fit. I'm interested in a general solution, what is the best approach? Try to take an equal amount of features and targets are first as many features as possible maybe?", "Reply Score": 1}]}, {"Comment": "Where is your constraint being applied?  Are you able to just continually load everything into memory and then do the processing there?\n\nIs the output straight up N*M in size or is there some sort of filtering being applied for pairs that are invalid?", "Score": 1, "Replies": [{"Reply": "The constraint is on the size of an API call input, and the output would be N\\*M if I could load all the features and targets to the input.", "Reply Score": 1}]}, {"Comment": "The constraint is constant but the size of the elements varies so I do not have a rule of how many fit. I'm interested in a general solution, what is the best approach? Try to take an equal amount of features and targets are first as many features as possible maybe?", "Score": 1, "Replies": [{"Reply": "I think conceptually the variable size of elements doesn\u2019t matter much. Just spilt the arrays into K chunks of equal size (not equal number of elements) and then do all K^2 in the obvious way.", "Reply Score": 2}]}, {"Comment": "The constraint is on the size of an API call input, and the output would be N\\*M if I could load all the features and targets to the input.", "Score": 1, "Replies": []}, {"Comment": "I think conceptually the variable size of elements doesn\u2019t matter much. Just spilt the arrays into K chunks of equal size (not equal number of elements) and then do all K^2 in the obvious way.", "Score": 2, "Replies": []}]},{"Title": "Build a fantasy CPU emulator in JavaScript", "Score": 0, "URL": "https://codeguppy.com/blog/fantasy-cpu-emulator/", "CreatedAt": 1702348517.0, "Full Content": "", "CntComments": 1, "Comments": []},{"Title": "Scrambling eggs for Spotify with Knuth's Fibonacci hashing", "Score": 26, "URL": "https://pncnmnp.github.io/blogs/fibonacci-hashing.html", "CreatedAt": 1702235436.0, "Full Content": "", "CntComments": 3, "Comments": [{"Comment": "Great post and nice diagrams - thanks for sharing! \n\nThis shows that there are interesting problems to be found/solved everywhere - even something like randomization that most people never even give a second thought about leads to a rabbit hole of user research and cool algorithms and optimizations.", "Score": 3, "Replies": [{"Reply": "Thank you!\n\nI agree with you. There's some really cool math behind these things. I encourage you to check out [Does Your iPod Really Play Favorites?](https://www.researchgate.net/profile/Jessica-Culhane/publication/227368948_Does_Your_iPod_Really_Play_Favorites/links/5766fb8b08aedbc345f5f4db/Does-Your-iPod-Really-Play-Favorites.pdf)\n\nI just discovered it yesterday.", "Reply Score": 4}]}, {"Comment": "Excellent post! I delved into Knuth's Algorithm X for a project recently, although I didn't implement it. Still, it was a fun & rewarding experience. Thanks for sharing!", "Score": 2, "Replies": []}, {"Comment": "Thank you!\n\nI agree with you. There's some really cool math behind these things. I encourage you to check out [Does Your iPod Really Play Favorites?](https://www.researchgate.net/profile/Jessica-Culhane/publication/227368948_Does_Your_iPod_Really_Play_Favorites/links/5766fb8b08aedbc345f5f4db/Does-Your-iPod-Really-Play-Favorites.pdf)\n\nI just discovered it yesterday.", "Score": 4, "Replies": []}]},{"Title": "The N\u00d7log(N) Limit of Multi-Factor Authentication", "Score": 0, "URL": "https://depauth.com/blog/mfa-n-log-n", "CreatedAt": 1702126880.0, "Full Content": "", "CntComments": 1, "Comments": [{"Comment": "\u201cAssume like 5 oversimplified constraints that don\u2019t correspond to real life. Here\u2019s a weird result that comes when you do that.\u201d", "Score": 6, "Replies": []}]},{"Title": "Why do x86 processors take up so much energy compared to ARM?", "Score": 147, "URL": "https://www.reddit.com/r/compsci/comments/18cup5v/why_do_x86_processors_take_up_so_much_energy/", "CreatedAt": 1701952646.0, "Full Content": "I understand they have CISC and RISC ISAs and all that.\n\nBut what happens down there inside the processor that makes it consume so much power?\n\nContext: \nmy Intel Core i7 MacBook Pro runs out of battery within 6 hours.\n\nMy M2 Pro Macbook runs for over 48 hours.", "CntComments": 111, "Comments": [{"Comment": "Intel did not have an incentive to focus on low power market for a long time. They focused on other aspects. But things are changing these days.", "Score": 142, "Replies": [{"Reply": "Which I'm frankly surprised at given how cheap electricity is and how with a laptop you have to carry around chargers anyway if you want to guarantee usage.", "Reply Score": 13}, {"Reply": "Thats the answer to the question. It is also not true. Intel had as much incentive to reduce power consumption as the rest of the competition. The competition was simply faster and better when it comes to mobile computing.\n\nIntel has also been putting their power saving improvements among the main marketing featueres in their CPU lines.", "Reply Score": 3}, {"Reply": "Speak for yourself, electricity is fucking expensive here (Germany)", "Reply Score": 8}, {"Reply": "Who would not wish thin, light, silent and cool laptop?\nI guess Intel should have focused on power efficiency long time back. May be they attempted with atom series of processors but failed due to their legacy of x86 platform.", "Reply Score": 1}]}, {"Comment": "In hand wavy terms it's because Intel is lugging around decades of compatibility that have crossed major thresholds from 16 to 32 to 64 bit processing.  \n\nSome of that is just area on the chip that causes traces to be longer.  Some of that may be keeping the chip thermals from getting too out of whack.  \n\nAnd Arm took some design choices in what they wanted to emphasize.  They traded some compute performance for better memory performance.  Which in many cases is a serious bonus.\n\nArm did some optimizing for modern programs, without having Intel's compatibility baggage. Plus having an expectation that the chip would need to be expanded from it's inception.\n\nThe deep down stuff is going to be proprietary..", "Score": 36, "Replies": [{"Reply": ">from its* inception", "Reply Score": 1}]}, {"Comment": "might be better to ask it in /r/hardware", "Score": 44, "Replies": [{"Reply": "Best advice given how wrong some answers are.", "Reply Score": 20}, {"Reply": "Sure", "Reply Score": 1}]}, {"Comment": "This is a really good article explaining your question:\n[ARM Architecture](https://arstechnica.com/features/2020/12/how-an-obscure-british-pc-maker-invented-arm-and-changed-the-world/)", "Score": 5, "Replies": []}, {"Comment": ">But what happens down there inside the processor that makes it consume so much power?  \n>  \n>Context: my Intel Core i7 MacBook Pro runs out of battery within 6 hours.\n\nRight now, from what we know, it's mostly down to the manufacturing process -- your M2 Pro Macbook is built on the latest TSMC process, the most advanced in the world.  ARM also has a long history of focusing on power consumption, because it had to, as they are embedded in all sorts of smallish devices.  Apple also focused on power consumption, because your M2 is essentially a mobile chip.\n\nAs I understand it, very high end AMD chips, produced on the highest end TSMC process, have similar performance per watt.\n\nSee for example: [https://forums.macrumors.com/threads/amd-claims-new-laptop-chip-is-30-faster-than-m1-pro-promises-up-to-30-hours-of-battery-life.2375919/](https://forums.macrumors.com/threads/amd-claims-new-laptop-chip-is-30-faster-than-m1-pro-promises-up-to-30-hours-of-battery-life.2375919/)\n\nCertain things *may* impact the inherent efficiency of x86 like its variable length instructions.  And also Apple Silicon has a very wide (8 wide) instruction decoder compared to the best of x86 (4ish).  But there may also be some benefit to variable instruction sizes too, for example x86 instructions may have a smaller size in cache, and as you may know being cache friendly is very important these days.\n\nYou should also consider software impact.  Apple controls much of the software stack.  That is -- Safari can pick and choose which cores to use for what on its Apple Silicon.  Compare your battery life using Chrome against Safari, etc.\n\nYou might see: [https://www.anandtech.com/show/16226/apple-silicon-m1-a14-deep-dive/2](https://www.anandtech.com/show/16226/apple-silicon-m1-a14-deep-dive/2)", "Score": 18, "Replies": [{"Reply": "Thanks. Where can I find resources on TSMC? Never heard of this term before", "Reply Score": 1}]}, {"Comment": "Wrong sub for the question. You'll probably have the best luck in r/hardware\n\nMeaning, 2/3 of what you'll get even there will be false (this is reddit, after all), and the other 1/3 will be \"close but incomplete with inaccurate weights attached to factors discussed\", and limited tools for knowing which is which.\n\nWhat you're asking requires deep understanding of internal u-architectures, logical and physical designs of modern industry leading implementations of processors from completely different companies at the same time (by one person). Such people barely exist at all in principle - I've worked with these guys, they aren't gods, they are people, and there are limits to one's ability to simultaneously know the big picture and the details across such a diverse domain. A randomly selected working architect (there are very many) of an Intel CPU at Intel does not necessary have a good answer to such a deep question about ARM off the top of their heads.\n\nAnd then most of the relevant info is a trade secret behind numerous NDAs. And then real professionals with experience and at that level don't hang out on Reddit in the first place - it's too toxic and too full of folks who have no idea what they are talking about to bother.\n\nI'd recommend googling extensively and for a substantial amount of time, searching for survey-style papers (Google Scholar is a great tool), reading up on a lot of that, checking out textbooks, very tentatively - available online lectures from renown professors at well-known universities, and getting some sort of rough picture of what the answer might be. Once again, approximately so, as none of the public sources are that privy into what corporations are doing behind closed doors (I've seen my fair share of issues in those sources too).", "Score": 23, "Replies": [{"Reply": "Chip architect here and definitely a person. I have worked across the x86, ARM, and RISC V space. There is quite a bit of cross pollination regarding architects and ISAs. Not all of us, but many of us do move around. There are challenges moving from ISA to ISA but the core concepts of extracting performance and power efficiency are the same.\n\nRegarding why Apple enjoys a perf/watt advantage is multifactorial but I can shed light on some of the key points. \n\n1) Apple is not nearly as restricted in hitting a price point for its chips vs other players. Traditional silicon design houses need to hit a price point because they need to sell their chips to OEM and have a margin of profit.\n\n2) Building off the first point, Apple can take the yield hit and throw silicon at the problem. Their philosoply is to achieve performance by being wide but relatively slow in terms of frequency. This is opposed to being narrow and faster. What I mean in terms of narrow/wide is the amount of instruction parallelism the silicon can handle. It's important to note that this isn't so much a function of ISA as it is a design choice. Apple uses width to achieve performance whereas the traditional players lean more heavily on narrower structures and higher frequency to achieve performance.  \n\n3) Being wide and slow benefits power efficiency because of the way power scales with frequency and voltage. Being \"slow\" allows the silicon to operate at a lower voltage. Dynamic power scales cubically with frequency (effective capacitance \\* frequency \\* voltage\\^2) and leakage power scales exponentially with voltage so you can a huge efficiency boost by being wide at the cost of extra silicon. \n\n4) Apple can extract extra efficiency because they are vertically integrated in terms of OS and hardware. Their scheduler and power management policies can be tightly coupled with their hardware. Other silicon vendors / OS combinations are limited because silicon design houses don't control the OS and vice versa.", "Reply Score": 5}, {"Reply": "This helped a lot. Ty", "Reply Score": 2}]}, {"Comment": "While its not limited to transistor count, the number of transistors in a microprocessor plays a huge role in energy consumption, especially for CMOS.\n\nThe logic gates of most modern microprocessors are implemented through a fabrication technique know as CMOS, or Complimentary Metal Oxide Semiconductor. This technique leverages a type of transistor know as a MOSFET, or Metal Oxide Semiconductor Field Effect Transistor. CMOS pairs n-channel MOSFETs and p-channel MOSFETS together in a manner which results in a high impedance. This occurs because the polarity needed to basis the gates of the two devices are opposite of one another. Since the impedance is high, the current that flows as a result is lowered, which in turn lowers the power consumption. This is why CMOS is such a popular fabrication process and where it gets the \"complimentary\" part of its name from.\n\nThe MOSFETs used by CMOS, as the name would suggest, are field effect transistors. They have a source, a gate, and a drain. The gate input is used to control the device by applying an electrical field that causes a channel to form between the gate and source. MOSETs can be thought of as a voltage controlled current device. That is, the magnitude of the voltage applied to the gate affects the amount of current that can flow between the source and drain.\n\nDepending on the device, the input applied to the gate can make the device act like an amplifier, with a small signal applied to the gate resulting in a larger signal in the form of current flowing between the drain and source. Past a certain point, a voltage applied to the gate causes the device to act like a switch. This switch like behavior is the mode of operation used by CMOS and what makes MOSFETs such an ideal transistor type for this application.\n\nA caveat of MOSFETs are their gates result in a small amount of capacitance to form since the gate is conductive and insulted from another conductor material via a dielectric material. This gate capacitance stores an electric charge that must build up in order for the voltage applied to the gate to form and for the channel between the drain and source to form. This capacitance must also discharge in order for the voltage applied to the gate to decline.\n\nThe charging and discharging of this gate capacitance is a major contributing factor to the power consumption of CMOS devices. It's also why the power consumption tends to increase as the clock speed of the device increases since you are charging and discharging the gate capacitance more often. Given two microprocessors with the same transistor density and node process, the device with less transistors will often consume less energy, but this is also effected by other factors too.\n\nImplementations of the ARM instruction set tend to use less transistors per instruction and have less instructions than their x86/x64 counter parts. This often results in them consuming less energy. There are also other factors that affect the energy consumption too. Other design aspects of a microprocessor, such as multi-threading and branch prediction can contribute to energy consumption too. So its not just the transistor count. \n\nThis is a very high level overview of why ARM tends to use less energy than x86, and CPU energy consumption in general. So I simplified a lot of it. I'm sure others will jump in and correct me with more specifics. Its a very complicated subject and one in which a lot of research has been done. As others have suggested, this might be a good question to post in /r/hardware or do further research elsewhere if you wish to know more. I hope this helped answer you questions.", "Score": 3, "Replies": []}, {"Comment": "ARM has always been extremely energy conscious, on the game boy advanced the ARM architecture had a special optional mode for an ever further reduced ISA called THUMB (get it, arm, thumb) when in this mode only basic operations were available but electricity usage is INSANELY low, you could keep the CPU alive for like WEEKS (rather than ~10 hours) on 2 AA batteries so long as you stayed in THUMB mode (which was hard, only 8bit instructions etc).\n\nIMHO these were awesome ideas! the fact that frequency scaling has hit such hard limits has meant all architectures have approached a kind of wall and now other properties (like energy efficiency) are the kickers making the difference in decision making,", "Score": 3, "Replies": [{"Reply": "> ARM has always been extremely energy conscious, on the original game boy the ARM architecture\n\nThe CPU on the original game boy was a Zilog Z80. The Z80 was ISA compatible with the Intel 8080. Nothing to do with ARM.", "Reply Score": 1}]}, {"Comment": "[deleted]", "Score": 2, "Replies": [{"Reply": "I think the \"culture\" of Win32 could be another issue. At Restaurant we run Ubuntu 22 LTS and we need to use Windows 10 just having a printer driver for a .NET application. The OS and Applications happily use both i5 cores for almost 20 minutes until the system become usable. Having the same printer I can compare it to MacOS and a pretty conservative Linux such as Ubuntu LTS. The crazy \"checking updates\" drama doesn't happen on both. I am saying even if there was a AI that will convert all that  x86-64 setup exactly to M2 code an OS which profiles the entire disk will eat the battery too. Just updating a single exe will trigger \"compatibility telemetry\". Imagine suggesting this idea to a Linux distribution or Apple. Another example? Steam. You need to add 32bit architecture to Linux to be able to run Steam. That means having an entire 32bit userland for a single application.", "Reply Score": 1}]}, {"Comment": "It's probably just by virtue of reduced instructions and tighter pathways on the ARM that saves it the most power.", "Score": 2, "Replies": []}, {"Comment": "Didn't see this mentioned in the other answers. \n\nThe difference between CISC and RISC is a key factor if you look at what parts consume most power in the processor.\n\nAll instructions that go into the processor need to be \"decoded\" into the control signals that orchestrate the execution of that instruction in the processor. So, the \"Decoder\" is the start of the processor pipeline. \n\nThe complex instructions of the CISC x86 ISA require a very elaborate hardware that consumes a lot of power. ARM's RISC ISA is designed specifically to simplify the Decoder hardware. That is a major factor that decides the total power consumption of the processor.\n\nIf you want, you can look at the RISC-V ISA and see how easy it is to decode. It is an open ISA and there are textbooks describing the design of the ISA and hardware.", "Score": 2, "Replies": [{"Reply": "Ahh now it makes sense!", "Reply Score": 1}]}, {"Comment": "[deleted]", "Score": 5, "Replies": [{"Reply": "The last generation of Intel Macbooks had battery life much shorter than the new ARM models as well, so even in a more direct comparison the ARM chips seem much more efficient.", "Reply Score": 12}, {"Reply": "Either way, other laptops running Intel and AMD processors run for about 7-8 hours before they give up", "Reply Score": 2}]}, {"Comment": "probably you don't compare apples to apples. e.g. this comparison from well-reputed site: [https://www.notebookcheck.net/Apple-MacBook-Pro-14-2023-review-The-M2-Pro-is-slowed-down-in-the-small-MacBook-Pro.687345.0.html#toc-7](https://www.notebookcheck.net/Apple-MacBook-Pro-14-2023-review-The-M2-Pro-is-slowed-down-in-the-small-MacBook-Pro.687345.0.html#toc-7)\n\nstates that Macbook Pro on average runs only 1.5x longer than the average notebook in its class (see the table in \"Battery Life\" section)\n\nthere are books that run about as long as your one, but they have worse performance. M1/M2 cpus has better perf/energy ratio than x86 ones because they were derived from smartphone cpus, and there was so much competition in this market that the top cpus (i.e. apple ones) managed to became almost as fast as desktop cpus.\n\ndesktop cpus, OTOH, stagnated in 2010s due to lack of competition. So, basically it's how competition works and have very little about RISC vs CISC wars :)\n\nBTW, we had opposite situation at late 90s - despite initial speed advantage, RISC cpus lost to x86 CISC cpus because desktop market was extremally competitive in 90s, while RISC market was almost killed by waiting for Itanium.", "Score": 2, "Replies": [{"Reply": "RISC was killed because SPARK and MIPS did not want to give up their margin. Also no investment in fabs. Someone pocketed all the money and the companies went bankrupt.", "Reply Score": 5}]}, {"Comment": "ARM employs a big.Little cluster combination.  The little clusters have fewer transistors and draw less power. When the device runs non-intense tasks it schedules to the low power cluster.  Processes that are hungry and can fit into the high power cluster are targeted there.  Otherwise they also get scheduled to the little cluster.  So that's the downside.  They became popular for mobile devices that need to conserve battery life.  And somewhat oddly have found their way into powered laptops, desktops and even data centers like AWS.", "Score": 1, "Replies": []}, {"Comment": "CISC chips waste a lot of power translating from a CISC instruction to RISC uOPs. There's a lot of decoding and shuffling around that the CPU has to do to execute them.", "Score": -3, "Replies": [{"Reply": "This is incorrect. CISC vs RISC is completely irrelevant in reality.", "Reply Score": 9}, {"Reply": "Love the downvotes from people that don't know shit", "Reply Score": 0}, {"Reply": "Why would CISC chips translate CISC to RISC to execute? Isn't the whole point of CISC chips that they can run CISC commands natively? Or are you just saying that it is harder to decode CISC commands to actually execute them?\n\nAt least that is my memory of my computer architecture course. I see in your other comments that you work in the industry so I could definitely be wrong.", "Reply Score": 1}]}, {"Comment": "CISC and RISC are very unlikely to have anything to do with it. Intel has long been RISC under the hood and the vast majority of ARM based devices are not going to get anywhere near that level of battery life.\n\nThe real reason is that Apple controls the entire environment, they can tweak the CPU, GPU, OS,  battery and all other hardware on the device and they invested massively to make the \"new\" ARM based laptops extremely power efficient holistically. A cheap Windows laptop, in comparison will be a hodgepodge of all sorts of components, graphic accelerators, competing settings and dumb games and viruses that are all eating up energy.\n\nSomewhere in between is your i7 Macbook Pro that is probably a couple of years old, with less battery capacity and power saving features (and experience) to begin with and a more worn battery. Or a top-of-the-line reputable machine (Lenovo X1, Dell XPS), that will likely get numbers much closer to your M2. (Though I believe that current Macs are the undisputed champions of long battery life.)\n\nTLDR: new macbooks are power efficient because Apple put an insane amount of resources into making it that way in all conceivable regards.", "Score": -12, "Replies": [{"Reply": ">dumb games and viruses\n\nYou seem biased.", "Reply Score": 7}, {"Reply": "I see. Maybe it is because my macbook is old. \n\nIve always been told that x86 has a complex ISA so the compilers are simple.\n\nArm has a simple instruction set hence the complex compilers.\n\n\nI never understood why they are that way!", "Reply Score": 0}]}, {"Comment": "It also feels like the Intel macs got substantially worse throughout their run, which makes me think battery degradation or OS degradation plays into it.", "Score": -1, "Replies": []}, {"Comment": "More transistors, more power draw.\n\nEdit: Apple is not exempt from the fundamental laws of physics and silicon behavior. You can pause cores and pipelines all you want, it\u2019s still true.", "Score": -6, "Replies": [{"Reply": "The M3 is likely to have an order of magnitude more transistors.", "Reply Score": 7}]}, {"Comment": "A big part of it is different design philosophies. x86 is server first. Servers care about performance first, energy consumption second. Consumer x86 cores have traditionally just been cut down server cores. ARM chips like the M2 were designed with laptop/battery use in mind, so energy consumption came first, performance came second. \n\nIf you wanted to you could design an x86 chip with energy consumption as the main priority or an ARM chip with performance as the main priority. Intel is starting to consider energy consumption more with their P/E cores since they are starting to care more about the consumer/laptop market. But before that they did not care and used scaled down server cores as consumer cores rather than designing specifically for the consumer.\n\nThe other benefit of Apple's M2 chips is power management. x86 often has the entire CPU running all the time but ARM chips are better about turning off parts they don't need. Apple was also able to design the M2 with the software it would be running in mind, so they can make optimizations that Intel can't.", "Score": 1, "Replies": []}, {"Comment": "Apple has a different history, they made portable devices, they also put up the ultraportable form factor. All those years they focused on thin and low power electronics. I forgot where I heard it but they explicitely asked fabs how to get the absolute best perf/power ratio possible with any given node.\n\nIntel is majoritatively a desktop (or larger) cpu designer.", "Score": 1, "Replies": []}, {"Comment": "Something to also take into account is that Intel laptops use discrete RAM. In the M2 the ram is integrated in the same package. \n\nAlso M2 architecture is Big.little, i.e. there are small efficient cores and large performance cores. Usually only the efficient cores are used in day to day tasks. \n\nIntel only had high performance cores, until Alder Lake (12th generation), plus it has a lot of baggage you carry for x86_64 support and a more capable I/O", "Score": 1, "Replies": []}, {"Comment": "I'm a bit surprised nobody \u2014 at least just running my eyes over the other comments \u2014 mentioned an incredibly important factor:\n\nYes, Intel processors use more power, naturally. But try to install **macOS** on your i7 PC and take a look how the battery lifetime goes.\n\nSoftware ain't 100% of the answer, but it's a *huge* part of it.", "Score": 1, "Replies": []}, {"Comment": "> Why do x86 processors take up so much energy compared to ARM?\n\nThis is a bit of a misconception. As someone pointed out, it's like comparing apples and oranges.\n\nAlso, the replies saying \"CISC v. RISC\" are not correct, this isn't really about CISC v. RISC.\n\nThe x86 architecture began its life primarily targeted at desktop computing. For this reason, x86 prioritizes low-latency over throughput. In the late 90's, x86 started to expand into server space with the rise of COTS-based server farms. For most server applications at that time, latency was the enemy, so the architecture was further optimized for latency. There has been a shift in demand since then, so a lot of server workloads are now throughput-oriented, but that simply wasn't the case 20 years ago when the x86 architecture was still being defined.\n\nTo use an analogy, a latency-optimized architecture is like a muscle-car designed to come off the line as fast as possible. You don't have to floor it if you don't want to, and it won't guzzle gas. But it will never be as efficient as, say, a hybrid car optimized to have the maximum range per gallon. This is like ARM, which is a throughput-optimized architecture. It is optimized to compute as many cycles as possible on the least energy possible, even at the cost of latency versus x86. I'm not aware of any ARM-based gaming desktops, correct me if I'm wrong. It's just not well-suited to that kind of ultra-low latency application. x86, however, is ideal for that and the internal architecture is tuned for it, from the ground-up. If you can afford to dump 500-1500 watts into your motherboard, the x86 CPU will maintain the highest possible frame-rate on that first-person shooter without any problems, just like a muscle-car will jump off the line when you floor it.\n\nSource: Me. I have done engineering work on the x86 architecture.", "Score": 1, "Replies": []}, {"Comment": "There are a lot of factors but I think because primarily Intel's market has been desktop pc, so power efficiency is not a key factor. \n\nAlso Intel needs a lot of circuit to remain compatible with legacy software, for example segmentation is not used by linux and windows afaik. This adds another layer of addressing that is not being used, but it draws power. Arm only supports pagination for example. \n\nMac also has more control over what software must be run, and they can remove unused features. But I'm not completely sure this is the main reason, because M chips can run x64 code.", "Score": 1, "Replies": []}, {"Comment": "You can't really compare the two in the way you want. Apple is doing something completely different here that Intel simply is unable to respond to. \n\nTake one part of the performance of Apple Silicon: retain and release of objects. That one thing is 5x faster on Apple Silicon than on Intel. And the software on your Mac does it a LOT. So how did Apple get a 5x speed benefit?\n\nWell, almost all software on the Mac is written in Objective C or in Swift (which Apple developed), both of which rely on reference counting of objects instead of garbage collection to release memory. The software was almost certainly compiled in Xcode (Apple's IDE) using Apple's Swift compiler on LLVM (the open source toolchain that Apple principally developed). Apple Silicon is then optimized specifically to quickly retain and release objects thanks to that reference counting approach. Another benefit of this is that these systems are much more efficient with heap allocation, they use caches more efficiency, so the caches operate more efficiently. There's also special hardware for branch prediction which is tuned specifically to how message passing works. \n\nThe issue isn't ARM vs x86, it's how when you have top to bottom control of a system, how you can make decisions from the design of a language, through how the compiler works to design decisions in which instructions to include and where your optimizations should happen. If you can get a 5% performance improvement at each level, that compounds to being able to double or more the performance in certain areas. \n\nThe problem with x86 isn't x86 - it's that Intel needs to serve a million masters - differently languages with performance bottlenecks in different places, different compilers, different operating systems with their own takes on scheduling, different OEMs that want performance and cost trade-offs in different places and so on. And each generalization to accommodate a different set of masters costs you something. And they add up, a LOT. This is also why Apple Silicon is so much faster than Qualcomm ARM chips on a similar process. It's why Apple Silicon is so much more RAM efficient. Apple tells developers 'convert your app to 64 bit or we'll pull it from the store'. They did that 5 years ago. So Apple Silicon can pull 32 bit ALUs, they can remove the need to dispatch 32 bit instructions, they can yank all of those complications out, which don't add a lot of performance, but again, it compounds with everything else. 5% in a lot of places becomes a lot of performance. Intel can't do that. Qualcomm can't do that. Component vendors have to serve the lowest common denominator - and that becomes it's own kind of technical debt. Apple doesn't do that - they are brutal about retiring the stuff they think it outdated, so their engineering is by comparison MUCH cleaner and efficient.", "Score": 1, "Replies": []}, {"Comment": "in 48 vs 2 hours its not about Cpu HW.\n\nShitty drivers and shitty hardware from random vendors cant do proper power management and platform never cant get to the proper low power states fast and often enough.", "Score": 1, "Replies": []}, {"Comment": "The biggest reason is you can't optimize for every factor simultaneously. You have to prioritize your goals, and sometimes choices that help one goal hurt another. It's like when you're writing code and you have to choose between code that runs faster versus code that uses less memory. Both approaches have merit, and which is better depends on the situation.\n\nIntel's primary market is desktop PCs and servers running off wall power. They prioritize maximum performance over everything else. Their design process is to set a target price / size / transistor count for a chip, and then do whatever they can to make it run as fast as possible. They will throw more cores, more compute blocks, more cache at things whenever possible to make it faster, as long as it fits within the overall budget for the chip.\n\nARM's primary market is battery powered devices. They're going to prioritize low power draw over everything else. If they can save some power at the cost of a little bit of speed, they'll do it. They'll be more conservative on how many of each component they put in the chip, as each one increases the power draw.\n\nCompletely making up an example here, let's look at math units. Adding the ability to do two adds at the same time might give you a 20% performance boost, and doing three adds might gain you another 5% above that. Intel would favor the performance and choose to have three add units on the chip while ARM would choose two because of the lower power draw.\n\nAnother difference is some of the core design decisions in Intel's chips go back to the early 1970s. ARM was founded in the mid 1980s. As far as chip design goes, that's a significant difference. ARM chips have a much more modern core, so there's some efficiency to be gained there.\n\nAnother reason is clock speeds. As you ramp up the clock speeds, power consumption increases faster than performance does. Intel chip speeds generally target a different point on the curve, increasing both performance and power consumption. Apple is able to somewhat make up for this with their design that includes the RAM inside the CPU. This allows them to run the RAM faster than an Intel chip can, which counters some of the performance difference in the processors.", "Score": 1, "Replies": []}, {"Comment": "But I will have to say Intel processors are very less error prone and reliable compared to Arm. Specially when you deal with embedded systems( opposed to desktop applications) Intel is solid. Arm has some random failures and what not during board bring up. Never saw that in Intel processors. May I repeat that Intel processors are solid.", "Score": 1, "Replies": []}, {"Comment": "Both \"CISC vs RISC\" and \"ARM vs x86\" has little to do with power efficiency in purely technical sense.\n\nThe biggest source of difference is the business strategy. Traditionally, both Intel and AMD targeted the server market first, and sold a reduced version of server chips in the consumer market. The raw computing power had always been the main goal, and power efficiency had been a second-thought.\n\nApple changed this trend by producing a powerful chip that is also power-efficient. However, unlike all the crazy hype around this chip, it's more or less a chip with different balance b/w performance and power consumption. As a proof, [AMD did catch up with this trend](https://www.phoronix.com/review/apple-m2-zen4-mobile/7) rather quickly, while Apple has been not-so-successful with their up-scaled M-series chips in Mac Pros.", "Score": 1, "Replies": []}, {"Comment": "A couple of big reasons and they all contribute:\n\n1) SoC approach - by placing all processing, interconnects, RAM, GPU onto a small integrated package, the voltage and power requirements are all reduced and performance increases. Having a memory subsystem with enough bandwidth for a decent GPU and low enough latency for a good CPU means the CPU are less bottlenecked by the memory subsystem than Intel/AMD systems.\n\n2) Manufacturing process - this is purely a money play, but Apple only produces a relatively small number of SKUs compared to Intel/AMD and they can afford to purchase vast manufacturing capacity on the world's most advanced processes.\n\n3) Mobile First High Performance RISC - this gets to the core of your question. Apple largely developed the ARM64 platform for their needs for the A7 chip in the iPhone 5S and development was mobile focused for many generations, though desktop was in the plans. You can get performance by cycling the logic faster or doing more operations per cycle (and eventually both). Power requirements rise as a power function with increased frequency but more slowly with the increased transistor count required to do more per cycle. Therefore, Apple chose to make physically larger CPUs that ran at a lower clock speed than the competition and purchase the best manufacturing process possible to keep the economics viable. \n\nNot all ISAs can easily go wide and this is a key point. The current A17 and M3 CPUs decode  9 instructions per cycle per core. I think these are the widest consumer cores on the market. Intel/AMD until recently were at 4 instructions per cycle and I think Intel was able to push theirs to 4 decode and 6 dispatch. This is where RISC vs CISC does play a role. CISC instructions are variable length and RISC are fixed length. It is far easier to decode the incoming streams of bits into instructions when the logic knows exactly how long each instruction is. The CISC cores get choked up on the front end and the execution units aren't as fully utilized as they otherwise could be. That's why Hyper Threading exists on these cores... that unused capacity is utilized for a second stream of instructions and then more hardware is required to keep track of virtual vs physical cores, etc. Apple's cores don't have Hyper Threading because they're better utilized and the overhead suddenly isn't worth it.\n\nThis is also why Apple invests so much logic in branch prediction. Because the cores can be fed 9 instructions at a time, the risk of having to flush the pipeline because of a bad prediction increases, so they work harder than Intel/AMD do to avoid this.\n\nThis CISC vs RISC issue DOES matter... even if AMD made a unified memory SoC on a 3nm process, it would still draw more power than an M3 series process of the same performance because it would require a higher clock speed (rather than wider decode/cores) to hit that performance. It's a fundamental limitation of the legacy Intel/AMD instruction sets. Apple also goes further in hacking out anything that isn't needed. Intel/AMD CPUs can still execute 16 and 32 bit code and Apple dropped the ability to even execute 32 code from their processors. Logic required to manage those different instruction sets is deleted and more space is available for what's required.\n\nHope this helps.", "Score": 1, "Replies": []}, {"Comment": "They do not. \n\nThere is no inherent difference in power consumption due to the underlying instruction set. \n\nThe difference comes down to the manufacturing process, design goals, other system components (screen, WiFi chip, ram, storage, battery size, etc), and software optimizations.", "Score": 1, "Replies": []}, {"Comment": "In RISC ISA you have normally less clock per instruction in comparison with CISC so you achive the same goal with less switching mosfets consuming less power\n\nHere some video about performance and power dissipation\n\n&#x200B;\n\nperformance [https://youtu.be/tpA2WhGiqLI](https://youtu.be/tpA2WhGiqLI) \n\npower dissipation [https://youtu.be/iT-E0kSBxYE](https://youtu.be/iT-E0kSBxYE)", "Score": 1, "Replies": []}, {"Comment": "Which I'm frankly surprised at given how cheap electricity is and how with a laptop you have to carry around chargers anyway if you want to guarantee usage.", "Score": 13, "Replies": [{"Reply": "Server farms, 1 cent saved can add up to millions. With how many data centers there are it makes a great deal of sense.", "Reply Score": 41}, {"Reply": "1. To a significant degree energy efficiency means performance as these devices are thermally limited.(\\*\\*1)\n2. Energy efficiency affects user experience as it determines the amount of noise.\n3. ... as well as the device dimensions and weight.\n4. The sentiment that battery life does not matter to users could not be any further from the truth.\n5. On the upper end of SKUs energy efficiency to performance translation occurs due to power delivery constraints as well.\n\n(\\*\\*1) AFAICT, M3 Max for instance has 12P cores not because of area or cost (CPU cores take \\~single digit % of the total chip area), but purely due to thermal constraints - those 12P cores thermal throttle in 16\" MBP's thermal package in multi-threaded workloads. It'd be pointless to add any more P cores to that SoC for that specific reason. In comparison, a many, many times larger (by area) GPU of the same SoCs doesn't even saturate the thermal limits of that package, despite being vastly more performant than the CPU in appropriate workloads.\n\nIt's hard to overstate the importance of energy efficiency in modern silicon engineering.", "Reply Score": 19}, {"Reply": "Electricity in Europe and California is anything but cheap.  Paying over 30c / kWh off peak.", "Reply Score": 1}]}, {"Comment": "Thats the answer to the question. It is also not true. Intel had as much incentive to reduce power consumption as the rest of the competition. The competition was simply faster and better when it comes to mobile computing.\n\nIntel has also been putting their power saving improvements among the main marketing featueres in their CPU lines.", "Score": 3, "Replies": []}, {"Comment": "Speak for yourself, electricity is fucking expensive here (Germany)", "Score": 8, "Replies": [{"Reply": "I\u2019m curious, what drives the cost of it higher?", "Reply Score": 1}]}, {"Comment": "Who would not wish thin, light, silent and cool laptop?\nI guess Intel should have focused on power efficiency long time back. May be they attempted with atom series of processors but failed due to their legacy of x86 platform.", "Score": 1, "Replies": []}, {"Comment": ">from its* inception", "Score": 1, "Replies": [{"Reply": "Do'H", "Reply Score": 1}]}, {"Comment": "Best advice given how wrong some answers are.", "Score": 20, "Replies": [{"Reply": "Yep, like the \"it's mostly down to the manufacturing process\" substantially upvoted [claim](https://www.reddit.com/r/compsci/comments/18cup5v/comment/kcdjd2i/?utm_source=reddit&utm_medium=web2x&context=3) which is grossly inaccurate, I'm just tired chasing down every exaggeration here attributing most of the difference to that one thing someone thought of. \n\nIf someone is willing to do the actual legwork a little and verify their claims at least somewhat, here's a script for this one in particular:\n\n1. it's very difficult to find any reliable information on CPU power (measured) that is done holistically at the right scope with a proper methodology that is anywhere near transferrable from one product to another (there is core power, core+LLC power, dram controllers, interconnect, thus whole package power, E-cores, P-cores, different workloads and so on).\n2. what I can suggest to get at least something sane in terms of reliability is looking up the recent [presentation](https://youtu.be/K7Q5iYHvgwo?t=130) from Qualcomm on Snapdragon X Elite SoC. You'll find single-threaded data on energy at *iso-performance* between Elite and M2, and Elite and 13th gen (IIRC) Intel Laptop CPU, which should let you compare M2 with that Intel at iso-perf. You should get something like \"70% less power\" (0.3) against \"30% less power\" (0.7), making M2 P-cores (1 / 0.3) / (1 / 0.7) = 0.7 / 0.3 = 2.3x lower power at iso-performance than Intel Laptop P-cores.\n3. Then you should be able to look up elsewhere that intel's 13th gen and recent AMD laptop aren't actually that far off from one another.\n4. Then that 5nm -> 3nm TSMC node only improved somewhere along the lines of \"power consumption by 25\u201330% at the same speed, increase speed by 10\u201315% at the same amount of power\" ([link](https://en.wikipedia.org/wiki/3_nm_process)) which is actually super-optimistic and almost never realized in full with real world designs, so that benefit is distributed between efficiency and speed and subdued due to design constraints, so, really Apple got maybe like 10% of \"free efficiency\" out of 3nm over 5nm (which is what AMD is on).\n5. Let's bump it back to 30% to account for AMD doing slightly better in laptop than Intel, I just don't remember how much that is exactly, please look and check.\n6. At which point, M2's P-core is 2.3 / 1.3 = 1.77x times lower power at iso-performance due to design differences (arch, u-arch, logic design, physical design, power management, boot algorithms and operating mode chosen, etc) and has nothing to do with diff. in process nodes.\n7. And then M2 has E-cores which are completely nuts, they are about 40% of P-cores performance, but like >10x less power, yielding >4x perf/w than M2's P-cores (or >7x Intel's P-cores), where's in Intel's E-cores (so called) are actually area efficient cores (perf/mm\\^2), not energy efficient cores at all - they have very similar perf/w to their own P-cores.\n8. You can also find data on perf & power of Qualcomm cores, also mobile, also TSMCs, find a pair of CPUs on exactly the same process node, and dig up perf/w data, and you'll see that Apple's E-cores (in particular) are like 2-3x perf/w than Qualcomm's E-cores (the most E, they have 3 tiers).\n\nu/small_kimono & u/OstrichWestern639", "Reply Score": 15}]}, {"Comment": "Sure", "Score": 1, "Replies": []}, {"Comment": "Thanks. Where can I find resources on TSMC? Never heard of this term before", "Score": 1, "Replies": [{"Reply": "Oh if you haven't heard of TSMC before, you're in for a treat. They are possibly the most important manufacturing company in the world. You can find lots of resources on YouTube etc. but I think this video does a good job of highlighting how important they are to the world economy\n\n https://youtu.be/_TOCRjF9WuE?si=4aO_CBb2fARAJIgg", "Reply Score": 9}, {"Reply": "TSMC (Taiwan Semiconductor) is one of the largest semiconductor manufacturers in the world. They have arguably the highest performance/efficiency process nodes in the industry. They produce the chips for AMD, Apple and Nvidia.", "Reply Score": 12}, {"Reply": "TSMC is basically what's stopping China from invading Taiwan.", "Reply Score": 2}, {"Reply": "https://www.youtube.com/playlist?list=PLKtxx9TnH76SRC7ZbOu2Nsg5mC72fy-GZ", "Reply Score": 1}]}, {"Comment": "Chip architect here and definitely a person. I have worked across the x86, ARM, and RISC V space. There is quite a bit of cross pollination regarding architects and ISAs. Not all of us, but many of us do move around. There are challenges moving from ISA to ISA but the core concepts of extracting performance and power efficiency are the same.\n\nRegarding why Apple enjoys a perf/watt advantage is multifactorial but I can shed light on some of the key points. \n\n1) Apple is not nearly as restricted in hitting a price point for its chips vs other players. Traditional silicon design houses need to hit a price point because they need to sell their chips to OEM and have a margin of profit.\n\n2) Building off the first point, Apple can take the yield hit and throw silicon at the problem. Their philosoply is to achieve performance by being wide but relatively slow in terms of frequency. This is opposed to being narrow and faster. What I mean in terms of narrow/wide is the amount of instruction parallelism the silicon can handle. It's important to note that this isn't so much a function of ISA as it is a design choice. Apple uses width to achieve performance whereas the traditional players lean more heavily on narrower structures and higher frequency to achieve performance.  \n\n3) Being wide and slow benefits power efficiency because of the way power scales with frequency and voltage. Being \"slow\" allows the silicon to operate at a lower voltage. Dynamic power scales cubically with frequency (effective capacitance \\* frequency \\* voltage\\^2) and leakage power scales exponentially with voltage so you can a huge efficiency boost by being wide at the cost of extra silicon. \n\n4) Apple can extract extra efficiency because they are vertically integrated in terms of OS and hardware. Their scheduler and power management policies can be tightly coupled with their hardware. Other silicon vendors / OS combinations are limited because silicon design houses don't control the OS and vice versa.", "Score": 5, "Replies": [{"Reply": "This is the best answer. \n\nTo add one more, it is a LOT of work to change your microarchitecture from narrow and fast to wide and slow.  In order to take advantage of the extra levels of logic when operating at slow frequencies, you basically need to repipeline your whole design.", "Reply Score": 2}, {"Reply": "Shouldn't there be a practical limit to how wide a decoder (and accompanying and/or dependent logic, from instruction prefetchers through branch predictors and more) can realistically be before becoming big and/or energy expensive enough to significantly impact the size and/or energy efficiency of the entire core when the encodings are variable length, such as x86-64?\n\nI'd naively imagine a parallel decoder for variable length would need contain circuitry that decodes at every byte offset, selecting in the end which byte offsets ended up being actually valid. All the while dealing with complex encodings as well. A problem AArch64 CPUs would not have.\n\nThen there is a \"popular\" argument about A) supporting outdated portions of the ISA, and B) having to have sophisticated u-op engines and performing a non-trivial ISA to u-op translation, with parts of the processor effectively duplicated, between multiple branch predictors, instruction (or u-op) caches and buffers, etc. How much area and energy this all is, %-wise at the scope of the entire CPU? How much more expensive this detail makes branch mispredictions?\n\nOne particular sub-example related to the above (but most definitely not defining it) is FLAGS updates on every arithmetic operation in x86-64. They should create reordering constraints either limiting OOO parallelism, or requiring more switches / circuitry to be tracked and dealt with, right?\n\nA similar example would be with 16-bit subregister updates retaining the values of the rest of the register in x86. While rarely emitted by modern compilers, those updates still have to be supported by the HW.\n\nHow would you assess the impact of the following idea: by the first order of approximation, modern optimizing compilers mostly focus on minimizing dynamic instruction counts that will be executed. With AArch64's ISA there is presumably very good correspondence between ISA instruction counts and internal u-op counts to execute, thus resulting in the compiler policy automatically minimizing u-op counts as well, playing into reducing the total amount of work to do on the rest of the core and amount of switching / energy to do so. With x64 ISA ISA instruction counts and u-op counts are substantially decoupled, thus compiler efforts are less effective at minimizing u-op counts, resulting in more work and switches / energy on the rest of the core: ultimately, trying to simultaneously satisfy 2 systems of constraints (code size and u-op count) should inevitably lead to suboptimal results on both. Not to mention there is human expertise effect: knowing u-op counts is very difficult for compiler engineers, inevitably their designs focus on externally observable properties of code at ISA level.?\n\nAlso, Apple's P-cores appear to have similar area to those of say AMD's Zen 4. In fact, [Zen 4 is much bigger by area](https://www.reddit.com/r/hardware/comments/17q6zbq/comment/k8bgjdf/?utm_source=reddit&utm_medium=web2x&context=3), it takes giving up a very large portion of frequency and performance to make a more compact Zen 4c design work, which in its own turn has a fairly similar area (on the same node) as Apple's P-cores. Granted, for the latter statement I'm not 100% sure the area data for Apple's P-core includes their L2 (I'm positive both data points exclude LLC, but Apple's L2 is where most of the cache capacity is at, it's big, unlike AMD's L2). How does this observation match up with the idea that Apple's design \"throws silicon at the problem\"?\n\nAs for cross-pollination, I have recently observed that both the leadership and the bulk of the rest of the silicon engineering at Apple seem to have very little overlap with anybody who worked at Intel, and only very marginally bigger overlap with AMD: [small analysis constrained](https://www.reddit.com/r/hardware/comments/17td3tt/comment/k90eapa/?utm_source=reddit&utm_medium=web2x&context=3) by what's available in public. Any comment there?\n\nPart of the reason of my statements in the parent comment at the top level is the observation that comparing ISA implications this deep into the design and yet at the scope of the entire system (including compilers) is most likely never really anybody's actual task or job: CPUs aren't GPUs, x86 CPU designers don't get to design or redesign x86 ISA, it's largely fixed, and to a very large extent (only marginally less so) the same is the case for AArch64 CPU designers. With limited time spent on designing ISAs themselves, least alone purposefully and with a substantial expenditure of time and resources researching comparatively the difference between x86-64 and AArch64 ISA and their impact on the resulting energy efficiency of the entire system, it's hard to imagine a randomly selected architect to know answers to the question in a complete, properly weighted per factor, and intimately true to reality fashion. You think this is not quite accurate?\n\nThanks!", "Reply Score": 1}]}, {"Comment": "This helped a lot. Ty", "Score": 2, "Replies": []}, {"Comment": "> ARM has always been extremely energy conscious, on the original game boy the ARM architecture\n\nThe CPU on the original game boy was a Zilog Z80. The Z80 was ISA compatible with the Intel 8080. Nothing to do with ARM.", "Score": 1, "Replies": [{"Reply": "woops! thanks for the error check, I corrected it ;)", "Reply Score": 1}]}, {"Comment": "I think the \"culture\" of Win32 could be another issue. At Restaurant we run Ubuntu 22 LTS and we need to use Windows 10 just having a printer driver for a .NET application. The OS and Applications happily use both i5 cores for almost 20 minutes until the system become usable. Having the same printer I can compare it to MacOS and a pretty conservative Linux such as Ubuntu LTS. The crazy \"checking updates\" drama doesn't happen on both. I am saying even if there was a AI that will convert all that  x86-64 setup exactly to M2 code an OS which profiles the entire disk will eat the battery too. Just updating a single exe will trigger \"compatibility telemetry\". Imagine suggesting this idea to a Linux distribution or Apple. Another example? Steam. You need to add 32bit architecture to Linux to be able to run Steam. That means having an entire 32bit userland for a single application.", "Score": 1, "Replies": []}, {"Comment": "Ahh now it makes sense!", "Score": 1, "Replies": []}, {"Comment": "The last generation of Intel Macbooks had battery life much shorter than the new ARM models as well, so even in a more direct comparison the ARM chips seem much more efficient.", "Score": 12, "Replies": []}, {"Comment": "Either way, other laptops running Intel and AMD processors run for about 7-8 hours before they give up", "Score": 2, "Replies": [{"Reply": "[deleted]", "Reply Score": -18}]}, {"Comment": "RISC was killed because SPARK and MIPS did not want to give up their margin. Also no investment in fabs. Someone pocketed all the money and the companies went bankrupt.", "Score": 5, "Replies": [{"Reply": "ARM chips are RISC chips, and very much alive.  \n\n\nFor RISC generally, see https://en.wikipedia.org/wiki/Reduced\\_instruction\\_set\\_computer", "Reply Score": 1}]}, {"Comment": "This is incorrect. CISC vs RISC is completely irrelevant in reality.", "Score": 9, "Replies": [{"Reply": "Lmao you're wrong. I literally design micros lmao. I explained why it's absolutely relevant. There is a lot of overhead and data shuffling which wastes power. If I were you I wouldn't comment on things you have no technical understanding of.", "Reply Score": -4}]}, {"Comment": "Love the downvotes from people that don't know shit", "Score": 0, "Replies": []}, {"Comment": "Why would CISC chips translate CISC to RISC to execute? Isn't the whole point of CISC chips that they can run CISC commands natively? Or are you just saying that it is harder to decode CISC commands to actually execute them?\n\nAt least that is my memory of my computer architecture course. I see in your other comments that you work in the industry so I could definitely be wrong.", "Score": 1, "Replies": [{"Reply": "Complex Instruction Set Computers (CISC) rarely execute their complex machine instructions In a single clock cycle. Rather, CISC instructions are usually unpacked by hardware into many (often dozens) micro-operations, which are then executed sequentially. So a single CISC instruction might take dozens of clock cycles to execute.\n\nReduced Instruction Set Computers (RISC) use far simpler machine instructions, but strive to execute each one in far fewer (ideally one!) micro-operations that execute in a correspondingly small number of clock cycles. Ideally, a single RISC instruction would execute in a single clock cycle.\n\nIf chip designers could make a CISC processor that executed its machine instructions in a single clock cycle then CISC chips would win in terms of speed. \n\nIn practice, almost all (?) CPUs are RISC chips at the micro-instruction level, with a hardware macro facility that expands single, seemingly simple CISC instructions into clumps of RISC instructions.", "Reply Score": 0}]}, {"Comment": ">dumb games and viruses\n\nYou seem biased.", "Score": 7, "Replies": [{"Reply": "oh my, I was hoping you wouldn't notice :D", "Reply Score": -9}]}, {"Comment": "I see. Maybe it is because my macbook is old. \n\nIve always been told that x86 has a complex ISA so the compilers are simple.\n\nArm has a simple instruction set hence the complex compilers.\n\n\nI never understood why they are that way!", "Score": 0, "Replies": [{"Reply": "This was the case 30 years ago. Nowadays all processors that are powerful enough to run a laptop are insanely complicated. Intel processors are RISC \"under the hood\" and the differences instruction sets make in power efficiency are going to only make up one tiny corner of power efficiency.\n\n> Maybe it is because my macbook is old.\n\nNo, it's because of a number of different factors.", "Reply Score": 0}]}, {"Comment": "The M3 is likely to have an order of magnitude more transistors.", "Score": 7, "Replies": [{"Reply": "And a lot not in the core.", "Reply Score": 2}, {"Reply": "No, you can\u2019t count the GPUs! Integer pipeline comparisons only. ARM cores are smaller than x86 in general.", "Reply Score": 1}]}, {"Comment": "Server farms, 1 cent saved can add up to millions. With how many data centers there are it makes a great deal of sense.", "Score": 41, "Replies": [{"Reply": "Yeah but there aren't server farms filled with macbooks. I do agree that servers are a good case and god bless graviton instances.", "Reply Score": 3}, {"Reply": "If you own computers in a datacenter you are not paying for the power.\n\nIs the crucial thing. You're paying for rack units but electricity is , for all intents & purposes, free.\n\nIt literally does not matter how power efficient your server is , if you're renting space from Layer1 etc you want the most performance per RU you can get. Electricity is not a concern.\n\nElectricity is only a cost factor if you own the datacenter, and if you're not Google or Amazon, you don't.", "Reply Score": 2}]}, {"Comment": "1. To a significant degree energy efficiency means performance as these devices are thermally limited.(\\*\\*1)\n2. Energy efficiency affects user experience as it determines the amount of noise.\n3. ... as well as the device dimensions and weight.\n4. The sentiment that battery life does not matter to users could not be any further from the truth.\n5. On the upper end of SKUs energy efficiency to performance translation occurs due to power delivery constraints as well.\n\n(\\*\\*1) AFAICT, M3 Max for instance has 12P cores not because of area or cost (CPU cores take \\~single digit % of the total chip area), but purely due to thermal constraints - those 12P cores thermal throttle in 16\" MBP's thermal package in multi-threaded workloads. It'd be pointless to add any more P cores to that SoC for that specific reason. In comparison, a many, many times larger (by area) GPU of the same SoCs doesn't even saturate the thermal limits of that package, despite being vastly more performant than the CPU in appropriate workloads.\n\nIt's hard to overstate the importance of energy efficiency in modern silicon engineering.", "Score": 19, "Replies": []}, {"Comment": "Electricity in Europe and California is anything but cheap.  Paying over 30c / kWh off peak.", "Score": 1, "Replies": [{"Reply": "So about 3c an hour while it's used. Hold off on buying a coffee or something just once and pay for 100 hours.", "Reply Score": -1}]}, {"Comment": "I\u2019m curious, what drives the cost of it higher?", "Score": 1, "Replies": [{"Reply": "My understanding is that Germany had a big movement to stop using nuclear power for a while, so they tended to use a lot of fossil fuels. Then when Russia invaded Ukraine the cost of fossil fuels went up if you didn\u2019t want to buy from Russia. So as a result cost of electricity generated by fossil fuels went up.", "Reply Score": 21}, {"Reply": "idiotic politics.", "Reply Score": 0}, {"Reply": "Combination of gas prices and taxes", "Reply Score": 1}]}, {"Comment": "Do'H", "Score": 1, "Replies": []}, {"Comment": "Yep, like the \"it's mostly down to the manufacturing process\" substantially upvoted [claim](https://www.reddit.com/r/compsci/comments/18cup5v/comment/kcdjd2i/?utm_source=reddit&utm_medium=web2x&context=3) which is grossly inaccurate, I'm just tired chasing down every exaggeration here attributing most of the difference to that one thing someone thought of. \n\nIf someone is willing to do the actual legwork a little and verify their claims at least somewhat, here's a script for this one in particular:\n\n1. it's very difficult to find any reliable information on CPU power (measured) that is done holistically at the right scope with a proper methodology that is anywhere near transferrable from one product to another (there is core power, core+LLC power, dram controllers, interconnect, thus whole package power, E-cores, P-cores, different workloads and so on).\n2. what I can suggest to get at least something sane in terms of reliability is looking up the recent [presentation](https://youtu.be/K7Q5iYHvgwo?t=130) from Qualcomm on Snapdragon X Elite SoC. You'll find single-threaded data on energy at *iso-performance* between Elite and M2, and Elite and 13th gen (IIRC) Intel Laptop CPU, which should let you compare M2 with that Intel at iso-perf. You should get something like \"70% less power\" (0.3) against \"30% less power\" (0.7), making M2 P-cores (1 / 0.3) / (1 / 0.7) = 0.7 / 0.3 = 2.3x lower power at iso-performance than Intel Laptop P-cores.\n3. Then you should be able to look up elsewhere that intel's 13th gen and recent AMD laptop aren't actually that far off from one another.\n4. Then that 5nm -> 3nm TSMC node only improved somewhere along the lines of \"power consumption by 25\u201330% at the same speed, increase speed by 10\u201315% at the same amount of power\" ([link](https://en.wikipedia.org/wiki/3_nm_process)) which is actually super-optimistic and almost never realized in full with real world designs, so that benefit is distributed between efficiency and speed and subdued due to design constraints, so, really Apple got maybe like 10% of \"free efficiency\" out of 3nm over 5nm (which is what AMD is on).\n5. Let's bump it back to 30% to account for AMD doing slightly better in laptop than Intel, I just don't remember how much that is exactly, please look and check.\n6. At which point, M2's P-core is 2.3 / 1.3 = 1.77x times lower power at iso-performance due to design differences (arch, u-arch, logic design, physical design, power management, boot algorithms and operating mode chosen, etc) and has nothing to do with diff. in process nodes.\n7. And then M2 has E-cores which are completely nuts, they are about 40% of P-cores performance, but like >10x less power, yielding >4x perf/w than M2's P-cores (or >7x Intel's P-cores), where's in Intel's E-cores (so called) are actually area efficient cores (perf/mm\\^2), not energy efficient cores at all - they have very similar perf/w to their own P-cores.\n8. You can also find data on perf & power of Qualcomm cores, also mobile, also TSMCs, find a pair of CPUs on exactly the same process node, and dig up perf/w data, and you'll see that Apple's E-cores (in particular) are like 2-3x perf/w than Qualcomm's E-cores (the most E, they have 3 tiers).\n\nu/small_kimono & u/OstrichWestern639", "Score": 15, "Replies": [{"Reply": ">Yep, like the \"it's mostly down to the manufacturing process\" substantially upvoted claim which is grossly inaccurate, I'm just tired chasing down every exaggeration here attributing most of the difference to that one thing someone thought of.\n\nYou're right, it's extraordinarily complex, given two leading edge designs, it appears *process does have a lot to do with it*.  But did I say it had everything to do with process?  No.\n\nI also said:\n\n>  ARM also has a long history of focusing on power consumption, because it had to, as they are embedded in all sorts of smallish devices. Apple also focused on power consumption, because your M2 is essentially a mobile chip.\n\nSo, yes, you're right perhaps I should have mentioned Apple's E-cores, because they had plenty to do with it, in particular, as well.  \n\nAnd remember what the OP is comparing:\n\n> Context: my Intel Core i7 MacBook Pro runs out of battery within 6 hours.  My M2 Pro Macbook runs for over 48 hours.\n\nOP was not comparing 5nm and 3nm TSMC, but his/her 5 year old computer to his/her new Macbook.  Process has a ton to do with global performance/power of the M line, especially given Intel was stuck on a process node, but design (E-cores!) and probably ISA play a huge part as well, especially re: power consumption, and I didn't mean to suggest otherwise.\n\nIf the Q is: Is there something fundamental re: x86 which makes this impossible to achieve, I think the answer is no.  See my discussion of new AMD mobile chips.", "Reply Score": 4}]}, {"Comment": "Oh if you haven't heard of TSMC before, you're in for a treat. They are possibly the most important manufacturing company in the world. You can find lots of resources on YouTube etc. but I think this video does a good job of highlighting how important they are to the world economy\n\n https://youtu.be/_TOCRjF9WuE?si=4aO_CBb2fARAJIgg", "Score": 9, "Replies": []}, {"Comment": "TSMC (Taiwan Semiconductor) is one of the largest semiconductor manufacturers in the world. They have arguably the highest performance/efficiency process nodes in the industry. They produce the chips for AMD, Apple and Nvidia.", "Score": 12, "Replies": [{"Reply": "And even some Intel dies starting with Meteor Lake.", "Reply Score": 1}]}, {"Comment": "TSMC is basically what's stopping China from invading Taiwan.", "Score": 2, "Replies": []}, {"Comment": "https://www.youtube.com/playlist?list=PLKtxx9TnH76SRC7ZbOu2Nsg5mC72fy-GZ", "Score": 1, "Replies": []}, {"Comment": "This is the best answer. \n\nTo add one more, it is a LOT of work to change your microarchitecture from narrow and fast to wide and slow.  In order to take advantage of the extra levels of logic when operating at slow frequencies, you basically need to repipeline your whole design.", "Score": 2, "Replies": []}, {"Comment": "Shouldn't there be a practical limit to how wide a decoder (and accompanying and/or dependent logic, from instruction prefetchers through branch predictors and more) can realistically be before becoming big and/or energy expensive enough to significantly impact the size and/or energy efficiency of the entire core when the encodings are variable length, such as x86-64?\n\nI'd naively imagine a parallel decoder for variable length would need contain circuitry that decodes at every byte offset, selecting in the end which byte offsets ended up being actually valid. All the while dealing with complex encodings as well. A problem AArch64 CPUs would not have.\n\nThen there is a \"popular\" argument about A) supporting outdated portions of the ISA, and B) having to have sophisticated u-op engines and performing a non-trivial ISA to u-op translation, with parts of the processor effectively duplicated, between multiple branch predictors, instruction (or u-op) caches and buffers, etc. How much area and energy this all is, %-wise at the scope of the entire CPU? How much more expensive this detail makes branch mispredictions?\n\nOne particular sub-example related to the above (but most definitely not defining it) is FLAGS updates on every arithmetic operation in x86-64. They should create reordering constraints either limiting OOO parallelism, or requiring more switches / circuitry to be tracked and dealt with, right?\n\nA similar example would be with 16-bit subregister updates retaining the values of the rest of the register in x86. While rarely emitted by modern compilers, those updates still have to be supported by the HW.\n\nHow would you assess the impact of the following idea: by the first order of approximation, modern optimizing compilers mostly focus on minimizing dynamic instruction counts that will be executed. With AArch64's ISA there is presumably very good correspondence between ISA instruction counts and internal u-op counts to execute, thus resulting in the compiler policy automatically minimizing u-op counts as well, playing into reducing the total amount of work to do on the rest of the core and amount of switching / energy to do so. With x64 ISA ISA instruction counts and u-op counts are substantially decoupled, thus compiler efforts are less effective at minimizing u-op counts, resulting in more work and switches / energy on the rest of the core: ultimately, trying to simultaneously satisfy 2 systems of constraints (code size and u-op count) should inevitably lead to suboptimal results on both. Not to mention there is human expertise effect: knowing u-op counts is very difficult for compiler engineers, inevitably their designs focus on externally observable properties of code at ISA level.?\n\nAlso, Apple's P-cores appear to have similar area to those of say AMD's Zen 4. In fact, [Zen 4 is much bigger by area](https://www.reddit.com/r/hardware/comments/17q6zbq/comment/k8bgjdf/?utm_source=reddit&utm_medium=web2x&context=3), it takes giving up a very large portion of frequency and performance to make a more compact Zen 4c design work, which in its own turn has a fairly similar area (on the same node) as Apple's P-cores. Granted, for the latter statement I'm not 100% sure the area data for Apple's P-core includes their L2 (I'm positive both data points exclude LLC, but Apple's L2 is where most of the cache capacity is at, it's big, unlike AMD's L2). How does this observation match up with the idea that Apple's design \"throws silicon at the problem\"?\n\nAs for cross-pollination, I have recently observed that both the leadership and the bulk of the rest of the silicon engineering at Apple seem to have very little overlap with anybody who worked at Intel, and only very marginally bigger overlap with AMD: [small analysis constrained](https://www.reddit.com/r/hardware/comments/17td3tt/comment/k90eapa/?utm_source=reddit&utm_medium=web2x&context=3) by what's available in public. Any comment there?\n\nPart of the reason of my statements in the parent comment at the top level is the observation that comparing ISA implications this deep into the design and yet at the scope of the entire system (including compilers) is most likely never really anybody's actual task or job: CPUs aren't GPUs, x86 CPU designers don't get to design or redesign x86 ISA, it's largely fixed, and to a very large extent (only marginally less so) the same is the case for AArch64 CPU designers. With limited time spent on designing ISAs themselves, least alone purposefully and with a substantial expenditure of time and resources researching comparatively the difference between x86-64 and AArch64 ISA and their impact on the resulting energy efficiency of the entire system, it's hard to imagine a randomly selected architect to know answers to the question in a complete, properly weighted per factor, and intimately true to reality fashion. You think this is not quite accurate?\n\nThanks!", "Score": 1, "Replies": [{"Reply": "See also [this rather excellent argument](https://www.reddit.com/r/hardware/comments/18cutwj/comment/kcdlsty/?utm_source=reddit&utm_medium=web2x&context=3) (not mine), which extends some of the above on E-cores where the decoder (and dependent logic) becomes in relative figures a bigger portion of the entire core (as the rest of the core is much smaller), both in area and energy.", "Reply Score": 1}]}, {"Comment": "woops! thanks for the error check, I corrected it ;)", "Score": 1, "Replies": []}, {"Comment": "[deleted]", "Score": -18, "Replies": [{"Reply": "The question wasn't about what the difference in power consumption is, but why that difference exists. Your reiteration of the question betrays the point. A more accurate form of your reiteration would be \"what about someone's car engine results in them getting home later compared to a different engine?\" The fact of the difference is already established, the details of it are what the question is asking for.", "Reply Score": 2}]}, {"Comment": "ARM chips are RISC chips, and very much alive.  \n\n\nFor RISC generally, see https://en.wikipedia.org/wiki/Reduced\\_instruction\\_set\\_computer", "Score": 1, "Replies": [{"Reply": "PowerPC is also alive because IBM can charge so much for their mainframes.\n\nARM is not greedy, was in the shadows. Like all those RISC microcontrollers. Then Apple refurbished Newton into iPhone .", "Reply Score": 1}]}, {"Comment": "Lmao you're wrong. I literally design micros lmao. I explained why it's absolutely relevant. There is a lot of overhead and data shuffling which wastes power. If I were you I wouldn't comment on things you have no technical understanding of.", "Score": -4, "Replies": [{"Reply": "Intuitively (as I do not design micros) I'd say it's absolutely undeniable that that translation (and IP duplication, like for instance having a secondary set of branch predictors in the CISC \"parser\" part in addition to another one in the RISC \"core\") costs energy, and so does the (buffered) transfer of the uOp stream from one to the other. The real question is, how much *at the scale of the entire SoC*.\n\nYou design micros for what, the entire system or one specific IP block?\n\nPlease forgive me my (hopefully healthy) skepticism towards \"*a lot* \\[of power\\]\" qualifier you have provided, but I've seen too many times block designers having rather no clue about exact contribution of the metrics of their block to the aggregate metrics of the entire final product / system / SoC.\n\nThe second major question is, how that \"how much\" compares to \"how much\" of various other factors, some of which are fairly obvious, like the [currently top-rated](https://www.reddit.com/r/compsci/comments/18cup5v/comment/kcd2ykv/?utm_source=reddit&utm_medium=web2x&context=3) \"real world x86 CPU designers we happen to have historically didn't need to focus on energy as much as real world ARM CPU designers we happen to have\", others -  a lot more contrived and most likely exceedingly difficult to asses even for working professionals within the industry, such as subtle differences in the memory model and even worse, non-trivial interactions with induced differences in code generation on the compiler side, where \"properties of code generation\" is a topic that most (if not all, sadly) silicon designers I have ever met in my life have pretty much no clue about, understandably so, and where the \"interactions with\" part, I suspect, most likely is a mystery for pretty much everyone, especially in a AArch64 vs x64 comparative manner.", "Reply Score": 10}, {"Reply": "uOpts are used in modern RISC CPUs (Such as ARM).  Granted, your overall point stands that x86, particularly with the mass of complex instructions/flags/etc, adds a lot to the heat budget.  There's also a big issue with variable width instructions in x86.  Even though ARM now has variable width instructions, they are still aligned and easy to decode.", "Reply Score": 1}]}, {"Comment": "Complex Instruction Set Computers (CISC) rarely execute their complex machine instructions In a single clock cycle. Rather, CISC instructions are usually unpacked by hardware into many (often dozens) micro-operations, which are then executed sequentially. So a single CISC instruction might take dozens of clock cycles to execute.\n\nReduced Instruction Set Computers (RISC) use far simpler machine instructions, but strive to execute each one in far fewer (ideally one!) micro-operations that execute in a correspondingly small number of clock cycles. Ideally, a single RISC instruction would execute in a single clock cycle.\n\nIf chip designers could make a CISC processor that executed its machine instructions in a single clock cycle then CISC chips would win in terms of speed. \n\nIn practice, almost all (?) CPUs are RISC chips at the micro-instruction level, with a hardware macro facility that expands single, seemingly simple CISC instructions into clumps of RISC instructions.", "Score": 0, "Replies": [{"Reply": "Ah I see what you\u2019re saying. You can\u2019t run individual RISC instructions from a CISC instruction pipeline though, right? \n\nI think your comment is just a bit misleading. Just because an addition operation is a stage in a CISC instruction pipeline doesn\u2019t mean it\u2019s using RISC under the hood. I could also call the mod operator subtraction because I can replicate its behavior with multiple subtractions rather than a single mod operation (bad example, I know in hardware mod is a single step but I can\u2019t think of a good example).\n\nI also wouldn\u2019t describe it as translating from CISC to RISC. Every time the translation is identical since it\u2019s hardware. And some things just naturally require multiple stages, like encryption. CISC is great for handling encryption natively. If I am remembering correctly encryption with CISC requires fewer memory reads than RISC since it is far fewer instructions. So for CISC you just use the encryption operation(s), but with CISC you need to run each step in encryption, wait for it to complete, then read that result to use in the next step. And it is true that it is possible to do that in 1 clock cycle, but the frequency of that clock will be significantly lower. \n\nAnd yes, I know the academic goal is single clock instructions. But 1 clock instructions aren\u2019t always good in the real world. Sometimes it is better to split an instruction over multiple clocks to implement pipelining or multistage concepts. Those allow instructions to take variable amounts of time, which is very useful.\n\nFor example in a multistage processor (1 clock per stage, I think you refer to stages as micro ops) memory reads take the longest (5 stages) but memory writes only take 4 stages. If we wanted 1 instruction per clock we would have to waste 20% of our time when reading waiting for the clock to finish. We would end up wasting 40% of our time for the BEQ instruction, since it is only 3 stages. (Numbers from the textbook on RISC processor architecture we used at school).", "Reply Score": 1}]}, {"Comment": "oh my, I was hoping you wouldn't notice :D", "Score": -9, "Replies": []}, {"Comment": "This was the case 30 years ago. Nowadays all processors that are powerful enough to run a laptop are insanely complicated. Intel processors are RISC \"under the hood\" and the differences instruction sets make in power efficiency are going to only make up one tiny corner of power efficiency.\n\n> Maybe it is because my macbook is old.\n\nNo, it's because of a number of different factors.", "Score": 0, "Replies": [{"Reply": "It makes up a tiny part of the silicon. Still, for a cache miss Intel needs to do more decoding, while ARM can process the raw instruction stream until the cache line fills up.\n\nAlso ARM can be more optimistic about multithreading ( more like MIPS ), while x86 is consistent all the time.", "Reply Score": 3}]}, {"Comment": "And a lot not in the core.", "Score": 2, "Replies": [{"Reply": "The Apple cores have absolutely massive reorder buffers and other OoO hardware that accounts for most of the transistors of modern cores.", "Reply Score": 1}]}, {"Comment": "No, you can\u2019t count the GPUs! Integer pipeline comparisons only. ARM cores are smaller than x86 in general.", "Score": 1, "Replies": []}, {"Comment": "Yeah but there aren't server farms filled with macbooks. I do agree that servers are a good case and god bless graviton instances.", "Score": 3, "Replies": [{"Reply": "ARM servers are getting increasingly popular, because they have great price/performance. X86 servers consuming power to run servers, generating heat and then cooling them, is very expensive!", "Reply Score": 19}, {"Reply": "There are Macs in data centers. There are also lots of x86 chips in those data centers.", "Reply Score": 6}]}, {"Comment": "If you own computers in a datacenter you are not paying for the power.\n\nIs the crucial thing. You're paying for rack units but electricity is , for all intents & purposes, free.\n\nIt literally does not matter how power efficient your server is , if you're renting space from Layer1 etc you want the most performance per RU you can get. Electricity is not a concern.\n\nElectricity is only a cost factor if you own the datacenter, and if you're not Google or Amazon, you don't.", "Score": 2, "Replies": [{"Reply": "That should depend on your contracts, but that aside very large number of companies are renting server time and cloud infrastructure now  though, so companies like Amazon and Microsoft who offer those products and do host their own DCs substitute a lot of those SMEs in the market for chips.\n\nAs others have mentioned thermal throttling also occurs sooner with higher power devices.", "Reply Score": 2}, {"Reply": ">Is the crucial thing. You're paying for rack units but electricity is , for all intents & purposes, free.\n\nAny colocation contract makes you pay for the rack AND the electricity.\n\nOtherwise people would just abuse it with hundreds of GPUs.", "Reply Score": 2}, {"Reply": "???\n\nIn a product or service costs are passed to the customer", "Reply Score": 2}]}, {"Comment": "So about 3c an hour while it's used. Hold off on buying a coffee or something just once and pay for 100 hours.", "Score": -1, "Replies": [{"Reply": "I have a super automatic espresso machine so not much coffee to hold off on. And while it uses a solid 1500W during its heat cycle, it\u2019s only about 60 seconds and then it drops to nothing.\n\nBut my home server, dual Xeon X5680, was costing me over $80/mo when peak rates were considered.  Now it just uses a significant portion of my battery storage.", "Reply Score": 0}]}, {"Comment": "My understanding is that Germany had a big movement to stop using nuclear power for a while, so they tended to use a lot of fossil fuels. Then when Russia invaded Ukraine the cost of fossil fuels went up if you didn\u2019t want to buy from Russia. So as a result cost of electricity generated by fossil fuels went up.", "Score": 21, "Replies": [{"Reply": "Germany dropped nuclear because the reactors were 40+ years old and needed expensive and long maintenance. Germany uses a lot of renewables.", "Reply Score": 3}]}, {"Comment": "idiotic politics.", "Score": 0, "Replies": [{"Reply": "Are there any other type of Politics?\n\n*Edit: proper grammar", "Reply Score": 1}]}, {"Comment": "Combination of gas prices and taxes", "Score": 1, "Replies": []}, {"Comment": ">Yep, like the \"it's mostly down to the manufacturing process\" substantially upvoted claim which is grossly inaccurate, I'm just tired chasing down every exaggeration here attributing most of the difference to that one thing someone thought of.\n\nYou're right, it's extraordinarily complex, given two leading edge designs, it appears *process does have a lot to do with it*.  But did I say it had everything to do with process?  No.\n\nI also said:\n\n>  ARM also has a long history of focusing on power consumption, because it had to, as they are embedded in all sorts of smallish devices. Apple also focused on power consumption, because your M2 is essentially a mobile chip.\n\nSo, yes, you're right perhaps I should have mentioned Apple's E-cores, because they had plenty to do with it, in particular, as well.  \n\nAnd remember what the OP is comparing:\n\n> Context: my Intel Core i7 MacBook Pro runs out of battery within 6 hours.  My M2 Pro Macbook runs for over 48 hours.\n\nOP was not comparing 5nm and 3nm TSMC, but his/her 5 year old computer to his/her new Macbook.  Process has a ton to do with global performance/power of the M line, especially given Intel was stuck on a process node, but design (E-cores!) and probably ISA play a huge part as well, especially re: power consumption, and I didn't mean to suggest otherwise.\n\nIf the Q is: Is there something fundamental re: x86 which makes this impossible to achieve, I think the answer is no.  See my discussion of new AMD mobile chips.", "Score": 4, "Replies": [{"Reply": ">But did I say it had everything to do with process?\n\nYou said \"mostly\", hence my wording, which said \"most\", not \"everything\". Have I misunderstood what \"mostly\" meant in the context it was used in? If so, my apologies.\n\nWe're in peace \u270c", "Reply Score": 2}]}, {"Comment": "And even some Intel dies starting with Meteor Lake.", "Score": 1, "Replies": []}, {"Comment": "See also [this rather excellent argument](https://www.reddit.com/r/hardware/comments/18cutwj/comment/kcdlsty/?utm_source=reddit&utm_medium=web2x&context=3) (not mine), which extends some of the above on E-cores where the decoder (and dependent logic) becomes in relative figures a bigger portion of the entire core (as the rest of the core is much smaller), both in area and energy.", "Score": 1, "Replies": []}, {"Comment": "The question wasn't about what the difference in power consumption is, but why that difference exists. Your reiteration of the question betrays the point. A more accurate form of your reiteration would be \"what about someone's car engine results in them getting home later compared to a different engine?\" The fact of the difference is already established, the details of it are what the question is asking for.", "Score": 2, "Replies": [{"Reply": "[deleted]", "Reply Score": 0}]}, {"Comment": "PowerPC is also alive because IBM can charge so much for their mainframes.\n\nARM is not greedy, was in the shadows. Like all those RISC microcontrollers. Then Apple refurbished Newton into iPhone .", "Score": 1, "Replies": []}, {"Comment": "Intuitively (as I do not design micros) I'd say it's absolutely undeniable that that translation (and IP duplication, like for instance having a secondary set of branch predictors in the CISC \"parser\" part in addition to another one in the RISC \"core\") costs energy, and so does the (buffered) transfer of the uOp stream from one to the other. The real question is, how much *at the scale of the entire SoC*.\n\nYou design micros for what, the entire system or one specific IP block?\n\nPlease forgive me my (hopefully healthy) skepticism towards \"*a lot* \\[of power\\]\" qualifier you have provided, but I've seen too many times block designers having rather no clue about exact contribution of the metrics of their block to the aggregate metrics of the entire final product / system / SoC.\n\nThe second major question is, how that \"how much\" compares to \"how much\" of various other factors, some of which are fairly obvious, like the [currently top-rated](https://www.reddit.com/r/compsci/comments/18cup5v/comment/kcd2ykv/?utm_source=reddit&utm_medium=web2x&context=3) \"real world x86 CPU designers we happen to have historically didn't need to focus on energy as much as real world ARM CPU designers we happen to have\", others -  a lot more contrived and most likely exceedingly difficult to asses even for working professionals within the industry, such as subtle differences in the memory model and even worse, non-trivial interactions with induced differences in code generation on the compiler side, where \"properties of code generation\" is a topic that most (if not all, sadly) silicon designers I have ever met in my life have pretty much no clue about, understandably so, and where the \"interactions with\" part, I suspect, most likely is a mystery for pretty much everyone, especially in a AArch64 vs x64 comparative manner.", "Score": 10, "Replies": []}, {"Comment": "uOpts are used in modern RISC CPUs (Such as ARM).  Granted, your overall point stands that x86, particularly with the mass of complex instructions/flags/etc, adds a lot to the heat budget.  There's also a big issue with variable width instructions in x86.  Even though ARM now has variable width instructions, they are still aligned and easy to decode.", "Score": 1, "Replies": []}, {"Comment": "Ah I see what you\u2019re saying. You can\u2019t run individual RISC instructions from a CISC instruction pipeline though, right? \n\nI think your comment is just a bit misleading. Just because an addition operation is a stage in a CISC instruction pipeline doesn\u2019t mean it\u2019s using RISC under the hood. I could also call the mod operator subtraction because I can replicate its behavior with multiple subtractions rather than a single mod operation (bad example, I know in hardware mod is a single step but I can\u2019t think of a good example).\n\nI also wouldn\u2019t describe it as translating from CISC to RISC. Every time the translation is identical since it\u2019s hardware. And some things just naturally require multiple stages, like encryption. CISC is great for handling encryption natively. If I am remembering correctly encryption with CISC requires fewer memory reads than RISC since it is far fewer instructions. So for CISC you just use the encryption operation(s), but with CISC you need to run each step in encryption, wait for it to complete, then read that result to use in the next step. And it is true that it is possible to do that in 1 clock cycle, but the frequency of that clock will be significantly lower. \n\nAnd yes, I know the academic goal is single clock instructions. But 1 clock instructions aren\u2019t always good in the real world. Sometimes it is better to split an instruction over multiple clocks to implement pipelining or multistage concepts. Those allow instructions to take variable amounts of time, which is very useful.\n\nFor example in a multistage processor (1 clock per stage, I think you refer to stages as micro ops) memory reads take the longest (5 stages) but memory writes only take 4 stages. If we wanted 1 instruction per clock we would have to waste 20% of our time when reading waiting for the clock to finish. We would end up wasting 40% of our time for the BEQ instruction, since it is only 3 stages. (Numbers from the textbook on RISC processor architecture we used at school).", "Score": 1, "Replies": []}, {"Comment": "It makes up a tiny part of the silicon. Still, for a cache miss Intel needs to do more decoding, while ARM can process the raw instruction stream until the cache line fills up.\n\nAlso ARM can be more optimistic about multithreading ( more like MIPS ), while x86 is consistent all the time.", "Score": 3, "Replies": []}, {"Comment": "The Apple cores have absolutely massive reorder buffers and other OoO hardware that accounts for most of the transistors of modern cores.", "Score": 1, "Replies": [{"Reply": "But still less. And good buffers push stuff through when they are empty. The fixed instruction length alone gives ARM a head start. Also reorder does not bloat, while x86 has to keep variable length or at least accept less then ideal RISC encoding.\n\nWhy does x86 have Soectre, but ARM has not? X86 cheats to be able to compete on performance.", "Reply Score": 1}]}, {"Comment": "ARM servers are getting increasingly popular, because they have great price/performance. X86 servers consuming power to run servers, generating heat and then cooling them, is very expensive!", "Score": 19, "Replies": []}, {"Comment": "There are Macs in data centers. There are also lots of x86 chips in those data centers.", "Score": 6, "Replies": []}, {"Comment": "That should depend on your contracts, but that aside very large number of companies are renting server time and cloud infrastructure now  though, so companies like Amazon and Microsoft who offer those products and do host their own DCs substitute a lot of those SMEs in the market for chips.\n\nAs others have mentioned thermal throttling also occurs sooner with higher power devices.", "Score": 2, "Replies": []}, {"Comment": ">Is the crucial thing. You're paying for rack units but electricity is , for all intents & purposes, free.\n\nAny colocation contract makes you pay for the rack AND the electricity.\n\nOtherwise people would just abuse it with hundreds of GPUs.", "Score": 2, "Replies": []}, {"Comment": "???\n\nIn a product or service costs are passed to the customer", "Score": 2, "Replies": [{"Reply": "And ? The customer follows the economic incentives of their material reality. If \\*everyone\\* bought less powerful but more efficient servers yeah the DC might choose to lower their prices , but me as someone renting 10 RU have absolutely no incentive whatsoever to do so.", "Reply Score": 0}, {"Reply": "And ? The customer follows the economic incentives of their material reality. If \\*everyone\\* bought less powerful but more efficient servers yeah the DC might choose to lower their prices , but me as someone renting 10 RU have absolutely no incentive whatsoever to do so.", "Reply Score": 0}]}, {"Comment": "I have a super automatic espresso machine so not much coffee to hold off on. And while it uses a solid 1500W during its heat cycle, it\u2019s only about 60 seconds and then it drops to nothing.\n\nBut my home server, dual Xeon X5680, was costing me over $80/mo when peak rates were considered.  Now it just uses a significant portion of my battery storage.", "Score": 0, "Replies": [{"Reply": "Yeah but I'm talking laptop here. Laptops have like a 100W power draw max.", "Reply Score": 1}]}, {"Comment": "Germany dropped nuclear because the reactors were 40+ years old and needed expensive and long maintenance. Germany uses a lot of renewables.", "Score": 3, "Replies": [{"Reply": "To my understanding nuclear was being slowly phased out starting 20+ years ago, so of course they didn\u2019t have new reactors. Germany has gone into renewables quite a bit, but still has heavy reliance on coal. If they didn\u2019t then energy prices wouldn\u2019t be so high there right now. You can read more on it [here](https://en.m.wikipedia.org/wiki/Nuclear_power_in_Germany).\n\nFrance on the other hand went hard into nuclear, especially after the gas crisis in the 70s. As a result their energy costs are half that of their neighbour Germany. See source [here](https://www.euronews.com/next/2023/03/29/energy-crisis-in-europe-which-countries-have-the-cheapest-and-most-expensive-electricity-a).\n\nEdit:\nApparently I need to take the cotton balls out of my ears and put them in my mouth. Would recommend reading the replies to this comment, and they seem to be better informed than me.", "Reply Score": 9}]}, {"Comment": "Are there any other type of Politics?\n\n*Edit: proper grammar", "Score": 1, "Replies": []}, {"Comment": ">But did I say it had everything to do with process?\n\nYou said \"mostly\", hence my wording, which said \"most\", not \"everything\". Have I misunderstood what \"mostly\" meant in the context it was used in? If so, my apologies.\n\nWe're in peace \u270c", "Score": 2, "Replies": []}, {"Comment": "[deleted]", "Score": 0, "Replies": [{"Reply": "It was stated that these are two MacBooks, which is pretty much an \"all else equal\" situation, at least as much as possible without heavy control, which people have done. Apple's own CPUs have been shown to have a performance per watt for beyond anything else on the consumer market.", "Reply Score": 0}]}, {"Comment": "But still less. And good buffers push stuff through when they are empty. The fixed instruction length alone gives ARM a head start. Also reorder does not bloat, while x86 has to keep variable length or at least accept less then ideal RISC encoding.\n\nWhy does x86 have Soectre, but ARM has not? X86 cheats to be able to compete on performance.", "Score": 1, "Replies": [{"Reply": "> But still less.\n\nNo, they are larger ROBs than x86.  http://www.complang.tuwien.ac.at/anton/robsize/\n\n> Why does x86 have Soectre, but ARM has not?\n\nLarger ARM cores are vulnerable to spectre, that's why there's an explict speculation barrier instruction for ARM.  https://developer.arm.com/documentation/ddi0596/2020-12/Base-Instructions/SB--Speculation-Barrier-", "Reply Score": 2}]}, {"Comment": "And ? The customer follows the economic incentives of their material reality. If \\*everyone\\* bought less powerful but more efficient servers yeah the DC might choose to lower their prices , but me as someone renting 10 RU have absolutely no incentive whatsoever to do so.", "Score": 0, "Replies": []}, {"Comment": "And ? The customer follows the economic incentives of their material reality. If \\*everyone\\* bought less powerful but more efficient servers yeah the DC might choose to lower their prices , but me as someone renting 10 RU have absolutely no incentive whatsoever to do so.", "Score": 0, "Replies": []}, {"Comment": "Yeah but I'm talking laptop here. Laptops have like a 100W power draw max.", "Score": 1, "Replies": [{"Reply": "Laptops were the example, but I thought the question was a lot broader.  There is a reason why most of the cloud providers are offering ARM options at a lower cost. And why I\u2019m looking at ARM solutions for nearly everything but my gaming PCs", "Reply Score": 1}]}, {"Comment": "To my understanding nuclear was being slowly phased out starting 20+ years ago, so of course they didn\u2019t have new reactors. Germany has gone into renewables quite a bit, but still has heavy reliance on coal. If they didn\u2019t then energy prices wouldn\u2019t be so high there right now. You can read more on it [here](https://en.m.wikipedia.org/wiki/Nuclear_power_in_Germany).\n\nFrance on the other hand went hard into nuclear, especially after the gas crisis in the 70s. As a result their energy costs are half that of their neighbour Germany. See source [here](https://www.euronews.com/next/2023/03/29/energy-crisis-in-europe-which-countries-have-the-cheapest-and-most-expensive-electricity-a).\n\nEdit:\nApparently I need to take the cotton balls out of my ears and put them in my mouth. Would recommend reading the replies to this comment, and they seem to be better informed than me.", "Score": 9, "Replies": [{"Reply": "As it stands right now, france has its very own battle with nuclear Power. The CRE wont get as much help from the french goverment as it used to get. The european Union is in dire need of clean Energy. But Nuclear in its current form is not it.", "Reply Score": 2}, {"Reply": ">  As a result their energy costs are half that of their neighbour Germany.\n\nThat more due to subsidies.\n\nAnd every summer Germany must prevent France from blackouts because their reactors need to shut down because the rivers get too hot.", "Reply Score": 0}]}, {"Comment": "It was stated that these are two MacBooks, which is pretty much an \"all else equal\" situation, at least as much as possible without heavy control, which people have done. Apple's own CPUs have been shown to have a performance per watt for beyond anything else on the consumer market.", "Score": 0, "Replies": [{"Reply": "[deleted]", "Reply Score": 1}]}, {"Comment": "> But still less.\n\nNo, they are larger ROBs than x86.  http://www.complang.tuwien.ac.at/anton/robsize/\n\n> Why does x86 have Soectre, but ARM has not?\n\nLarger ARM cores are vulnerable to spectre, that's why there's an explict speculation barrier instruction for ARM.  https://developer.arm.com/documentation/ddi0596/2020-12/Base-Instructions/SB--Speculation-Barrier-", "Score": 2, "Replies": []}, {"Comment": "Laptops were the example, but I thought the question was a lot broader.  There is a reason why most of the cloud providers are offering ARM options at a lower cost. And why I\u2019m looking at ARM solutions for nearly everything but my gaming PCs", "Score": 1, "Replies": []}, {"Comment": "As it stands right now, france has its very own battle with nuclear Power. The CRE wont get as much help from the french goverment as it used to get. The european Union is in dire need of clean Energy. But Nuclear in its current form is not it.", "Score": 2, "Replies": []}, {"Comment": ">  As a result their energy costs are half that of their neighbour Germany.\n\nThat more due to subsidies.\n\nAnd every summer Germany must prevent France from blackouts because their reactors need to shut down because the rivers get too hot.", "Score": 0, "Replies": []}, {"Comment": "[deleted]", "Score": 1, "Replies": [{"Reply": "That's at maximum power draw, performance per watt is a totally different measurement, not to mention the M2 has a much, much higher performance GPU in it.", "Reply Score": 1}]}, {"Comment": "That's at maximum power draw, performance per watt is a totally different measurement, not to mention the M2 has a much, much higher performance GPU in it.", "Score": 1, "Replies": [{"Reply": "[deleted]", "Reply Score": 1}]}, {"Comment": "[deleted]", "Score": 1, "Replies": [{"Reply": "Not at all. The M2 has higher performance per watt, which means that accomplishing the same tasks as the i7 will use less power and result in a longer battery life. Saying that two products in the same line, with the same model name have different goals is pretty presumptuous. They're the same product, just different iterations of it. The goal for Apple is and always has been to make a product that feels premium. The goal for Intel with the i7 is to make a top of the line productivity CPU, and that was more or less the same with Apple's M2. The difference is how they get there, and that's where the difference in performance comes in. They are vastly different CPUs, but that doesn't make asking why one is more efficient than the other a stupid or malformed question.\n\nAnd by the way, there's extensive testing online about Apple's M CPUs because people were so skeptical of them to begin with. There are tons, and tons of test results spanning dozens, maybe hundreds, of different benchmarks.", "Reply Score": 1}]}, {"Comment": "Not at all. The M2 has higher performance per watt, which means that accomplishing the same tasks as the i7 will use less power and result in a longer battery life. Saying that two products in the same line, with the same model name have different goals is pretty presumptuous. They're the same product, just different iterations of it. The goal for Apple is and always has been to make a product that feels premium. The goal for Intel with the i7 is to make a top of the line productivity CPU, and that was more or less the same with Apple's M2. The difference is how they get there, and that's where the difference in performance comes in. They are vastly different CPUs, but that doesn't make asking why one is more efficient than the other a stupid or malformed question.\n\nAnd by the way, there's extensive testing online about Apple's M CPUs because people were so skeptical of them to begin with. There are tons, and tons of test results spanning dozens, maybe hundreds, of different benchmarks.", "Score": 1, "Replies": []}]},{"Title": "Open Automation project", "Score": 1, "URL": "https://github.com/kaleLetendre/automation", "CreatedAt": 1701973826.0, "Full Content": "Hey fellow scientists, I thought some of yall may find this interesting. Its a little project I started for a sort of modular automation system. Feel free to branch it, fork it or send pull requests :D\n\nAt the moment all automation is based on image/script pairs. The first big addition I'd like to make is to add schedule/script state/script automation as well.", "CntComments": 0, "Comments": []},{"Title": "Computer Science Major Experience [Survey]", "Score": 0, "URL": "https://www.reddit.com/r/compsci/comments/18ax538/computer_science_major_experience_survey/", "CreatedAt": 1701731256.0, "Full Content": "Were you ever enrolled as a CS major at a college or university and decided to leave the major? We are four students from the Information Science Department at CU Boulder interested in learning about what leads students to leave a CS major.  \nThis survey will provide an opportunity to share your compliments and complaints through a fully anonymous system. The survey itself should only take 5-10 minutes to complete. Whether your experiences were positive, negative, or somewhere in-between, your experience is extremely valuable in allowing us to understand how we can improve the CS major and make it a more positive experience for others in the future.\n\n[https://cuboulder.qualtrics.com/jfe/form/SV\\_8qfDAR7ahmh9vvg](https://cuboulder.qualtrics.com/jfe/form/SV_8qfDAR7ahmh9vvg)", "CntComments": 7, "Comments": [{"Comment": "I didn\u2019t change majors or drop out, but the major draw back I\u2019m seeing with friends and myself, we weren\u2019t taught anything practical or enough in depth so we feel unprepared. My degree feels like a bunch of random classes crammed together that should be their own degrees. Computer science has gotten so broad today that our degrees are becoming diluted with so much information that what we really want to do gets hidden behind irrelevancy. I took an operating systems class, could not tell you how to build an operating system  because we didn\u2019t go that in depth. Networking class, we learn a little bit of how networks should work \u201ctheoretically,\u201d but I couldn\u2019t set up my own network to save my life, my professor never even used the word node in the whole class. Schools should start breaking up the \u201ccomputer science\u201d degree into its subfields like engineering has done. You don\u2019t get a degree in engineering, you do MechE, EE, Civil, etc. Should be the same for computer science: Software, Cloud, networking, security, etc.", "Score": 5, "Replies": []}, {"Comment": "Isn\u2019t it obviously because they don\u2019t understand the value of random theoretical CS classes, when really they wanted to build and sell apps, they don\u2019t see how learning assembly or heap sort or anything else in CS will get them there. They came looking to become rockstar coders but don\u2019t get the practical experience they are looking for. This is a major problem in CS imo, that the classes are randomly cobbled together and it\u2019s unclear what value they add while you\u2019re taking them. It isn\u2019t until possibly well after you graduate, and get a high paying job, that you start to see how capable you are due to your CS background. But man is it tough in the moment.", "Score": -7, "Replies": [{"Reply": "Life pro-tip: When speculating about why someone does something, it's almost never because they're dumb, or arrogant, or any other negative personal attribute. It's almost always because they have some circumstances you don't know about and they're trying to choose the best option they have.\n\nThe fancy term for this is \"[fundamental attribution error](https://en.wikipedia.org/wiki/Fundamental_attribution_error)\".\n\n> Isn\u2019t it obviously because...\n\nNo. They could have responsibilities in their personal life that interfere with school. They could have discovered some other career or topic that they like more than CS. They could have decided that the degree didn't work for them financial reasons. None of this is obvious unless you know their personal circumstances.\n\nTry to catch yourself when you're making negative assumptions about someone. It's easy and tempting, but it's almost always wrong and it will interfere with your ability to have good relationships and people will think you're a jerk.", "Reply Score": 6}]}, {"Comment": "Life pro-tip: When speculating about why someone does something, it's almost never because they're dumb, or arrogant, or any other negative personal attribute. It's almost always because they have some circumstances you don't know about and they're trying to choose the best option they have.\n\nThe fancy term for this is \"[fundamental attribution error](https://en.wikipedia.org/wiki/Fundamental_attribution_error)\".\n\n> Isn\u2019t it obviously because...\n\nNo. They could have responsibilities in their personal life that interfere with school. They could have discovered some other career or topic that they like more than CS. They could have decided that the degree didn't work for them financial reasons. None of this is obvious unless you know their personal circumstances.\n\nTry to catch yourself when you're making negative assumptions about someone. It's easy and tempting, but it's almost always wrong and it will interfere with your ability to have good relationships and people will think you're a jerk.", "Score": 6, "Replies": [{"Reply": "I wasn\u2019t assuming anything about the person dropping out, rather, I was talking about the university CS experience. It only gets rewarding as you approach the end, or when you get your first job. Nothing about my comment said anything of the individuals dropping out, who, of course, are subject to variance of life circumstance.", "Reply Score": 2}, {"Reply": "Idk man, I make at least half of my decisions out of stupidity", "Reply Score": 2}, {"Reply": "It seemed pretty clear to me they were primarily just relaying their grievances they had with their CS education.", "Reply Score": 1}]}, {"Comment": "I wasn\u2019t assuming anything about the person dropping out, rather, I was talking about the university CS experience. It only gets rewarding as you approach the end, or when you get your first job. Nothing about my comment said anything of the individuals dropping out, who, of course, are subject to variance of life circumstance.", "Score": 2, "Replies": []}, {"Comment": "Idk man, I make at least half of my decisions out of stupidity", "Score": 2, "Replies": []}, {"Comment": "It seemed pretty clear to me they were primarily just relaying their grievances they had with their CS education.", "Score": 1, "Replies": []}]},{"Title": "The First Law of Complexodynamics (Scott Aaronson, 2011)", "Score": 12, "URL": "https://scottaaronson.blog/?p=762", "CreatedAt": 1701644649.0, "Full Content": "", "CntComments": 0, "Comments": []},{"Title": "What numeral system would work well with a Trenary computer", "Score": 13, "URL": "https://www.reddit.com/r/compsci/comments/188ysbh/what_numeral_system_would_work_well_with_a/", "CreatedAt": 1701503532.0, "Full Content": "Hello. Just daydreaming about science fiction. I have only a tiny background in CompSci so I know about some concepts without necessarily understanding it.\n\n\nWith a Trenary computer, what would be a good numeral system to use with it? (Eg. Base10, Base16/Hex)\n\nI read that Hex is easy for human recognition as well, appreciate if someone could ease my curiosity by giving 2 answers -\n\n1. Easy for human usage as well as Trenary computers,\n2. Disregarding humans, just for use in Trenary computers. (AI programming themselves?)", "CntComments": 30, "Comments": [{"Comment": "Do you mean a ternary computer? You've answered your own question really, ternary computers have 3 states instead of the usual 2 and so will use trinary (base 3) by default.\n\nHex is only a good shortcut for humans with \"normal\" binary based computers because of the mathematical relationship between base 2 (binary) and base 16(hex) - each 4 binary digits = 1 hex digit. \n\nTo do the same for trinary, you'd need to use base 81 which woukd be very problematic as you'd need 81 different symbols. Numeric digits, uppercase and lowercase would get you to 62, so you'd need to give values to 19 different symbols. Good luck with that!\n\nA more sensible answer would be base 27, where each 3 trinary digits = 1 base 27 digit. Then you could just use 0 to 9 and 17 letters.\n\nYour question though also highlights a very common misconception by students around nunber systems. There is no inherent number system that a computer \"understands\" (in fact I have a huge problem with language that personifies computers like this, but that's another story). A standard computers uses tiny switches that can be on or off, hence we represent them as 1 or 0, or binary. But that's all it is, a representation. If we switch to hexadecimal, we're just changing how we write down the state of those switches, where one hex digit can represent 4 switches. It doesn't change how the computer stores any data or anything at all inside the machine. It's just easier for us as humans to remember/communicate/write down/use.", "Score": 47, "Replies": [{"Reply": "> A more sensible answer would be base 27, where each 3 trinary digits = 1 base 27 digit. Then you could just use 0 to 9 and 17 letters.\n\nOr just use base 9. In the minicomputer era, octal notation was pretty common and possibly more common than hex. If you look at the front panel for the PDP-8 and PCP-11, you see that the switches are arranged in groups of 3 for this reason.", "Reply Score": 7}, {"Reply": "\"the switches can be on or off\" is also a simplification for human consumption. The switches are capacitors, and they are (usually) set to 0v or 5v and that represents 0 or 1. But it takes energy to set them, and the more precisely they are set the more energy (releasing more heat and taking more time) that takes. So really anything below 2.0v is interpreted as 0 and anything above 3.0v is a 1, with a middle range indicating error.\n\nIn a ternary computer, one would use the digits -1,0,1 (balanced ternary, and we'd call them trits!) and have three ranges of voltage: say from 0v-1v for -1, 1.5v-2.5v for 0, and 3v-4v for a 1. Balanced ternary has some natural advantages over binary as a number system (see Knuth for details) for arithmetic. But tri-valued logic is not much more efficient than binary logic, and computers do a lot more logic than they do arithmetic.\n\nNote: I'm not an expert, and any corrections or amplifications from experts would be welcome.\n\n[https://en.wikipedia.org/wiki/Balanced\\_ternary](https://en.wikipedia.org/wiki/Balanced_ternary)\n\n[https://en.wikipedia.org/wiki/Ternary\\_computer](https://en.wikipedia.org/wiki/Ternary_computer)", "Reply Score": 5}, {"Reply": "Base 27 also probably makes more sense because the word size on a ternary computer would be a power of 3, which 4-trit digits wouldn't divide evenly into.", "Reply Score": 5}, {"Reply": "Thanks for the answer! And realizing that I don't fully know what I'm talking about. Well explained.\n\nSo one answer is base27.\n\nFollow up question: Ternary (thanks for the correction) computers could use 81 right? I'm assuming if we could get there we could always use programming languages to compile into base81 for us (or something). As for enough symbols we could just make up some or use special characters. I assume so because we don't program in binary, but programming languages allow us more human friendly methods to code.", "Reply Score": 0}]}, {"Comment": "If you are trying to think of 'how would an alien race that developed with tri-state electronics do mathematics in their computers', I think the most likely answer is \"Balanced Ternary\".  This is a numeric representation where each digit is either +, 0, or -.\n\nHere are some small numbers in balanced ternary:\n\n    -+++ = -14 (-27 + 9 + 3 + 1)\n    --- = -13 (-9 + -3 + -1)\n    -00 = -9\n    -++ = -5 (-9 + 3 + 1)\n    -- = -4\n    -0 = -3\n    -+ = -2\n    - = -1\n    0 = 0\n    + = 1\n    +- = 2 (3 + -1)\n    +0 = 3\n    ++ = 4\n    +-- = 5\n    +00 = 9\n    +++ = 13\n    +--- = 14 (27 + -9 + -3 + -1)\n\nA 9-trit balanced ternary number has a range from -29524 to +29524, which is pretty close to the range of integers handled by 16-bit human computers (-32768 to +32767 in twos-complement).  This might make sense as a byte replacement, especially if they had a very large alphabet (think 'asian language') and needed to represent a lot of characters in whatever their equivalent of ASCII is.  9 also has the nice property of being 3^2 (just as 16 is 2^4).\n\nI think the fundamental gates they would build those computers out of is a harder question to answer; in binary, there just aren't that many gates, and you can derive all of them from NAND.  In ternary logic there are many more possible gates and most of them are pretty confusing to think about.", "Score": 15, "Replies": [{"Reply": "3 balanced ternary trits can be rather nicely encoded in base 27 with a-m, _, n-z", "Reply Score": 1}]}, {"Comment": "A computer can handle any number base. It doesn\u2019t matter if its architecture is binary or ternary. \n\nHex is used sometimes on binary computers because you can encode a byte with 2 hex digits.\n\nI guess the equivalent on a ternary system would be base 9. The possible bases that directly map to ternary bits (trits?) are 3^n so 3, 9, 27, 81 etc. in that list, 3 is a bit too small and 27 a bit too large to be comfortably used by humans. \n\n2-digit ternary numbers would fit in 4 ternary bits and range from 0 to 80.", "Score": 3, "Replies": [{"Reply": "Well akshually bits is short for Binary digITs, so ternary digit would be tits\ud83e\udd13", "Reply Score": 9}]}, {"Comment": "If you're trying to write a science fiction story, I would avoid this \"ternary computer\" thing. It's not good science fiction. A computer scientist's reaction to a ternary computer would be \"so what?\". It wouldn't be able to do anything that a regular computer couldn't nor would it magically be faster just because it was ternary.\n\nIt's kind of like the number of letters in an alphabet. The English alphabet uses 26 letters. This ternary computer stuff is kind of like talking about a civilization that uses a 27 letter alphabet. That's not a great plot device since a language that uses 27 letters wouldn't really be all that different from one that uses 26. They might be able to have slightly shorter words on average (since you could represent more concepts with fewer letters if there are more distinct letters) but that language wouldn't fundamentally be any different from normal language.", "Score": 5, "Replies": [{"Reply": "I think a better analogy would be aliens using a non-decimal base, which is already explored in lots of sci fi (e.g. base 6 in Project Hail Mary).\n\nYou're of course right that it doesn't fundamentally change anything and just requires a simple base conversion, but it can still be interesting to read about, in the same way that aliens would be expected to use different time/length/etc. scales and that can be interesting to briefly explore.", "Reply Score": 3}, {"Reply": "Well, actually, by default, depending on the design, considering the most optimal code possible and comparing to an equivalent binary machine, [it could be about 60% more efficient and, so, potentially faster](https://web.williams.edu/Mathematics/sjmiller/public_html/105Sp10/addcomments/Hayes_ThirdBase.htm). That is why some were built in real life. It has to do with 3 being a little closer to the Euler's number and being able to represent more and bigger numbers with less digits but that's as far as I'm gonna tell you. [Here](https://www.techopedia.com/why-not-ternary-computers/2/32427) some interesting history. \n\nRegarding science fiction, heh, a society 60% more efficient in computing? A lot of people would tell you we could get 10000x more efficient tomorrow (or even more) getting rid of javascript and lazy programmers...efficiency is *definitely not* our priority as a species nowadays. Neither is real profit. It is difficult even to pinpoint what it is, we appear to be totally lost since the commies went down in shit and we supposedly won...but I'm going to stop right here, I am beginning [to sound like Jon Blow enough already...](https://m.youtube.com/watch?v=ZSRHeXYDLko)", "Reply Score": 1}, {"Reply": "Thanks for the perspective!\n\nWell, its supposed to be for a humanity simulation, so I was tossing about ideas for something scifi but not totally talking out of my ass.\n\nSomething a little different just to cause some realized differences in the environment. I was thinking of just explaining it away by... That's all they could manage instead of even higher-tiered computer systems.\n\nWould a Quarnery (what's the name?) or something higher be better?\n\nAlso, I was already thinking of the significance of 3 in my little daydream world before I thought about this and seemed to fit.", "Reply Score": 1}]}, {"Comment": "Different numeral systems just give us different *representations* of the same numerical values. Binary representation corresponds to how data is physically stored in a binary computer: in on/off switches. But we use programming languages rather than individually programming a quadrillion on/off switches. In those languages, we would use binary representation, octal, or hex, only very occasionally. For the rare cases where visualising individual or groups of bits makes it easier to understand what you are doing.", "Score": 2, "Replies": []}, {"Comment": "Negative base numbers.\n\nNot an answer to your question. But it's interesting to think about how a negative base number system would work.\nhttps://en.m.wikipedia.org/wiki/Negative_base", "Score": 2, "Replies": [{"Reply": "There is an interesting base-3 numerical system where the digits, instead of being 0, 1, and 2 are -1, 0, and +1. It has a lot of useful properties. Sadly I can't find anything about it on Wikipedia, I read about it on a paper from Kunth many years ago.", "Reply Score": 3}, {"Reply": "Thank you for the reference, but it's beyond me right now.\n\nSeems about the same though? In my limited understanding, it saves 1 digit when storing negative numbers but uses 1 more digit when storing positive numbers... So I failed to see what the point is.", "Reply Score": 1}]}, {"Comment": "Some floating point division hardware uses ternary bits.\n\nIt's the same as binary except that a bit can be 1, 0, -1.\n\nYou remember how when you are doing long division on paper, if you guess too high for a quotient digit and you have to erase a bunch of stuff?  In this system, rather than erase, you just make the next digit be negative.  Then at the end you fix it.", "Score": 2, "Replies": []}, {"Comment": "Binary computer is a base 2 computer therefore 111 in its register means 7. It logically implies that a ternary would be base 3. We don't need to find an alien species for computers with other bases. In the early days of computing there used to be decimal computers.\n\nComputer operations like ADD, SUBTRACT etc. can be defined for any natural number base, therefore it's conceivable that somebody can design hardware to do these operations in these bases.\n\nSince humans are most familiar with base 10 the output to a human user is usually given in base 10 but is most efficiently stored as whatever base the machine has.\n\nThe reason you might say that hex is human friendly is because it's being compared to binary. Imagine having to reason about this:\n\n101101010001\n\nA little bit easier would be to deal it in chunks of 4, for the same reason phone numbers are spelled out in 3-3-4 chunks:\n\n1011 0101 0001\n\nThere are as many kinds of 4 digit binary numbers 16 as there are digits in the hexadecimal or base 16 which are 0..9..a..f so each kind of four digit binary number can be assigned a character.\n\nB51 in hex. Sometimes hex numbers are printed with the 0x prefix like 0xB51.\n\nYou can think of a similar chunked notation for ternary computers too. While the numbers organically only contain the digits 0, 1 and 2, they can be grouped in chunks of 3 with 27 characters: 0..9..a..q.\n\n0xB51 in ternary is\n\n010 222 022\n\nIn base 27 is\n\n3Q8 which you could say is easier to read, memorize and spell out than 010 222 022.", "Score": 2, "Replies": []}, {"Comment": "Ternary would use base 27 by grouping into 3 trits (ternary digit) instead of 2^2 bits (binary digits), as ternary is base 3, and it would be writable without much ado with alphanumerical digits. Why? Base 81 is too large for alphanumerical digits, and is base 2 number of trit, which is not useful. A base 27 alfanumerical digit is 10 numbers + 17 letters: a,b,c,d,e, f,g,h,i,j, k,l,m,n,o, p,,q. \n\nA hex digit is a half word. The ternary word would not be 8 trits as 8 is not divisible by 3. Word size is base 2, as binary is base 2. The BCD proves why base number is important for artithmetics.", "Score": 1, "Replies": []}, {"Comment": "Well, actually, by default, depending on the design, considering the most optimal code possible and comparing to an equivalent binary machine, [it could be about 60% more efficient and, so, potentially faster](https://web.williams.edu/Mathematics/sjmiller/public_html/105Sp10/addcomments/Hayes_ThirdBase.htm). That is why some were built in real life. It has to do with 3 being a little closer to the Euler's number and being able to represent more and bigger numbers with less digits but that's as far as I'm gonna tell you. [Here](https://www.techopedia.com/why-not-ternary-computers/2/32427) some interesting history. \n\nRegarding science fiction, heh, a society 60% more efficient in computing? A lot of people would tell you we could get 10000x more efficient tomorrow (or even more) getting rid of javascript and lazy programmers...efficiency is *definitely not* our priority as a species nowadays. Neither is real profit. It is difficult even to pinpoint what it is, we appear to be totally lost since the commies went down in shit and we supposedly won...but I'm going to stop right here, I am beginning [to sound like Jon Blow enough already...](https://m.youtube.com/watch?v=ZSRHeXYDLko)", "Score": 1, "Replies": [{"Reply": "[6% (under some very speculative assumptions). Not 60%.](https://www.reddit.com/r/compsci/comments/188ysbh/what_numeral_system_would_work_well_with_a/kbo2e24/)", "Reply Score": 3}]}, {"Comment": "Thank you all for the answers! A few ppl have mentioned it and I have also briefly read about it - the cost efficiency of ternary computing.\n\nI had it in my mind, but didn't realize the importance of it. I was just thinking that in the future, we would be using something other than traditional transistors (or whatever... Pardon my ignorance). Instead having found something that naturally has 3 (or more) states, like some mystical quantum quark thingamajig. Such that cost alone would not be a factor, and for extreme computation needed.\n\nI was planning on ignoring (scifi?) the inherent difficulties of on/off and close to on/off (assuming electrical-thingy) for the 3rd state. And thinking of some magic hocus pocus like some quantum single/paired/paired but separated or something fancy like that.\n\nEdit: prob not the proper example. Around. Not around. Around but can't be seen/observed. Observable but not around.", "Score": 1, "Replies": []}, {"Comment": "> A more sensible answer would be base 27, where each 3 trinary digits = 1 base 27 digit. Then you could just use 0 to 9 and 17 letters.\n\nOr just use base 9. In the minicomputer era, octal notation was pretty common and possibly more common than hex. If you look at the front panel for the PDP-8 and PCP-11, you see that the switches are arranged in groups of 3 for this reason.", "Score": 7, "Replies": []}, {"Comment": "\"the switches can be on or off\" is also a simplification for human consumption. The switches are capacitors, and they are (usually) set to 0v or 5v and that represents 0 or 1. But it takes energy to set them, and the more precisely they are set the more energy (releasing more heat and taking more time) that takes. So really anything below 2.0v is interpreted as 0 and anything above 3.0v is a 1, with a middle range indicating error.\n\nIn a ternary computer, one would use the digits -1,0,1 (balanced ternary, and we'd call them trits!) and have three ranges of voltage: say from 0v-1v for -1, 1.5v-2.5v for 0, and 3v-4v for a 1. Balanced ternary has some natural advantages over binary as a number system (see Knuth for details) for arithmetic. But tri-valued logic is not much more efficient than binary logic, and computers do a lot more logic than they do arithmetic.\n\nNote: I'm not an expert, and any corrections or amplifications from experts would be welcome.\n\n[https://en.wikipedia.org/wiki/Balanced\\_ternary](https://en.wikipedia.org/wiki/Balanced_ternary)\n\n[https://en.wikipedia.org/wiki/Ternary\\_computer](https://en.wikipedia.org/wiki/Ternary_computer)", "Score": 5, "Replies": [{"Reply": "The switches are transistors, not capacitors.", "Reply Score": 7}]}, {"Comment": "Base 27 also probably makes more sense because the word size on a ternary computer would be a power of 3, which 4-trit digits wouldn't divide evenly into.", "Score": 5, "Replies": []}, {"Comment": "Thanks for the answer! And realizing that I don't fully know what I'm talking about. Well explained.\n\nSo one answer is base27.\n\nFollow up question: Ternary (thanks for the correction) computers could use 81 right? I'm assuming if we could get there we could always use programming languages to compile into base81 for us (or something). As for enough symbols we could just make up some or use special characters. I assume so because we don't program in binary, but programming languages allow us more human friendly methods to code.", "Score": 0, "Replies": [{"Reply": "You could use *any* base inside a ternary computer, just like you can use any base inside a binary computer. \n\nOne of the key concepts in computer science is abstraction. The low-level workings of the computer don't matter to the programmer. We hide away the binary details inside abstract data types like integers or floats, and we would do the same thing on a ternary computer. \n\nIn some cases ([Java-like languages](https://en.wikipedia.org/wiki/Java_virtual_machine)) we emulate an entire *abstract machine*, with completely different architecture than the underlying computer, and do our programming for that. You could theoretically make a JVM implementation for your ternary computer and run standard Java programs on it with no code changes.", "Reply Score": 20}]}, {"Comment": "3 balanced ternary trits can be rather nicely encoded in base 27 with a-m, _, n-z", "Score": 1, "Replies": []}, {"Comment": "Well akshually bits is short for Binary digITs, so ternary digit would be tits\ud83e\udd13", "Score": 9, "Replies": []}, {"Comment": "I think a better analogy would be aliens using a non-decimal base, which is already explored in lots of sci fi (e.g. base 6 in Project Hail Mary).\n\nYou're of course right that it doesn't fundamentally change anything and just requires a simple base conversion, but it can still be interesting to read about, in the same way that aliens would be expected to use different time/length/etc. scales and that can be interesting to briefly explore.", "Score": 3, "Replies": []}, {"Comment": "Well, actually, by default, depending on the design, considering the most optimal code possible and comparing to an equivalent binary machine, [it could be about 60% more efficient and, so, potentially faster](https://web.williams.edu/Mathematics/sjmiller/public_html/105Sp10/addcomments/Hayes_ThirdBase.htm). That is why some were built in real life. It has to do with 3 being a little closer to the Euler's number and being able to represent more and bigger numbers with less digits but that's as far as I'm gonna tell you. [Here](https://www.techopedia.com/why-not-ternary-computers/2/32427) some interesting history. \n\nRegarding science fiction, heh, a society 60% more efficient in computing? A lot of people would tell you we could get 10000x more efficient tomorrow (or even more) getting rid of javascript and lazy programmers...efficiency is *definitely not* our priority as a species nowadays. Neither is real profit. It is difficult even to pinpoint what it is, we appear to be totally lost since the commies went down in shit and we supposedly won...but I'm going to stop right here, I am beginning [to sound like Jon Blow enough already...](https://m.youtube.com/watch?v=ZSRHeXYDLko)", "Score": 1, "Replies": [{"Reply": "Where is the 60% figure coming from? It's not anywhere in the article you linked to. It seems you have gotten the math wrong.\n\nThe article suggests using `rw` as a measure of efficiency. As speculative as that is, under the `rw` measure ternary would be at best 6% -- not 60% -- more efficient than binary. Over the past few decades we have seen a billionfold increase in the efficiency of computation. An additional 6% would be nice, but it would be a drop in the bucket compared to the efficiency increases we've already seen.\n\nAnd I think there is a lot to be said against the `rw` measure of efficiency. It presupposes that a ternary digit would only \"cost\" 50% more than a binary digit. That assumption (for which there is absolutely no evidence, all evidence points to ternary digits being way less efficient than binary digits) would only have to be slightly untrue for the 6% savings to disappear.", "Reply Score": 8}]}, {"Comment": "Thanks for the perspective!\n\nWell, its supposed to be for a humanity simulation, so I was tossing about ideas for something scifi but not totally talking out of my ass.\n\nSomething a little different just to cause some realized differences in the environment. I was thinking of just explaining it away by... That's all they could manage instead of even higher-tiered computer systems.\n\nWould a Quarnery (what's the name?) or something higher be better?\n\nAlso, I was already thinking of the significance of 3 in my little daydream world before I thought about this and seemed to fit.", "Score": 1, "Replies": [{"Reply": "> Would a Quarnery (what's the name?) or something higher be better?\n\nModern computers already are quaternary essentially. Any binary number can be interpreted as a quaternary number with half as many digits, just by grouping the bits together in pairs. The binary number 10101101 is the same as the quarternary number 2231. We just split the bits up into groups of 2, so 10 10 11 01, and then use the fact that 10 = 2, 11 = 3, 01 = 1.\n\nWe usually think of computers as working in binary, but this is only a matter of interpretation. We could just as well interpret computers as working in quaternary (or any other power of two base number system).\n\nComputers don't generally do operations on individual bits, but rather work with longer strings of bits: most commonly 32-bit or 64-bit numbers are treated as the smallest \"unit\". Rather than working with sequences of 32 (or 64) binary digits, a ternary computer might instead work with sequences of 20 (or 40) ternary digits. This is why choosing a different base for a computer to work in isn't that interesting: you are just trading off the size of the base for the number of digits.", "Reply Score": 7}]}, {"Comment": "There is an interesting base-3 numerical system where the digits, instead of being 0, 1, and 2 are -1, 0, and +1. It has a lot of useful properties. Sadly I can't find anything about it on Wikipedia, I read about it on a paper from Kunth many years ago.", "Score": 3, "Replies": []}, {"Comment": "Thank you for the reference, but it's beyond me right now.\n\nSeems about the same though? In my limited understanding, it saves 1 digit when storing negative numbers but uses 1 more digit when storing positive numbers... So I failed to see what the point is.", "Score": 1, "Replies": [{"Reply": "It doesn't really have a point. Other than the noodle bending fun of realizing it's possible", "Reply Score": 1}]}, {"Comment": "[6% (under some very speculative assumptions). Not 60%.](https://www.reddit.com/r/compsci/comments/188ysbh/what_numeral_system_would_work_well_with_a/kbo2e24/)", "Score": 3, "Replies": []}, {"Comment": "The switches are transistors, not capacitors.", "Score": 7, "Replies": []}, {"Comment": "You could use *any* base inside a ternary computer, just like you can use any base inside a binary computer. \n\nOne of the key concepts in computer science is abstraction. The low-level workings of the computer don't matter to the programmer. We hide away the binary details inside abstract data types like integers or floats, and we would do the same thing on a ternary computer. \n\nIn some cases ([Java-like languages](https://en.wikipedia.org/wiki/Java_virtual_machine)) we emulate an entire *abstract machine*, with completely different architecture than the underlying computer, and do our programming for that. You could theoretically make a JVM implementation for your ternary computer and run standard Java programs on it with no code changes.", "Score": 20, "Replies": [{"Reply": "Thanks again. Hmm... Higher level talk here. (For me)\n\nSo in essence, it seems my question is moot. Because it doesn't matter to the computer what system you use, just for the ease of the human programmer.\n\nIt could be a base100 and works great on computer, but when displayed to a human, it would need to be converted to decimal or hex, or else the human would need to understand 100 symbols.\n\nIs that right?", "Reply Score": 3}]}, {"Comment": "Where is the 60% figure coming from? It's not anywhere in the article you linked to. It seems you have gotten the math wrong.\n\nThe article suggests using `rw` as a measure of efficiency. As speculative as that is, under the `rw` measure ternary would be at best 6% -- not 60% -- more efficient than binary. Over the past few decades we have seen a billionfold increase in the efficiency of computation. An additional 6% would be nice, but it would be a drop in the bucket compared to the efficiency increases we've already seen.\n\nAnd I think there is a lot to be said against the `rw` measure of efficiency. It presupposes that a ternary digit would only \"cost\" 50% more than a binary digit. That assumption (for which there is absolutely no evidence, all evidence points to ternary digits being way less efficient than binary digits) would only have to be slightly untrue for the 6% savings to disappear.", "Score": 8, "Replies": []}, {"Comment": "> Would a Quarnery (what's the name?) or something higher be better?\n\nModern computers already are quaternary essentially. Any binary number can be interpreted as a quaternary number with half as many digits, just by grouping the bits together in pairs. The binary number 10101101 is the same as the quarternary number 2231. We just split the bits up into groups of 2, so 10 10 11 01, and then use the fact that 10 = 2, 11 = 3, 01 = 1.\n\nWe usually think of computers as working in binary, but this is only a matter of interpretation. We could just as well interpret computers as working in quaternary (or any other power of two base number system).\n\nComputers don't generally do operations on individual bits, but rather work with longer strings of bits: most commonly 32-bit or 64-bit numbers are treated as the smallest \"unit\". Rather than working with sequences of 32 (or 64) binary digits, a ternary computer might instead work with sequences of 20 (or 40) ternary digits. This is why choosing a different base for a computer to work in isn't that interesting: you are just trading off the size of the base for the number of digits.", "Score": 7, "Replies": []}, {"Comment": "It doesn't really have a point. Other than the noodle bending fun of realizing it's possible", "Score": 1, "Replies": []}, {"Comment": "Thanks again. Hmm... Higher level talk here. (For me)\n\nSo in essence, it seems my question is moot. Because it doesn't matter to the computer what system you use, just for the ease of the human programmer.\n\nIt could be a base100 and works great on computer, but when displayed to a human, it would need to be converted to decimal or hex, or else the human would need to understand 100 symbols.\n\nIs that right?", "Score": 3, "Replies": [{"Reply": "Pretty much. A binary computer and a ternary computer can all do the same things and act in the same way. \n\nWe use binary computers because they are easier to build out of transistors. A base 100 computer would need to electrically measure the difference between 100 partially on/off states, which would be difficult and prone to error.", "Reply Score": 10}]}, {"Comment": "Pretty much. A binary computer and a ternary computer can all do the same things and act in the same way. \n\nWe use binary computers because they are easier to build out of transistors. A base 100 computer would need to electrically measure the difference between 100 partially on/off states, which would be difficult and prone to error.", "Score": 10, "Replies": []}]},{"Title": "Kabsch-Umeyama Algorithm - How to Align Point Patterns", "Score": 4, "URL": "https://www.reddit.com/r/compsci/comments/1891d9i/kabschumeyama_algorithm_how_to_align_point/", "CreatedAt": 1701514732.0, "Full Content": "Hi there,\n\nI've created a video [here](https://youtu.be/nCs_e6fP7Jo) where I explain how the Kabsch-Umeyama algorithm can be used to align point patterns.\n\nI hope it may be of use to some of you out there. Feedback is more than welcomed! :)", "CntComments": 0, "Comments": []},{"Title": "Artificial intelligence is already in our hospitals. 5 questions people want answered", "Score": 0, "URL": "https://theconversation.com/artificial-intelligence-is-already-in-our-hospitals-5-questions-people-want-answered-217374", "CreatedAt": 1701594154.0, "Full Content": "", "CntComments": 27, "Comments": [{"Comment": "I'm sorry but most of the questions in this article refer to concerns people working in healthcare have. The \"consumers\" of healthcare are called patients and it's weird to me why patients in this article are referred to as consumers. Patients want to be alive and healthy and want as little interaction with the healthcare system as needed to achieve this. From our own interactions with actual patients, their families, and patient advocates when presenting them innovations such as expert systems for diagnosis and treatment, the only things they cared about was if the innovations worked, if they'll live, live more, better, and if they'll be cured. Everything else is noise to them.", "Score": 14, "Replies": [{"Reply": "AI is the new snake oil, the cureall that fixes everything, and that is how it is being propagandized.  It works in some niche aspects.  Outside of that, its inaccuracy prevents it from being of more use.  That's why these people likely don't have questions, which is sad.  Most folks are only aware of the positives of AI and not the ethical questions and complications.  Maybe they should be.\n\nAs a person with multiple chronic conditions, I sure as heck would be asking these questions.", "Reply Score": 2}]}, {"Comment": "We should be more than happy to have AI in hospitals or medicine in general!\r  \n\r  \nThe accuracy, efficiency, and speed of AI can support doctors and researchers in making better decisions.", "Score": -1, "Replies": [{"Reply": "Who will be responsible if there is wrong decision?\n\nWho will be the Judge?", "Reply Score": -6}]}, {"Comment": ">AI can look for patterns in medical images to help diagnose disease. It can help predict who in a hospital ward might deteriorate. It can rapidly summarise medical research papers to help doctors stay up-to-date with the latest evidence.  \n>  \n>These are examples of AI making or shaping decisions health professionals previously made. More applications are being developed.  \n>  \n>But what do consumers think of using AI in health care? And how should their answers shape how it\u2019s used in the future?  \n>  \n>1. Does the AI work?  \n>  \n>2. Who\u2019s responsible if AI gets it wrong?  \n>  \n>3. Will AI make health care less fair?  \n>  \n>4. Will AI dehumanise health care?  \n>  \n>5. Will AI de-skill our health workers?", "Score": -10, "Replies": [{"Reply": "Can you not replace \"AI\" here with a \"book on medicine\"?  \n\n\n1. Does the book on medicine work? (As in, have correct infomation - probably mostly yes, sometimes no)\n2. Who's responsible if the book gets it wrong? (Whoever still used its incorrect judgement. The maker of the book is also probably responsible. But mostly, do not ask computer scientists, ask philosophers and lawyers.)\n3. Will the book make health care less fair? (Probably not. It will probably make it better for the priviliged, but not worse for the underpriviliged.)\n4. Will the book dehumanise health care? (No.)\n5. Will the book de-skill our health workers? (No, but it will change the skills they need.)", "Reply Score": 14}, {"Reply": "Got all the answers for you right here: https://xkcd.com/1289/", "Reply Score": 13}]}, {"Comment": "AI is the new snake oil, the cureall that fixes everything, and that is how it is being propagandized.  It works in some niche aspects.  Outside of that, its inaccuracy prevents it from being of more use.  That's why these people likely don't have questions, which is sad.  Most folks are only aware of the positives of AI and not the ethical questions and complications.  Maybe they should be.\n\nAs a person with multiple chronic conditions, I sure as heck would be asking these questions.", "Score": 2, "Replies": []}, {"Comment": "Who will be responsible if there is wrong decision?\n\nWho will be the Judge?", "Score": -6, "Replies": [{"Reply": "Who is responsible now when the hospital makes a bad decision?", "Reply Score": 9}, {"Reply": "The company that made the AI should be responsible, obviously. \n\nThat said, humans make wrong decisions all the time too. I'm okay with occasional errors *if* the error rate is lower than human.", "Reply Score": 8}, {"Reply": "Who's responsible for human wrong decisions? AI needs to be disciplined just like humans. Because behind an AI, there must be people who nurture it and check its results.", "Reply Score": 3}]}, {"Comment": "Can you not replace \"AI\" here with a \"book on medicine\"?  \n\n\n1. Does the book on medicine work? (As in, have correct infomation - probably mostly yes, sometimes no)\n2. Who's responsible if the book gets it wrong? (Whoever still used its incorrect judgement. The maker of the book is also probably responsible. But mostly, do not ask computer scientists, ask philosophers and lawyers.)\n3. Will the book make health care less fair? (Probably not. It will probably make it better for the priviliged, but not worse for the underpriviliged.)\n4. Will the book dehumanise health care? (No.)\n5. Will the book de-skill our health workers? (No, but it will change the skills they need.)", "Score": 14, "Replies": [{"Reply": ">Who's responsible if the book gets it wrong?\n\nThis question is the reason why AI is (was) often used as a \"second reader\". This means the person (a radiologist or oncologist or whatever) looks at the medical images without any AI help and makes a first diagnosis (so the person is \"the first reader\") and then the AI is \"turned on\" and it can give its contribution in different ways: it can highlight suspect areas to be checked again, or it can spit out a second diagnosis, which may be the same or differ from the first. Source: I've been working on software for computer assisted diagnosis since 2007.", "Reply Score": 10}]}, {"Comment": "Got all the answers for you right here: https://xkcd.com/1289/", "Score": 13, "Replies": []}, {"Comment": "Who is responsible now when the hospital makes a bad decision?", "Score": 9, "Replies": []}, {"Comment": "The company that made the AI should be responsible, obviously. \n\nThat said, humans make wrong decisions all the time too. I'm okay with occasional errors *if* the error rate is lower than human.", "Score": 8, "Replies": [{"Reply": "What is the sense of human that always listens to AI?\n\nHumans will not decide and die of depression quite soon.", "Reply Score": -1}]}, {"Comment": "Who's responsible for human wrong decisions? AI needs to be disciplined just like humans. Because behind an AI, there must be people who nurture it and check its results.", "Score": 3, "Replies": [{"Reply": "You are not going to put AI in jail, are you? It does not care.That's nonsense.", "Reply Score": -6}]}, {"Comment": ">Who's responsible if the book gets it wrong?\n\nThis question is the reason why AI is (was) often used as a \"second reader\". This means the person (a radiologist or oncologist or whatever) looks at the medical images without any AI help and makes a first diagnosis (so the person is \"the first reader\") and then the AI is \"turned on\" and it can give its contribution in different ways: it can highlight suspect areas to be checked again, or it can spit out a second diagnosis, which may be the same or differ from the first. Source: I've been working on software for computer assisted diagnosis since 2007.", "Score": 10, "Replies": []}, {"Comment": "What is the sense of human that always listens to AI?\n\nHumans will not decide and die of depression quite soon.", "Score": -1, "Replies": [{"Reply": "https://xkcd.com/1289/", "Reply Score": 5}]}, {"Comment": "You are not going to put AI in jail, are you? It does not care.That's nonsense.", "Score": -6, "Replies": [{"Reply": "Nobody's talking about jail, but norms and law. Not for AI itself, but for the people who manage it.", "Reply Score": 2}]}, {"Comment": "https://xkcd.com/1289/", "Score": 5, "Replies": []}, {"Comment": "Nobody's talking about jail, but norms and law. Not for AI itself, but for the people who manage it.", "Score": 2, "Replies": [{"Reply": "people will not manage it if it is always better - they will just do whatever it tells to do.\n\nDoctors will turn into nurses.", "Reply Score": -1}]}, {"Comment": "people will not manage it if it is always better - they will just do whatever it tells to do.\n\nDoctors will turn into nurses.", "Score": -1, "Replies": [{"Reply": "A person who always just does as they're told is perhaps less worthy of being considered intelligent than the AI. That sort of person is of limited use in complex situations even now, if all they do is follow another human's orders. Critical thinking and understanding of the decisions being made is essential, unless humans just want to just give up, roll over, and stop using their brains. No matter how superior the AI's judgment may get, we **must** try to keep someone competent in the loop, or else we are giving away our autonomy. And if AIs get so sophisticated, no human can even evaluate their decisions, then we need to use technology to catch humanity up, perhaps find a more efficient interface to communicate with computers.", "Reply Score": 3}, {"Reply": "This means going against the progress, but I respect your POV.", "Reply Score": 4}]}, {"Comment": "A person who always just does as they're told is perhaps less worthy of being considered intelligent than the AI. That sort of person is of limited use in complex situations even now, if all they do is follow another human's orders. Critical thinking and understanding of the decisions being made is essential, unless humans just want to just give up, roll over, and stop using their brains. No matter how superior the AI's judgment may get, we **must** try to keep someone competent in the loop, or else we are giving away our autonomy. And if AIs get so sophisticated, no human can even evaluate their decisions, then we need to use technology to catch humanity up, perhaps find a more efficient interface to communicate with computers.", "Score": 3, "Replies": []}, {"Comment": "This means going against the progress, but I respect your POV.", "Score": 4, "Replies": [{"Reply": "If humans turning into dumb slaves of AI is \"progress\" then how actually progress will happen?\n\nIf thinking yourself is not \"good feature\" anymore, how humans will evolve?\n\nDo you know how parasits lose their organs as they don't need them anymore? Do you want to be such parasit?", "Reply Score": -1}]}, {"Comment": "If humans turning into dumb slaves of AI is \"progress\" then how actually progress will happen?\n\nIf thinking yourself is not \"good feature\" anymore, how humans will evolve?\n\nDo you know how parasits lose their organs as they don't need them anymore? Do you want to be such parasit?", "Score": -1, "Replies": [{"Reply": "Are you saying you would rather die of disease than be cured by AI? Where's the sense in that?", "Reply Score": 3}, {"Reply": "It's all about interacting with AI responsibly, being ethically prepared, and have proper regulations.", "Reply Score": 1}]}, {"Comment": "Are you saying you would rather die of disease than be cured by AI? Where's the sense in that?", "Score": 3, "Replies": [{"Reply": "Sense is in progress, evolution of life.\n\nEverybody will die. Don't you know?\n\nSense is in cleverer children, not in your \"live forever\"", "Reply Score": 2}]}, {"Comment": "It's all about interacting with AI responsibly, being ethically prepared, and have proper regulations.", "Score": 1, "Replies": []}, {"Comment": "Sense is in progress, evolution of life.\n\nEverybody will die. Don't you know?\n\nSense is in cleverer children, not in your \"live forever\"", "Score": 2, "Replies": [{"Reply": "It's not about living forever - your quality of life depends a lot on the medical care you receive. \n\nEvolution is slow, cruel, and stupid. It would let a child die, when we could cure them.", "Reply Score": 3}]}, {"Comment": "It's not about living forever - your quality of life depends a lot on the medical care you receive. \n\nEvolution is slow, cruel, and stupid. It would let a child die, when we could cure them.", "Score": 3, "Replies": [{"Reply": "cruel? You exist only because of evolution.\n\nQuality of life has nothing to do with progress. Stone has best quality of life. And that life lasts for billions of years.", "Reply Score": 2}, {"Reply": "You\u2019re missing their point. When does it stop? We allow AI to guide our every move, when do we stop growing as a people and trusting our own minds and step into a world where we have no more free will. The will is the will of AI and nothing more. And with AI advancements and those who are in control over these AI, there will be snakes in the grass.", "Reply Score": 2}]}, {"Comment": "cruel? You exist only because of evolution.\n\nQuality of life has nothing to do with progress. Stone has best quality of life. And that life lasts for billions of years.", "Score": 2, "Replies": []}, {"Comment": "You\u2019re missing their point. When does it stop? We allow AI to guide our every move, when do we stop growing as a people and trusting our own minds and step into a world where we have no more free will. The will is the will of AI and nothing more. And with AI advancements and those who are in control over these AI, there will be snakes in the grass.", "Score": 2, "Replies": [{"Reply": "You are both anthropomorphizing the hell out of this. AI doesn't have its own will - we're talking about image recognition models for cancer diagnosis, not artificial life. It's a tool we create to advance *our* will, and achieve *our* goals.", "Reply Score": 2}]}, {"Comment": "You are both anthropomorphizing the hell out of this. AI doesn't have its own will - we're talking about image recognition models for cancer diagnosis, not artificial life. It's a tool we create to advance *our* will, and achieve *our* goals.", "Score": 2, "Replies": [{"Reply": "Ignoring the implications on if you were wrong on your first statement. The AI that is simply a tool is heavily biased by whatever and whoever is programming it. Following blindly an algorithm is going to lead everyone backwards.", "Reply Score": 2}]}, {"Comment": "Ignoring the implications on if you were wrong on your first statement. The AI that is simply a tool is heavily biased by whatever and whoever is programming it. Following blindly an algorithm is going to lead everyone backwards.", "Score": 2, "Replies": []}]},{"Title": "In which situation should a Segmented Tree should be used instead of Fenwick Tree / Binary Indexed Tree?", "Score": 24, "URL": "https://www.reddit.com/r/compsci/comments/1852b98/in_which_situation_should_a_segmented_tree_should/", "CreatedAt": 1701089655.0, "Full Content": "I was going through some theory on ST (Segmented Tree) and BIT (Binary Indexed Tree), and I came to know that BIT is more memory efficient, has smaller constant (In context of time complexity), has same complexity as ST, and is faster in construction from a given array.\n\nSo my question is, In which situation should a Segmented Tree should be used instead of Fenwick Tree / Binary Indexed Tree?", "CntComments": 2, "Comments": [{"Comment": "segmented Trees should be used instead of Fenwick Trees (Binary Indexed Trees) in scenarios where the problem requires not only point updates and range queries, but also more complex operations that BIT cannot support. These include handling non-commutative operations, queries that demand more information than just sums (like minimum or maximum in a range), and updates on a range of elements. While BIT is more memory-efficient and faster to construct, Segmented Trees offer greater flexibility in handling various types of queries and updates.", "Score": 7, "Replies": []}, {"Comment": "When should you use ST:\n\n- When the problem requires handling range updates, not just point updates.\n- If you need to support non-commutative operations (like min or max in a range).\n- In scenarios where you might need to handle dynamic data (insertions and deletions).\n\nWhen should you use BIT:\n\n- When the problem is limited to range sum queries and point updates or if you\u2019re dealing only with prefix sums.\n- In memory-constrained situations.\n- When simplicity and speed of construction are priorities. \n\n\nWhile BIT is more memory-efficient and simpler for certain types of problems, ST offers greater flexibility and capability, especially for complex range operations and dynamic datasets. The choice depends on the specific requirements and constraints of your problem.", "Score": 8, "Replies": []}]},{"Title": "Np hardness and Np completeness", "Score": 6, "URL": "https://www.reddit.com/r/compsci/comments/1859gnl/np_hardness_and_np_completeness/", "CreatedAt": 1701108764.0, "Full Content": "Hey, i read a lot about the Definition of these two things but i dont get it. Can someone explain with extremely simple english and slowly what np hard and np complete is?", "CntComments": 23, "Comments": [{"Comment": "Here's some brief, informal definitions:\n\nP: Problems that can be solved in polynomial time\n\nNP: Problems that can have their solutions _verified_ in polynomial time\n\nNP-Complete: The hardest decision problems we know in NP. We can use NP-Complete problems to simulate all other NP decision problems. That is, you can translate any NP decision problem into an NP-Complete problem in polynomial time, solve the NP-C problem, then translate back again in polynomial time.\n\nNP-Hard: Problems at least as hard as the hardest problems in NP. That is, NP-C problems count as NP-Hard, but there are harder problems in NP-Hard that _cannot_ be verified in polynomial time.\n\nI've written more about this with diagrams [in a blog post](https://backdrifting.net/post/065_algorithmic_complexity), but the [Wikipedia article on NP-Hardness](https://en.wikipedia.org/wiki/NP-hardness) and its diagram also do a good job of distinguishing the two.", "Score": 14, "Replies": [{"Reply": "Not really relevant for this response (which is good and correct), but I'm not sure I agree with everything you write about the limitations of big O analysis in you blog post.\n\nIn particular, although the most common resource to be indicated with big O is time (or computation work), it's entirely possible to use it for space complexity as well. And even computational complexity classes for space complexity are a thing (e.g. PSPACE). So even though time complexity is the one that's most often analysed and indicated for various algorithms, that's not really a limitation of the notation.\n\nAlso, I don't think the need to distinguish between worst-case, average-case and best-case complexity is a limitation of big O or Bachmann-Landau notation. Rather it would be prudent to always indicate whether a given complexity, whether in terms of time or space or any resource, is for the worst, average or best case.\n\nIn less formal contexts it's rather common to see statements such as \"insertion sort is in O( n^2 )\" or that \"quicksort is in O(n log n)\", while it would be more accurate to say that insertion sort works in O( n^2 ) time in the worst and average cases, or that the time complexity of quicksort is in O(n log n) in the average case and in O( n^2 ) in the worst case. Similar information for best case analysis or for space complexity can also be added -- often they just aren't, outside of proper algorithm research papers or textbooks.\n\nThe other limitations you mention are true, of course.", "Reply Score": 3}, {"Reply": "Thank you.\n\nFor np hard: i read that \"problems at least as hard as the hardest problems in np\" every damn time but i dont get it. Are Things that are np hard also np? What is the difference then between np and np hard when they are the same?\n\nFor np conplete: so you can make np Problems harder? In polynomial time? What is the use for this?", "Reply Score": -1}]}, {"Comment": "Any language A is NP-complete if:\n\n1. It is itself in NP.\n2. For any language L that is in NP, L can be reduced in polynomial-time to A.\n\nThe first requirement is quite simple: It just means that there's a way to verify the correctness of a solution in polynomial time.\n\nThe second requirement means that every problem can (in theory) be translated into a variation of our NP-complete problem in polynomial time. Literally, it means we can take any problem in NP and rewrite it such that it's just a variant of an NP-complete problem.\n\n&#x200B;\n\nAny language A is NP-hard if:\n\n1. For any language L that is in NP, L can be reduced in polynomial time to A.\n\nNotice that it is missing one of the elements that would classify it as NP-complete. What does it mean that it is not itself in NP? It means that there's no guarantee that checking the correctness of an answer is fast.\n\nConsider 3SAT - this problem asks \"Given a boolean formula, is there a set of values we can assign to the variables that make the statement true?\". Now consider TSP: \"Is this cycle the shortest one connecting all cities?\". Let's evaluate what complexity class they belong to.\n\nYou'll have to take my word for this: It turns out that any problem in NP can be translated into a variant of either 3SAT or TSP. This means that both of these problems are NP-hard.\n\nNow, the question is: Are either of these NP-complete? 3SAT is, and it's because checking if a candidate answer is correct is really fast - we just plug in the values to the formula and see if it evaluates to true. Now, what about TSP? No such luck. It is not NP-complete because, even if I gave you a candidate answer, you have no quick way of checking if my cycle is actually the shortest one. You need to compare it to all other cycles, and see where it ranks. Very slow.", "Score": 2, "Replies": []}, {"Comment": "Not really relevant for this response (which is good and correct), but I'm not sure I agree with everything you write about the limitations of big O analysis in you blog post.\n\nIn particular, although the most common resource to be indicated with big O is time (or computation work), it's entirely possible to use it for space complexity as well. And even computational complexity classes for space complexity are a thing (e.g. PSPACE). So even though time complexity is the one that's most often analysed and indicated for various algorithms, that's not really a limitation of the notation.\n\nAlso, I don't think the need to distinguish between worst-case, average-case and best-case complexity is a limitation of big O or Bachmann-Landau notation. Rather it would be prudent to always indicate whether a given complexity, whether in terms of time or space or any resource, is for the worst, average or best case.\n\nIn less formal contexts it's rather common to see statements such as \"insertion sort is in O( n^2 )\" or that \"quicksort is in O(n log n)\", while it would be more accurate to say that insertion sort works in O( n^2 ) time in the worst and average cases, or that the time complexity of quicksort is in O(n log n) in the average case and in O( n^2 ) in the worst case. Similar information for best case analysis or for space complexity can also be added -- often they just aren't, outside of proper algorithm research papers or textbooks.\n\nThe other limitations you mention are true, of course.", "Score": 3, "Replies": [{"Reply": "Thank you for the feedback! I agree with all of it. My intention with that section was to say \"analyzing average time complexity is an awesome shorthand for comparing algorithms and is often our first tool, but don't overlook memory usage and limitations of the approach.\" I should have been more careful to distinguish limitations of the notation (which as you point out, can be applied to other resources and best-/worst-case) from limitations of asymptotic analysis as a whole and average runtime analysis in particular.", "Reply Score": 2}]}, {"Comment": "Thank you.\n\nFor np hard: i read that \"problems at least as hard as the hardest problems in np\" every damn time but i dont get it. Are Things that are np hard also np? What is the difference then between np and np hard when they are the same?\n\nFor np conplete: so you can make np Problems harder? In polynomial time? What is the use for this?", "Score": -1, "Replies": [{"Reply": "NP-Hard problems include the hardest NP problems (the NP-Complete problems) and _harder_ problems that are outside of NP, such as Traveling Salesman. See the left side of [this diagram](https://en.wikipedia.org/wiki/NP-hardness#/media/File:P_np_np-complete_np-hard.svg) for a visual explanation. Most of NP-Hard is outside of NP, and usually when we are discussing NP-Hard problems we are talking about problems that can't even be verified in polynomial time, and so are outside of NP - but technically NP-C is included in NP-Hard, too.\n\nFor NP-Complete: The use is \"if I can find a fast solution to an NP-Complete problem then I have found a fast solution to _all_ NP problems, because we can translate all NP problems into this NP-Complete problem.\" This is especially relevant in P=NP debates. If we can find a poly-time solution to any NP-C problem, then that implies that P=NP=NP-C, because we can solve every NP-C problem, and all NP problems, including all P problems, in polynomial time. This is primarily of academic interest, because no such fast solution has been found.\n\nNotably, if P=NP that does not mean that _all_ problems can be solved in polynomial time. The NP-Hard problems outside of NP will remain expensive (see the right side [of that same diagram](https://en.wikipedia.org/wiki/NP-hardness#/media/File:P_np_np-complete_np-hard.svg))", "Reply Score": 3}, {"Reply": "You should read \u201cat least as hard\u201d as sort of like a greater-than-equal (ie >= ) relationship. Many cases, \u201cat least as hard\u201d will end up being the same hardness (ie in same complexity class).\n\nWith that said, it\u2019s also possible to be NP-hard and *not* in NP, because there are are problems harder than every problem in NP, AND no problem in NP is harder than it. In fact there are classes of problems harder than NP problems.\n\nNP completeness simply means the problem is are both \u201cat least as hard\u201d as every NP problem, and it\u2019s also in NP (by the definition of NP from the above comment).", "Reply Score": 1}]}, {"Comment": "Thank you for the feedback! I agree with all of it. My intention with that section was to say \"analyzing average time complexity is an awesome shorthand for comparing algorithms and is often our first tool, but don't overlook memory usage and limitations of the approach.\" I should have been more careful to distinguish limitations of the notation (which as you point out, can be applied to other resources and best-/worst-case) from limitations of asymptotic analysis as a whole and average runtime analysis in particular.", "Score": 2, "Replies": []}, {"Comment": "NP-Hard problems include the hardest NP problems (the NP-Complete problems) and _harder_ problems that are outside of NP, such as Traveling Salesman. See the left side of [this diagram](https://en.wikipedia.org/wiki/NP-hardness#/media/File:P_np_np-complete_np-hard.svg) for a visual explanation. Most of NP-Hard is outside of NP, and usually when we are discussing NP-Hard problems we are talking about problems that can't even be verified in polynomial time, and so are outside of NP - but technically NP-C is included in NP-Hard, too.\n\nFor NP-Complete: The use is \"if I can find a fast solution to an NP-Complete problem then I have found a fast solution to _all_ NP problems, because we can translate all NP problems into this NP-Complete problem.\" This is especially relevant in P=NP debates. If we can find a poly-time solution to any NP-C problem, then that implies that P=NP=NP-C, because we can solve every NP-C problem, and all NP problems, including all P problems, in polynomial time. This is primarily of academic interest, because no such fast solution has been found.\n\nNotably, if P=NP that does not mean that _all_ problems can be solved in polynomial time. The NP-Hard problems outside of NP will remain expensive (see the right side [of that same diagram](https://en.wikipedia.org/wiki/NP-hardness#/media/File:P_np_np-complete_np-hard.svg))", "Score": 3, "Replies": [{"Reply": "Thank you again for your answer. Could it be that my professor messed up np hard and np complete? He defined it as following:\n\nNp hard: a problem L is np hard if for every L' element NP, there is a polynomial-time reduction from L' to L\n\nI understand that as following: L is np hard if you can convert any other problem that is NP or NP complete in polinomial time to L\n\nNp complete: the problem L is NP complete if L is NP hard and L element NP\n\nThat i do not understand because then my first Definition would be wrong and thats where i have problems understanding this stuff...\n\n\nWith your Definition i would say it as follows:\n\nNp hard: all problems that are NP complete or harder are NP hard. These Problems solutions cannot be verifier in polynomial time.\n\nNp complete: i dont get it tbh... So these solutions can also not be verified in polynomial time? But than its the same as np hard. But if they can be verified in polinomial time then they are the same as NP. So how exactly do they differ from both?", "Reply Score": 1}]}, {"Comment": "You should read \u201cat least as hard\u201d as sort of like a greater-than-equal (ie >= ) relationship. Many cases, \u201cat least as hard\u201d will end up being the same hardness (ie in same complexity class).\n\nWith that said, it\u2019s also possible to be NP-hard and *not* in NP, because there are are problems harder than every problem in NP, AND no problem in NP is harder than it. In fact there are classes of problems harder than NP problems.\n\nNP completeness simply means the problem is are both \u201cat least as hard\u201d as every NP problem, and it\u2019s also in NP (by the definition of NP from the above comment).", "Score": 1, "Replies": [{"Reply": "But that means that np hard problems are also in np. Which means np=np complete= np hard?", "Reply Score": 1}]}, {"Comment": "Thank you again for your answer. Could it be that my professor messed up np hard and np complete? He defined it as following:\n\nNp hard: a problem L is np hard if for every L' element NP, there is a polynomial-time reduction from L' to L\n\nI understand that as following: L is np hard if you can convert any other problem that is NP or NP complete in polinomial time to L\n\nNp complete: the problem L is NP complete if L is NP hard and L element NP\n\nThat i do not understand because then my first Definition would be wrong and thats where i have problems understanding this stuff...\n\n\nWith your Definition i would say it as follows:\n\nNp hard: all problems that are NP complete or harder are NP hard. These Problems solutions cannot be verifier in polynomial time.\n\nNp complete: i dont get it tbh... So these solutions can also not be verified in polynomial time? But than its the same as np hard. But if they can be verified in polinomial time then they are the same as NP. So how exactly do they differ from both?", "Score": 1, "Replies": [{"Reply": "Every problem in NP can have its solution verified in polynomial time. This includes NP-Complete problems. NP-Complete problems might be very hard to _solve,_ but they're easy to verify. Consider the graph-coloring problem: given a map, can you color all countries using only three colors such that no two adjacent countries have the same color? Very expensive to find a color assignment that works, but very easy to take someone's solution and confirm that it works.\n\nNP-Hard problems include NP-Complete (which can be verified in poly-time) and additional problems outside of NP (which cannot be verified in poly-time - usually because verifying a solution requires re-solving the problem, and we don't know how to solve these problems in polynomial time).", "Reply Score": 2}, {"Reply": "NP-Complete have to be decision problems with yes/no answers. NP-Hard problems are usually the optimization problems associated with an NP-Complete problem i.e. \"In this graph is there a tour shorter than a fixed distance k\" is the NP-Complete formulation of traveling salesman problem. \"What is the shortest tour in this graph\" is the NP-Hard version. At least that's what I remember, though it's been a bit since I took the class.", "Reply Score": 0}]}, {"Comment": "But that means that np hard problems are also in np. Which means np=np complete= np hard?", "Score": 1, "Replies": [{"Reply": "Like I said, NP complete is the elements that are both in NP and in NP Hard  (intersection). So:\n\n- NP complete is *entirely within* NP hard, but not the other way around. \n\n- NP complete is also *entirely within* NP, but not the other way around.", "Reply Score": 1}, {"Reply": "No, a problem that requires exponential time to verify is not in np but is probably np hard.", "Reply Score": 1}]}, {"Comment": "Every problem in NP can have its solution verified in polynomial time. This includes NP-Complete problems. NP-Complete problems might be very hard to _solve,_ but they're easy to verify. Consider the graph-coloring problem: given a map, can you color all countries using only three colors such that no two adjacent countries have the same color? Very expensive to find a color assignment that works, but very easy to take someone's solution and confirm that it works.\n\nNP-Hard problems include NP-Complete (which can be verified in poly-time) and additional problems outside of NP (which cannot be verified in poly-time - usually because verifying a solution requires re-solving the problem, and we don't know how to solve these problems in polynomial time).", "Score": 2, "Replies": [{"Reply": "May i ask why np complete is in np hard? Seems weird for me because they are things in np hard that differ from np complete. (Np complete can be verified in polynomial time but no hard cant yet they are both np hard how can that be?) I dont get it how you can say no hard cant be verified in polynomial time but it can be verified in polynomial time?", "Reply Score": 1}]}, {"Comment": "NP-Complete have to be decision problems with yes/no answers. NP-Hard problems are usually the optimization problems associated with an NP-Complete problem i.e. \"In this graph is there a tour shorter than a fixed distance k\" is the NP-Complete formulation of traveling salesman problem. \"What is the shortest tour in this graph\" is the NP-Hard version. At least that's what I remember, though it's been a bit since I took the class.", "Score": 0, "Replies": [{"Reply": "But how can np hard and np complete then be in the same class if they are clearly not the same?", "Reply Score": 1}]}, {"Comment": "Like I said, NP complete is the elements that are both in NP and in NP Hard  (intersection). So:\n\n- NP complete is *entirely within* NP hard, but not the other way around. \n\n- NP complete is also *entirely within* NP, but not the other way around.", "Score": 1, "Replies": [{"Reply": "Ok that kinda confuses me. Idk why\n\nThank you.\n\nSo could i say that:\n\nNp: everything in Here, including np complete problems solutions are verifyable in polynomial time\n\nNp complete: just the \"hardest\" problems of np (but dont know how exactly hardest is defined)\n\nNp hard: all problems not verifyable in polinomial time + np complete problems", "Reply Score": 1}]}, {"Comment": "No, a problem that requires exponential time to verify is not in np but is probably np hard.", "Score": 1, "Replies": []}, {"Comment": "May i ask why np complete is in np hard? Seems weird for me because they are things in np hard that differ from np complete. (Np complete can be verified in polynomial time but no hard cant yet they are both np hard how can that be?) I dont get it how you can say no hard cant be verified in polynomial time but it can be verified in polynomial time?", "Score": 1, "Replies": [{"Reply": "> May i ask why np complete is in np hard?\n\nNP Hard is defined as the problems that are at least as hard as the hardest problems in NP. The hardest problems in NP are called NP-Complete. Therefore, NP-Complete is part of NP-Hard. It would perhaps be more convenient if NP-Hard were defined as \"harder\" than NP, but that's not the definition.\n\n> I dont get it how you can say no hard cant be verified in polynomial time but it can be verified in polynomial time?\n\nI haven't said this. Some NP-Hard problems can be verified in polynomial time (those in NP-C) while others cannot (those outside NP).", "Reply Score": 3}]}, {"Comment": "But how can np hard and np complete then be in the same class if they are clearly not the same?", "Score": 1, "Replies": [{"Reply": "NP-Hard contains NP-Complete but not the other way around.", "Reply Score": 3}]}, {"Comment": "Ok that kinda confuses me. Idk why\n\nThank you.\n\nSo could i say that:\n\nNp: everything in Here, including np complete problems solutions are verifyable in polynomial time\n\nNp complete: just the \"hardest\" problems of np (but dont know how exactly hardest is defined)\n\nNp hard: all problems not verifyable in polinomial time + np complete problems", "Score": 1, "Replies": [{"Reply": "In your last comment, it should be \u201c*some* NP hard problems are in NP\u201d, but certainly not *all*. Specifically, it\u2019s the NP complete ones which are both in NP and are NP hard.", "Reply Score": 1}]}, {"Comment": "> May i ask why np complete is in np hard?\n\nNP Hard is defined as the problems that are at least as hard as the hardest problems in NP. The hardest problems in NP are called NP-Complete. Therefore, NP-Complete is part of NP-Hard. It would perhaps be more convenient if NP-Hard were defined as \"harder\" than NP, but that's not the definition.\n\n> I dont get it how you can say no hard cant be verified in polynomial time but it can be verified in polynomial time?\n\nI haven't said this. Some NP-Hard problems can be verified in polynomial time (those in NP-C) while others cannot (those outside NP).", "Score": 3, "Replies": [{"Reply": "But shouldnt we then differentiate between \"np hard outside of np\" and \"np hard inside of np\"?\n\nThat would make a lot more sense to em", "Reply Score": 1}]}, {"Comment": "NP-Hard contains NP-Complete but not the other way around.", "Score": 3, "Replies": []}, {"Comment": "In your last comment, it should be \u201c*some* NP hard problems are in NP\u201d, but certainly not *all*. Specifically, it\u2019s the NP complete ones which are both in NP and are NP hard.", "Score": 1, "Replies": [{"Reply": "Ok. Thank you.\n\nHow exactly is np complete defined as being \"the hardest problems in np?\" Do they mean with the longest solving time? Longest verify time?", "Reply Score": 1}]}, {"Comment": "But shouldnt we then differentiate between \"np hard outside of np\" and \"np hard inside of np\"?\n\nThat would make a lot more sense to em", "Score": 1, "Replies": [{"Reply": "Sure, that's sometimes helpful. But usually if we're talking about _solving_ a problem then the difference doesn't matter (all of NP-Hard is difficult to solve, NP-C or not), and if we're talking about _verifying_ a problem then we'd refer to NP-C as NP-C and not NP-Hard. Rarely ambiguous in practice.", "Reply Score": 2}]}, {"Comment": "Ok. Thank you.\n\nHow exactly is np complete defined as being \"the hardest problems in np?\" Do they mean with the longest solving time? Longest verify time?", "Score": 1, "Replies": [{"Reply": "\u201cat least as hard\u201d is defined formally through reductions. There are different kinds, and can get somewhat complicated, but the gist of it is pretty simple. If I give you a solution to problem A, and you can use that as part of the solution to problem B, then A must be at least as hard as B. \n\nIntuitively this sort of makes sense. Suppose this sentence is true: \u201cif I had 100 trillion dollars, I could solve world hunger\u201d. A solution to the problem of \u201chow to get 100 trillion dollars\u201d becomes a solution to solving world hunger. IF we had a way to obtain 100 trillion dollars, solving world hunger becomes simple, so the problem solving world hunger reduces to getting a lot of money. However, it could be the case that solving world hunger is easier than getting 100 trillion dollars. So the problem of getting 100 trillion dollars is \u201cat least as hard\u201d as solving world hunger. Could be the same, or easier.\n\nA more concrete example, if I had a 3sat solution, as in an algorithm that takes 3sat instances and produces 3sat solutions, I could use this to solve graph coloring problems. To do this, I read in my graph, and transform the constraints into 3sat constraints, then solve them with my algorithm. This tells me that 3sat coloring is at least as hard as graph coloring. Note that this transformation must be at most polynomial time.\n\nIn this case you can also do it the other way around, you can convert a 3sat problem into a graph, where the satisfiability is linked to coloring the graph. In this case you could say that 3sat and graph coloring are as hard as one another, and both are NP complete.", "Reply Score": 1}]}, {"Comment": "Sure, that's sometimes helpful. But usually if we're talking about _solving_ a problem then the difference doesn't matter (all of NP-Hard is difficult to solve, NP-C or not), and if we're talking about _verifying_ a problem then we'd refer to NP-C as NP-C and not NP-Hard. Rarely ambiguous in practice.", "Score": 2, "Replies": []}, {"Comment": "\u201cat least as hard\u201d is defined formally through reductions. There are different kinds, and can get somewhat complicated, but the gist of it is pretty simple. If I give you a solution to problem A, and you can use that as part of the solution to problem B, then A must be at least as hard as B. \n\nIntuitively this sort of makes sense. Suppose this sentence is true: \u201cif I had 100 trillion dollars, I could solve world hunger\u201d. A solution to the problem of \u201chow to get 100 trillion dollars\u201d becomes a solution to solving world hunger. IF we had a way to obtain 100 trillion dollars, solving world hunger becomes simple, so the problem solving world hunger reduces to getting a lot of money. However, it could be the case that solving world hunger is easier than getting 100 trillion dollars. So the problem of getting 100 trillion dollars is \u201cat least as hard\u201d as solving world hunger. Could be the same, or easier.\n\nA more concrete example, if I had a 3sat solution, as in an algorithm that takes 3sat instances and produces 3sat solutions, I could use this to solve graph coloring problems. To do this, I read in my graph, and transform the constraints into 3sat constraints, then solve them with my algorithm. This tells me that 3sat coloring is at least as hard as graph coloring. Note that this transformation must be at most polynomial time.\n\nIn this case you can also do it the other way around, you can convert a 3sat problem into a graph, where the satisfiability is linked to coloring the graph. In this case you could say that 3sat and graph coloring are as hard as one another, and both are NP complete.", "Score": 1, "Replies": []}]},{"Title": "The largest number representable in 64 bits", "Score": 0, "URL": "https://tromp.github.io/blog/2023/11/24/largest-number", "CreatedAt": 1701124585.0, "Full Content": "", "CntComments": 13, "Comments": [{"Comment": "\"largest number representable in 64 bits\" is a bit of a stretch...the author just chose an arbitrary encoding system that's really just the busy beaver function in disguise, and declared that to be a \"representation\" of a 64 bit number.\n\nSure, you can define a binary encoding system under which a bit string decodes to (i.e., \"represents\") the number of 1s on the output tape of the Turing machine encoded by that bit string (if it halts), but for one, such an encoding system is not computable, and second, you can easily choose another arbitrary encoding system that beats the busy beaver function.\n\nJust define another binary encoding system that follows the same idea, but let the underlying function be the busy beaver function for Turing machines equipped with a halting oracle for Turing machines.\n\nOr another example: let the bit string `s` represent `Rayo(10^(x))` where `x` is the natural number represented by bit string `s` under a standard binary encoding, and there you go, a \"representation\" of numbers where the largest representable number in 64 bits is larger than what the author presented.\n\nBut more importantly, you can't cheat the rules of information theory: 64 bits can only encode 64 bits of information. Whatever encoding scheme you choose, you'll never be able to represent more than 2^64 distinct entities with it. If your encoding scheme is really just the busy beaver function, you will be up to represent at most up to 2^64 busy beaver numbers, but never more, and never any numbers that fall outside of the codomain of the busy beaver function.\n\nBy the author's logic, the \"largest number representable in 64 bits\" is undefined and unbounded, because you can always define a larger, arbitrary natural number \"represented\" with 64 bits of information as long as you choose the right encoding system on those 64 bits of information.", "Score": 44, "Replies": [{"Reply": "Yeah, the entire article is bullshit. I can represent 10 \\^ 1000 on 1 bit with a simple system: 0 -> 0, 1 -> 10\\^1000. The important question is not how large is the largest number that you can represent, but how many distinct numbers you can represent. You can always create a function that maps those distinct numbers to any other numbers.", "Reply Score": 11}]}, {"Comment": "Floating point has representations for infinity and negative infinity so I think infinity would beat their answer here", "Score": 19, "Replies": [{"Reply": "Their Turing machine can also represent infinity by looping forever.", "Reply Score": 4}, {"Reply": "Infinity isn\u2019t a number", "Reply Score": 1}]}, {"Comment": "9", "Score": 4, "Replies": []}, {"Comment": "I can do even better by interpreting the 64bit number n as representing the n'th n-hugh cardinal which is pretty large, actually far larger than these tiny BB numbers.", "Score": 3, "Replies": []}, {"Comment": "Yeah, the entire article is bullshit. I can represent 10 \\^ 1000 on 1 bit with a simple system: 0 -> 0, 1 -> 10\\^1000. The important question is not how large is the largest number that you can represent, but how many distinct numbers you can represent. You can always create a function that maps those distinct numbers to any other numbers.", "Score": 11, "Replies": []}, {"Comment": "Their Turing machine can also represent infinity by looping forever.", "Score": 4, "Replies": [{"Reply": "If it doesn't halt, does that count as a valid output of the machine?\n\nEdit: if so, theoretical computer science would like to have a word with you", "Reply Score": 10}]}, {"Comment": "Infinity isn\u2019t a number", "Score": 1, "Replies": [{"Reply": "It is in IEEE 754 floating point!", "Reply Score": 11}]}, {"Comment": "If it doesn't halt, does that count as a valid output of the machine?\n\nEdit: if so, theoretical computer science would like to have a word with you", "Score": 10, "Replies": [{"Reply": "We're making up the rules for this number system as we go along, it counts if you want it to count.", "Reply Score": 6}]}, {"Comment": "It is in IEEE 754 floating point!", "Score": 11, "Replies": [{"Reply": "Is it? Or is it just a special token that we've agreed represents infinity.", "Reply Score": 1}]}, {"Comment": "We're making up the rules for this number system as we go along, it counts if you want it to count.", "Score": 6, "Replies": []}, {"Comment": "Is it? Or is it just a special token that we've agreed represents infinity.", "Score": 1, "Replies": [{"Reply": "How is that distinct from any other token we agree to signify mathematical objects? Like \"0\" or \"\u211a\"", "Reply Score": 1}]}, {"Comment": "How is that distinct from any other token we agree to signify mathematical objects? Like \"0\" or \"\u211a\"", "Score": 1, "Replies": []}]},{"Title": "Adaptive Sampling For a Grid With Rough Estimates?", "Score": 1, "URL": "https://www.reddit.com/r/compsci/comments/184army/adaptive_sampling_for_a_grid_with_rough_estimates/", "CreatedAt": 1701005459.0, "Full Content": "Let's say you have a function that returns a response for a given X,Y coordinate, a grid of coordinates, and there are some known points that are roughly where you want to focus a search. What techniques would you use to create an adaptively-sampled grid? \n\nThis could either be coarse-to-fine and increase accuracy as you get closer to the points, or it could just be generating \"outward\" from the known points with a kind of falloff for the precision (e.g. one step away from the point, it's still relatively accurate, but as the cartesian distance from each point of interest increases, the estimates become more coarse)?\n\nI've tried quad-trees, but in this particular case, the regions of interests may fall near the edge of the trees and I don't want the heatmap to have any \"cutoffs\" when it crosses into another quadrant. I'm thinking there may be some kind of \"breadth-first\" approach where you start out from the gridpoint closest to the estimated X,Y coordinate and slowly increase the size of the region as you take more steps away from the initial estimate, but I forsee that being relatively complex, trying to group together gridpoints as the estimate gets more coarse.\n\nAny suggestions?", "CntComments": 7, "Comments": [{"Comment": "If the function being evaluated is really expensive, then a Bayesian optimization approach makes sense: fit some surrogate function on a grid, like a spline, and then adaptively sample points based on some notion of uncertainty. \n\nIn general, you may be interested in the literature on Gaussian processes. They let you easily go from a set of samples and some idea of how different function values are related (i.e., a distance function) to a mean and variance for any point. They are very principled and give good uncertainty estimates.\n\nIf function evaluation is really cheap, than I'd probably just think of it as a locally distorted grid. Perhaps you have some kind of gradient information or other knowledge that tells you where to focus your search, but a simple approach would be, say, a polar grid with density that is inversely proportional to the distance to the \"center\" you're sampling around. You can then use any interpolation algorithm you want to fill in the gaps.", "Score": 3, "Replies": [{"Reply": "Thank you!! This was definitely a huge help, I'll start reading up on Bayesian optimizations!", "Reply Score": 2}]}, {"Comment": "'Quad-tree' imlpies 2d.\n\nI would use Fourier approximation and refined this approximation with circular membrane harmonics or similar augmentation at points of interest.", "Score": 1, "Replies": []}, {"Comment": "Thank you!! This was definitely a huge help, I'll start reading up on Bayesian optimizations!", "Score": 2, "Replies": []}]},{"Title": "Packrat Parsing and Parsing Expression Grammars", "Score": 16, "URL": "https://shi.foo/weblog/packrat-parsing-and-parsing-expression-grammars", "CreatedAt": 1700921952.0, "Full Content": "", "CntComments": 1, "Comments": []},{"Title": "An idea regarding solving Data Hazards", "Score": 0, "URL": "https://www.reddit.com/r/compsci/comments/18472qc/an_idea_regarding_solving_data_hazards/", "CreatedAt": 1700991065.0, "Full Content": "At the software level, resolving data hazards by reordering instructions is a crucial optimization technique. In today's landscape, we benefit from chatbots that assist in solving programming challenges, offering convenience even if not always completely accurate. What if a similar approach were applied to instruction reordering?\n\nImagine implementing an Instruction Reordering Unit (IRU) within a processor, leveraging a feed-forward neural network (AI) to rearrange instructions intelligently. The primary goal? Ensuring simultaneous arrival of the rd and the Result, thus minimizing data hazards and enhancing result accuracy.\n\nHowever, the challenge lies in effectively training and validating this neural network. It's a complex task that involves exposing the network to diverse instruction sequences and their dependencies to ensure it can reorder instructions accurately across various scenarios and workloads.\n\nNow training the network to understand and predict these relationships as well as validating the neural network's performance across different processor architectures and workloads? That's another story.\n\nHowever, regardless of the challenges I see potential in this idea if it has not been implemented yet.", "CntComments": 16, "Comments": [{"Comment": "> However, the challenge lies in effectively training and validating this neural network.\n\nnot correct. training is the easy part - especially because you can generate tons of truthed data. the primary challenge is latency - how fast do you think instruction reordering needs to happen so that the perf gain doesn't wash out? i.e., avoiding a stall using your NN-IRU is more efficient than the stall and bubble and flush?\n\nit annoys me to no end that everyone loves these NN ideas because NNs are magic but no one has any clue about what their actual latency is (because it's all taken care of by frameworks). spoiler alert: any sizable net is going to be orders of magnitude too slow.\n\nthat being said, i know someone that was working on this for branch prediction. they were very small - he called them nanonets.", "Score": 11, "Replies": [{"Reply": "While I agree with you in principal about training being the easy part, I think OP is underestimating the fact that the context of what any instructions are actually doing is nuanced because they are related to the purpose of the code. Insuring changes won\u2019t break the result of a series of instructions is going to be difficult and making the correct changes to an algorithm to reduce port pressure and maximize instructions per cycle is likely not merely a simple reordering but rather a structural change to the algorithm.", "Reply Score": 2}, {"Reply": "A small neural network is still a neural network. You\u2019re making the mistake of comparing a neural network trained on the fly with an already trained neural network that\u2019s embedded in hardware. The former will take longer to train than to execute by several orders of magnitude.", "Reply Score": 1}, {"Reply": "Don't you think giving the AI the knowledge on how to re order instructions correctly is going to be difficult considering the chatbots we have which make countless mistakes? Here mistakes would be more costly\n\nAlso if latency is high, what about the final throughput? Can it still be high?", "Reply Score": -13}]}, {"Comment": "This is something OOE does partially. But I\u2019ve hand optimized a lot of x86-64 asm using Intel\u2019s latency analyzer and it is very hard to do without context of the individual algorithm you\u2019re implementing, because as the programmer you\u2019re the one who knows if you should keep a constant in a register because you\u2019ll need it again near the end of a long function.  Compilers tend to be short sighted, and while a modern C compiler isn\u2019t stupid it\u2019s not doing planning across the scope of a function with regard to the context of the algorithm.", "Score": 2, "Replies": [{"Reply": "In your experience, wouldn't a neural network be able to figure out the context of the algorithm you want to implement?\n\nEdit: grammar mistakes", "Reply Score": 1}]}, {"Comment": "While I agree with you in principal about training being the easy part, I think OP is underestimating the fact that the context of what any instructions are actually doing is nuanced because they are related to the purpose of the code. Insuring changes won\u2019t break the result of a series of instructions is going to be difficult and making the correct changes to an algorithm to reduce port pressure and maximize instructions per cycle is likely not merely a simple reordering but rather a structural change to the algorithm.", "Score": 2, "Replies": [{"Reply": "Knuth has already told us not to expect compilers to do these types of instruction reordering any time soon, and I tend to listen when Knuth gives advice. When somebody who makes the near impossible seem accessible comes along saying something is \"nearly impossible\" I don't hold out much hope.", "Reply Score": 2}]}, {"Comment": "A small neural network is still a neural network. You\u2019re making the mistake of comparing a neural network trained on the fly with an already trained neural network that\u2019s embedded in hardware. The former will take longer to train than to execute by several orders of magnitude.", "Score": 1, "Replies": [{"Reply": "> You\u2019re making the mistake\n\nI'm not making any mistakes - I'm a researcher that focuses on exactly this - I know exactly what I'm talking about.\n\n> The former will take longer to train than to execute by several orders of magnitude.\n\nNo one is talking about training here at all. So saying that training will take longer by even more orders of magnitude than an already untenable inference latency doesn't add anything to the conversation.", "Reply Score": 1}]}, {"Comment": "Don't you think giving the AI the knowledge on how to re order instructions correctly is going to be difficult considering the chatbots we have which make countless mistakes? Here mistakes would be more costly\n\nAlso if latency is high, what about the final throughput? Can it still be high?", "Score": -13, "Replies": []}, {"Comment": "In your experience, wouldn't a neural network be able to figure out the context of the algorithm you want to implement?\n\nEdit: grammar mistakes", "Score": 1, "Replies": [{"Reply": "You\u2019re talking about one model that is expected to do quite a few different things. It is unlikely you would approach what you are asking with a single model.  But rather a chain of systems designed to perform different stages.\n\nAnd also starting with existing assembly code may not be very the best approach anyway.  Focusing efforts towards a model capable of taking English and turning it into optimized SIMD assembly would be probably yield more gains.  This would require a lot of work and added validation steps though, it is not a trivial endeavor.", "Reply Score": 2}]}, {"Comment": "Knuth has already told us not to expect compilers to do these types of instruction reordering any time soon, and I tend to listen when Knuth gives advice. When somebody who makes the near impossible seem accessible comes along saying something is \"nearly impossible\" I don't hold out much hope.", "Score": 2, "Replies": []}, {"Comment": "> You\u2019re making the mistake\n\nI'm not making any mistakes - I'm a researcher that focuses on exactly this - I know exactly what I'm talking about.\n\n> The former will take longer to train than to execute by several orders of magnitude.\n\nNo one is talking about training here at all. So saying that training will take longer by even more orders of magnitude than an already untenable inference latency doesn't add anything to the conversation.", "Score": 1, "Replies": [{"Reply": "You said training is the easy part, so your response is patently false. Moving on\u2026", "Reply Score": 1}]}, {"Comment": "You\u2019re talking about one model that is expected to do quite a few different things. It is unlikely you would approach what you are asking with a single model.  But rather a chain of systems designed to perform different stages.\n\nAnd also starting with existing assembly code may not be very the best approach anyway.  Focusing efforts towards a model capable of taking English and turning it into optimized SIMD assembly would be probably yield more gains.  This would require a lot of work and added validation steps though, it is not a trivial endeavor.", "Score": 2, "Replies": []}, {"Comment": "You said training is the easy part, so your response is patently false. Moving on\u2026", "Score": 1, "Replies": [{"Reply": "who exactly are you talking to? me or yourself?\n\n> You said training is the easy part, so your response is patently false.\n\nin the real/professional world, the hard part of training isn't the time it takes because, spoiler-alert, we have lots of hardware at our disposal, but the quality of your data.", "Reply Score": 2}]}, {"Comment": "who exactly are you talking to? me or yourself?\n\n> You said training is the easy part, so your response is patently false.\n\nin the real/professional world, the hard part of training isn't the time it takes because, spoiler-alert, we have lots of hardware at our disposal, but the quality of your data.", "Score": 2, "Replies": [{"Reply": "Read the first two sentences of your response to the original poster. Your second fucking sentence: \u201cnot correct. training is the easy part\u2026\u201d You want to tell me again that you didn\u2019t say that shit?", "Reply Score": 2}, {"Reply": "I don\u2019t know if you\u2019re going to get through to him with the truth.", "Reply Score": 0}]}, {"Comment": "Read the first two sentences of your response to the original poster. Your second fucking sentence: \u201cnot correct. training is the easy part\u2026\u201d You want to tell me again that you didn\u2019t say that shit?", "Score": 2, "Replies": [{"Reply": "...but it is and you're wrong so what exactly are you getting at?\n\nedit: lol my guy got the last word by responding with \"lates\" (even though he's 45) and then blocking me.", "Reply Score": 1}]}, {"Comment": "I don\u2019t know if you\u2019re going to get through to him with the truth.", "Score": 0, "Replies": []}, {"Comment": "...but it is and you're wrong so what exactly are you getting at?\n\nedit: lol my guy got the last word by responding with \"lates\" (even though he's 45) and then blocking me.", "Score": 1, "Replies": [{"Reply": "You\u2019re such a poser and a liar. Lates.", "Reply Score": 2}, {"Reply": "In this thread you\u2019ve said both \u201ctraining is the easy part\u201d and \u201cno one is talking about training here at all\u201d lmao", "Reply Score": 1}]}, {"Comment": "You\u2019re such a poser and a liar. Lates.", "Score": 2, "Replies": []}, {"Comment": "In this thread you\u2019ve said both \u201ctraining is the easy part\u201d and \u201cno one is talking about training here at all\u201d lmao", "Score": 1, "Replies": []}]},{"Title": "What kind of algorithm would you design to create a map like this? To me this looks like a very interesting graph theory question.", "Score": 96, "URL": "https://i.redd.it/ljmtxyihf42c1.png", "CreatedAt": 1700776047.0, "Full Content": "", "CntComments": 21, "Comments": [{"Comment": "As a graph problem we would put weights on the nodes and given a list of ints/countries we want to partition the graph into connected components such that the weight of each component corresponds to one of the ints in our list. \n\nThere is a subset sum problem in the background, but if we assume there are enough small cities we have data on then greedy is nearly optimal. So probably just build out each component greedily, depending on your preference we should avoid holes. Rinse repeat, see how it looks, if it doesn\u2019t look good enough then do a local search.", "Score": 24, "Replies": [{"Reply": "What do you mean do a local search? It just dawned on me that this is indeed a series of subset sum problems. Do you mean using greedy to build up a set of satisfactory states/counties for each European country the using a modified subset sum solver to find a combination of them on the map?\n\nEdit: There's a minimum number of counties that can be used to represent each country (optimal). I just can't understand how to pair that with avoiding gaps in the map/overlap.", "Reply Score": 3}]}, {"Comment": "here is how I would tackle this.  \n\n\nFirst I would get a data set of the GDPs for the European countries and then US gdp broken down by both state and county.  I would sort the European GDP data set in decreasing order.  Then I would do the following algo for each European country.  \n\n\nUse a greedy algo to find a set of connected states whose total GDP is as close to the countries without going over.  Then for remaining GDP again use a greedy algo to keep adding bordering counties until you are within whatever tolerance you have determined appropriate.  For the edge case where the European countries GDP is smaller than any available state then pick the state which is closest to the countries GDP and then use a greedy algo to find a set of connected counties within that state which best matches the countries GDP.", "Score": 4, "Replies": [{"Reply": "So whenever counties are used from a certain state, we can subtract the used boundary counties from its total gdp and area and use the resulting values as a state's new data for future mapping. This makes a lot of sense but here's the question. The map seems to have close to 100% coverage of the provided area. There's obviously thousands of permutations of these maps. Do we just rinse and repeat till we find the most productive map or is there a more optimal implementation that takes into account cavities(gaps) forming when assigning areas? Like keeping track of closed gaps smaller than any possible GDP that can be assigned to them?", "Reply Score": 1}]}, {"Comment": "Just squeaked my way into Italy! TAKE THAT, HUNGARY!!!!!", "Score": 3, "Replies": []}, {"Comment": "I'd guess they started by finding cities in the US that match the cities in the EU, then do a greedy algorithm on counties that are connected to that city until it reaches the size of the EU economy +/- some percent.", "Score": 3, "Replies": []}, {"Comment": "Since when Morocco is a European country?", "Score": 3, "Replies": []}, {"Comment": "You could fill it in one by one I think. Keep expanding a country using BFS until it\u2019s big enough.\n\nRegardless, it is a strange map. Spain contains most of Texas, while Texas actually has a higher GDP than Spain. I think if you look carefully, they gave Houston to France, which accounts for like a quarter of Texas GDP.\nBecause European city names are plastered on the map for what I can tell only aesthetic and arbitrary reasons, it leaves out the American cities\u2026. a fun way to confuse people (and to make the map a lot less informative than it could be)", "Score": 2, "Replies": []}, {"Comment": "France's GDP is 2-3\u00d7 the GDP of Spain, why is Spain bigger", "Score": 2, "Replies": [{"Reply": "Not all counties have an equivalent gdp", "Reply Score": 5}]}, {"Comment": "Place a European country in an unclaimed random US county. Add counties to the country until target GDP is reached based on a rule like distance. Repeat for all EU countries. If a country is too big, you could try asking a surrounding country to give up a spot and try to take another spot on its border recursively.", "Score": 1, "Replies": []}, {"Comment": "The coloring part at least looks like a Constraint Satisfaction Problem.", "Score": 0, "Replies": [{"Reply": "^[Sokka-Haiku](https://www.reddit.com/r/SokkaHaikuBot/comments/15kyv9r/what_is_a_sokka_haiku/?utm_source=share&utm_medium=web2x&context=3) ^by ^shseham:\n\n*The coloring part*\n\n*At least looks like a Constraint*\n\n*Satisfaction Problem.*\n\n---\n^Remember ^that ^one ^time ^Sokka ^accidentally ^used ^an ^extra ^syllable ^in ^that ^Haiku ^Battle ^in ^Ba ^Sing ^Se? ^That ^was ^a ^Sokka ^Haiku ^and ^you ^just ^made ^one.", "Reply Score": 3}]}, {"Comment": "The main solution I see is to map the vertices onto a graphs.\n\nEstablish to two graphs (either directed or undirected) and perform a rotating walk between them. A given vertex, v\\_i, in graph G would correspond with a vertex, v\\_j in graph G'; the weight of one vertex will be equal/equivalent to the other. The vertices would maintain the relationships when performing a rotating walk.\n\nFrom here you can perform whatever operation you need to. I don't see the point of graph coloring or BFS/DFS in the context of what the information being represented.", "Score": 0, "Replies": []}, {"Comment": "I took the wooooooooook to Poland", "Score": 1, "Replies": []}, {"Comment": "This graphic is hugely misleading. The GDP of Europe is about half that of the US.", "Score": 1, "Replies": []}, {"Comment": "What do you mean do a local search? It just dawned on me that this is indeed a series of subset sum problems. Do you mean using greedy to build up a set of satisfactory states/counties for each European country the using a modified subset sum solver to find a combination of them on the map?\n\nEdit: There's a minimum number of counties that can be used to represent each country (optimal). I just can't understand how to pair that with avoiding gaps in the map/overlap.", "Score": 3, "Replies": [{"Reply": "By local search I mean doing \u201csmall changes\u201d such as doing swaps with neighbouring regions. \n\nI\u2019m saying that in practice for this application, we can pretty much ignore the subset sum aspect and solve it greedily. Subset sum type problems go through a phase transition when there are many small items where the problem is easy and greedy is actually optimal, so I\u2019m saying basically just ignore it. \n\nSimilarly if you keep adding in smaller countries you can fill up missing pieces. \n\nTo avoid holes we can leverage the fact that we already have a planar embedding, just keep seeding your components from cities one the edge. \n\nRealistically because what we are looking for is just something aesthetic rather than objective you\u2019re probably better off doing parts of it manually instead of using one algorithm out the box.", "Reply Score": 2}]}, {"Comment": "So whenever counties are used from a certain state, we can subtract the used boundary counties from its total gdp and area and use the resulting values as a state's new data for future mapping. This makes a lot of sense but here's the question. The map seems to have close to 100% coverage of the provided area. There's obviously thousands of permutations of these maps. Do we just rinse and repeat till we find the most productive map or is there a more optimal implementation that takes into account cavities(gaps) forming when assigning areas? Like keeping track of closed gaps smaller than any possible GDP that can be assigned to them?", "Score": 1, "Replies": []}, {"Comment": "Not all counties have an equivalent gdp", "Score": 5, "Replies": []}, {"Comment": "^[Sokka-Haiku](https://www.reddit.com/r/SokkaHaikuBot/comments/15kyv9r/what_is_a_sokka_haiku/?utm_source=share&utm_medium=web2x&context=3) ^by ^shseham:\n\n*The coloring part*\n\n*At least looks like a Constraint*\n\n*Satisfaction Problem.*\n\n---\n^Remember ^that ^one ^time ^Sokka ^accidentally ^used ^an ^extra ^syllable ^in ^that ^Haiku ^Battle ^in ^Ba ^Sing ^Se? ^That ^was ^a ^Sokka ^Haiku ^and ^you ^just ^made ^one.", "Score": 3, "Replies": [{"Reply": "Well done sokka bot", "Reply Score": 2}]}, {"Comment": "By local search I mean doing \u201csmall changes\u201d such as doing swaps with neighbouring regions. \n\nI\u2019m saying that in practice for this application, we can pretty much ignore the subset sum aspect and solve it greedily. Subset sum type problems go through a phase transition when there are many small items where the problem is easy and greedy is actually optimal, so I\u2019m saying basically just ignore it. \n\nSimilarly if you keep adding in smaller countries you can fill up missing pieces. \n\nTo avoid holes we can leverage the fact that we already have a planar embedding, just keep seeding your components from cities one the edge. \n\nRealistically because what we are looking for is just something aesthetic rather than objective you\u2019re probably better off doing parts of it manually instead of using one algorithm out the box.", "Score": 2, "Replies": [{"Reply": "Thank you!", "Reply Score": 2}]}, {"Comment": "Well done sokka bot", "Score": 2, "Replies": []}, {"Comment": "Thank you!", "Score": 2, "Replies": [{"Reply": ">Thank you!\n\nYou're welcome!", "Reply Score": 0}]}, {"Comment": ">Thank you!\n\nYou're welcome!", "Score": 0, "Replies": []}]},{"Title": "Cyber-physical decentralized planning for communizing", "Score": 2, "URL": "https://journals.sagepub.com/doi/full/10.1177/10245294231213141", "CreatedAt": 1700834779.0, "Full Content": "", "CntComments": 1, "Comments": []},{"Title": "Is there room for significant innovations in computer science, or has it reached its peak?", "Score": 0, "URL": "https://www.reddit.com/r/compsci/comments/1837ahk/is_there_room_for_significant_innovations_in/", "CreatedAt": 1700873682.0, "Full Content": "While numerous advancements are happening in computer science, they don't seem to be inherently disruptive. Do you believe significant innovations, such as compilers, interpreters, and operating systems, are still feasible?", "CntComments": 90, "Comments": [{"Comment": "Two answers:\n\n1. In the field of computer science, there plenty of advances to be made. It\u2019s sorta like saying \u201cwe\u2019ve found all the math\u201d. \nE.g. graph theory has a bunch of areas to explore still. \n\n2. In the field of software engineering (which is confused for CS a lot), there are plenty \u201cdisruptions\u201d to be made. SWE is sorta the \u201cscience of abstractions\u201d, so if something can be abstracted, there\u2019s progress to be made. \n\nE.g. kubernetes has \u201cabstracted\u201d away the datacenter,  from application deployment, \u201ctreating\u201d clusters like a singular operating system.", "Score": 61, "Replies": [{"Reply": "On point #2 I think it's more a matter of removing accidental complexity (ala Fred Brooks).  I think we have lots more accidental complexities which can be removed.", "Reply Score": 2}]}, {"Comment": "the field is so young, we are still in baby shoes. I mean don\u2019t you see how fast advancements happen. new breakthroughs every year. So many problems aren\u2019t solved or at least not efficient enough", "Score": 30, "Replies": [{"Reply": "[deleted]", "Reply Score": -29}]}, {"Comment": "We haven't even solved P=NP\n\nThat's some pretty fundamental stuff we have no clue about.", "Score": 24, "Replies": [{"Reply": "there\u2019s a whole lot of seemingly much \u201ceasier\u201d unsolved problems in complexity theory than P=NP", "Reply Score": 18}, {"Reply": "We don't even know if it's fundamentally possible\n\nWe just hope it is since it would make us happier", "Reply Score": 3}, {"Reply": "Well, it\u2019s fundamental in the sense that it\u2019s the foundation upon which our understanding of algorithm optimality is built, but our inability to determine its truth is by no means an indicator of the immaturity of the field. I\u2019m sure you\u2019d agree that our mathematical expertise is quite mature - and yet nowhere near mature enough for us to crack this problem.", "Reply Score": 1}]}, {"Comment": "Ah, have you missed the last two or three years of AI and what's happened with GPUs?\n\nSpecific to compilers, we're still seeing pretty damn good efficiency wins.\n\nFor operating systems, well, quantum computing and cloud-native OS, plus things like docker/kubernetes.  \n\nOr uh, yes, there's always room.  And it rarely looks like there's more room until there was.", "Score": 12, "Replies": []}, {"Comment": "Considering generative AI just took off like yesterday, there is still much to be conquered.", "Score": 7, "Replies": []}, {"Comment": "The field hasn't been around for very long.  Field like civil engineering go back to Roman times.  There is a lot more coming, even if you can't imagine it.", "Score": 4, "Replies": []}, {"Comment": "Not purely computer science. I recently saw A. Karpathy discussing about LLM-OS.\n\nhttps://youtu.be/zjkBMFhNj_g?si=krCPwieFfxFjAT3i&t=2542", "Score": 6, "Replies": []}, {"Comment": "This is like asking if science has peaked. No, it hasn\u2019t. If you believe this, you\u2019ve stopped understanding the concept.", "Score": 3, "Replies": [{"Reply": "Some sciences have indeed peaked, its a hard to believe concept, but just take a quick look at Physics to see that the progress of significant discoveries has slowed down quite a bit.  \n\n\nIt is quite possible that the growth of science isn't exponential but rather logarithmic, and Physics being one of the oldest and most mature branch is definitely showcasing this.", "Reply Score": 2}]}, {"Comment": "A lot of enhancement in the compiler/proogramming languages but still there are bugs. More than 5 decades of research in the area that gurantees if the program behaves as per the developer's intent.\nIn a nutshell, I am talking about formal verification of correctness of program.\n\nThere are some programming languages developed in the last decade like Dafny, F*, P#, TLA+ which can prove a program that is totally matching the specification.\n\nThese languages are very complex to use. It is comparatively easy while proving declarative code but very hard to prove imparative code also consumes a lot of developer's time along with computational resources.", "Score": 2, "Replies": [{"Reply": "Look at recent research projects such as RefinedC as well, which are not so complex to use. In this case the specification language is embedded into your C code, using the existing `[[annotation]]` syntax. Perhaps I'm kidding myself but it seems more declarative when types are used to automate a proof search, rather than feeling like I'm just writing my algorithm at a higher level than my programming language.", "Reply Score": 2}]}, {"Comment": "It's just the beginning.  Neural networks, Machine Learning classifiers. Distributed processing, TPU pods with over 2k cores, IoT, personal area networks, sensors and more home automation. Gen AI code generators, Gen AI adversarial Gen AI/detectors.\n\nAnd that is just what I could think of in 2 min...", "Score": 4, "Replies": [{"Reply": "[deleted]", "Reply Score": -5}]}, {"Comment": "Hell Yes.\n\nMy friend writes induction based program synthesis technologies and they are insane.\n\nIt automatically writes any length program by splitting the problem down the axis (executing the next instruction) that most reduces the problems remaining entropy.\n\nIt can do crazy things like reverse SHA, outcompress PNG and MP4 and instantly generate correct classifiers (eg, is this image a cat or a dog)\n\nand his algorithm is < 200 lines of code!!!, He doesnt like me giving away all the details but if you have any questions ill can give you answers.\n\nYou should see my path finders and renderers etc.\n\nComputer science is just getting started!, Peace.", "Score": 3, "Replies": [{"Reply": "[These claims require a meme, imo.](https://giphy.com/gifs/anchorman-lies-lie-EouEzI5bBR8uk)", "Reply Score": 7}, {"Reply": "[deleted]", "Reply Score": 3}, {"Reply": "But will it tell me if my code halts?", "Reply Score": 3}, {"Reply": ">It automatically writes any length program by splitting the problem down the axis (executing the next instruction) that most reduces the problems remaining entropy.\n\nSounds like your friend has rediscovered decision trees", "Reply Score": 1}]}, {"Comment": "[Here, the answer to your questions...](http://www.loper-os.org/?p=46)", "Score": 1, "Replies": [{"Reply": "Drivel.", "Reply Score": 0}]}, {"Comment": "Theoretical CS is still stuck on whether P=NP and has been for decades. \n\nOr are you really asking about innovations in software engineering? It's not the same as CS.", "Score": -3, "Replies": [{"Reply": "Theoretical CS has a lot more than P = NP\n\nThere's a lot more work on Proof Assistants, Boolean-SAT and SMT solvers since then. \n\nCategory/Type theory and Graph Theory have a lot of ongoing work. (Other definitely do too given the number of conferences, but I shan't speak of what I'm unfamiliar with)\n\nPolyhedral compilation is another - while the compiler itself is arguably software engineering - the math around polyhedral transformation is strongly within graphs and computational geometry.\n\nThere's the whole thing with program equivalences and all that too, which is also heavily graph based.\n\nAs always, over time a lot of work becomes incremental instead of radical. We're improving small operations on established theory instead of establishing theory - but that's the nature of research. Eventually the horizons are wide, but far away from those that aren't actively pursuing it.", "Reply Score": 7}, {"Reply": "Mathematics as a whole is still stuck on whether P=NP, but we wouldn\u2019t use this as an indicator to suggest our mathematical understanding is at its infancy.", "Reply Score": 5}, {"Reply": "P vs NP is the *newest* Millennium prize problem. It is less than 60 years old (as is the Yang-Mills existence and mass gap problem, but that doesn't sound as good on TV). It is *fantastically* young as far as unsolved problems go. Using it as an example to show that it's \"stuck\" is shockingly ignorant of not just computer science, but basically the entirety of science/mathematics.", "Reply Score": 5}]}, {"Comment": "Not much has happened in theoretical CS since, like, the 80's. It barely got started.", "Score": -6, "Replies": [{"Reply": "I do not know who told you this, but this is simply not true.\n\nYou do not even have to be an expert on TCS to see the amount of work done. For example, essentially all work on (theoretical) quantum computing (such as Shor's algorithm) are newer than the 80s. And that is just the very surface level stuff, that even reaches a general audience.", "Reply Score": 1}]}, {"Comment": "When the bridge between natural and synthetic systems connects at scale there will be an explosion of new fields emerge", "Score": -1, "Replies": []}, {"Comment": "[deleted]", "Score": -27, "Replies": [{"Reply": "We understand literally almost nothing about computation. I know this might seem bizarre because as you say, you can go buy computers and calculate with them. However, computer science is a field of mathematics meant to answer questions about computation and information, and we are still in the era of not being able to answer extremely, extremely simple questions. Questions you can ask a child, we can't answer. Same situation as say some other fields of mathematics that are hundreds of years old, yet we do not have the understanding required to answer the most rudimentary of questions. Consider the Collatz conjecture, you can ask children the question, yet some of the smartest folks on earth can say confidently \"we probably are not advanced enough to answer these types of questions with mathematics at this time.\" https://www.youtube.com/watch?v=5mFpVDpKX70\n\nI'm not downvoting you btw, I consider your comment a good starting point for further, extremely fun, investigation of CS.", "Reply Score": 9}, {"Reply": "What do you mean by solved?\n\n>Most architectures have been solved before by someone smart.\n\nwhat do you mean with that?\n\n>We\u2019re about at the peak.\n\nwhat?\n\n>The math to make machine learning work is already mostly there.\n\nYou are getting down voted because either you don't know what you are talking about or you are not explaining yourself very well.", "Reply Score": 8}, {"Reply": "You\u2019re getting downvoted because you\u2019re only talking about computer hardware and seem to be confusing computer science with computer engineering.\n\nAlso what do you mean by the math to make machine learning work is almost there?", "Reply Score": 7}]}, {"Comment": "AI is the significant innovation happening right now.", "Score": 1, "Replies": []}, {"Comment": "Think of it as you don\u2019t know what you don\u2019t know. We couldn\u2019t fly until we did. Every time we think we are at our peak we find something else.", "Score": 1, "Replies": []}, {"Comment": "Imagine aliens are real for five minutes then get back to work", "Score": 1, "Replies": []}, {"Comment": "You ask this question while we're literally in the age of parallel programming and new parallel algorithms are being released rapidly. The GPU is more than just video games-it's fundamentally changing computing. In addition, we also have more and more access to the cloud which also relies on massive parallel computation.  \n\n\nThe field is so rapidly progressing, that almost everything you learned in school, or from a book is likely outdated. Even if that book was 4 years ago.", "Score": 1, "Replies": []}, {"Comment": "we are in its infancy", "Score": 1, "Replies": []}, {"Comment": "You think LLMs aren\u2019t disruptive?", "Score": 1, "Replies": []}, {"Comment": "I find the question genuinely odd given the wild pace of innovation in the field. This is a young field. The answer about whether computer science has \"peaked\" is an obvious and resounding \"no.\"", "Score": 1, "Replies": []}, {"Comment": ">they don't seem to be inherently disruptive\n\nTwo views:\n\n1. If you don't think advances in AI have been and continue to be disruptive, we might disagree about the meaning of that word. Same goes for blockchain, augmented reality, etc.\n2. Disruption is not inherently good. It tends to happen when some old model or way of thinking stops being as useful as it once was, and a better model emerges. That's exciting, because it creates lots of opportunity to look at every aspect of the field through the lens of the new model, and often leads to more innovation. But a lack of disruption isn't unhealthy \u2014 it may simply mean that the current model is largely correct.\n\n&#x200B;\n\n>Do you believe significant innovations, such as compilers, interpreters, and operating systems, are still feasible?\n\nSignificant innovations are always feasible, but they might or might not be in any of the areas you've listed.", "Score": 1, "Replies": []}, {"Comment": "On point #2 I think it's more a matter of removing accidental complexity (ala Fred Brooks).  I think we have lots more accidental complexities which can be removed.", "Score": 2, "Replies": [{"Reply": "Maybe?\nMoving from gates to circuits to machine code to assembly to a higher language to a container to a container orchestration isn\u2019t exactly accidental complexity. I feel the abstraction is necessary and not removing accidental complexity. \n\nComplexity is a serious concern though, with Ousterhout\u2019s book on A Philosophy of Software Design a treatise on modern software engineering and the fight against complexity. \n\nI would even argue that eliminating accidental (and intentional) complexity a driving factor in the creation of Go at Google.", "Reply Score": 3}]}, {"Comment": "[deleted]", "Score": -29, "Replies": [{"Reply": "there are so many that i don\u2019t know which to mention. Just a random one for example is research in ML based solvers for NP hard problems. Humanoid robots are not even close to mimic human hands and coordination. They struggle with foldable things like paper. Quantum computing is the obvious one. DNA hard drives is another one because there has to be an advancement with all the data we get nowadays. iirc a sssp problem that i had in my class was just solved 2 years ago or something. New techniques on Mips/integer programming, explainable and robust ai\u2026 i mean look at the news, ai is just getting started. So much to be done i have no idea how people think we are at the peak when we litteraly just started out", "Reply Score": 14}]}, {"Comment": "there\u2019s a whole lot of seemingly much \u201ceasier\u201d unsolved problems in complexity theory than P=NP", "Score": 18, "Replies": [{"Reply": "Where\u2019s the money at on the [Unique Games Conjecture ](https://en.wikipedia.org/wiki/Unique_games_conjecture) these days?", "Reply Score": 1}, {"Reply": "Any P=PSPACE fans?", "Reply Score": 1}]}, {"Comment": "We don't even know if it's fundamentally possible\n\nWe just hope it is since it would make us happier", "Score": 3, "Replies": [{"Reply": "> it would make us happier\n\nIf P=NP I suspect we will go extinct not so long after understanding why. I do not believe human beings are remotely equipped for that eventuality. Instead of a \"force multiplier\" it'd be more appropriate to consider that type of knowledge a \"force exponentiator.\"", "Reply Score": 2}]}, {"Comment": "Well, it\u2019s fundamental in the sense that it\u2019s the foundation upon which our understanding of algorithm optimality is built, but our inability to determine its truth is by no means an indicator of the immaturity of the field. I\u2019m sure you\u2019d agree that our mathematical expertise is quite mature - and yet nowhere near mature enough for us to crack this problem.", "Score": 1, "Replies": [{"Reply": "Precisely. And as long as we still have such questions open there will still be plenty to learn", "Reply Score": 1}]}, {"Comment": "Some sciences have indeed peaked, its a hard to believe concept, but just take a quick look at Physics to see that the progress of significant discoveries has slowed down quite a bit.  \n\n\nIt is quite possible that the growth of science isn't exponential but rather logarithmic, and Physics being one of the oldest and most mature branch is definitely showcasing this.", "Score": 2, "Replies": []}, {"Comment": "Look at recent research projects such as RefinedC as well, which are not so complex to use. In this case the specification language is embedded into your C code, using the existing `[[annotation]]` syntax. Perhaps I'm kidding myself but it seems more declarative when types are used to automate a proof search, rather than feeling like I'm just writing my algorithm at a higher level than my programming language.", "Score": 2, "Replies": [{"Reply": "https://gitlab.mpi-sws.org/iris/refinedc/-/blob/master/examples/mpool.c", "Reply Score": 2}]}, {"Comment": "[deleted]", "Score": -5, "Replies": [{"Reply": "That and more training data, more parameters, more hardware for training - practical advancements, but hardly theoretical changes", "Reply Score": 1}, {"Reply": "I am not sure if this is really true - it kind of depends on what you call recent advancements. \n\nThe transformer architecture is from is 2017 - 6 years ago. That is absolutely recent. And it would be bold to say that the transformer architecture was not \"disruptive\".", "Reply Score": 1}]}, {"Comment": "[These claims require a meme, imo.](https://giphy.com/gifs/anchorman-lies-lie-EouEzI5bBR8uk)", "Score": 7, "Replies": [{"Reply": "\u201cReverse SHA\u201d \n*side eye*", "Reply Score": 3}, {"Reply": "Jaja yeah fair position.\n\nThe trick is to convert the problem to a many to one mappings task, all the possible output bits have their address encoded into the input so you can ask for each one while the whole system remains many to one.\n\nThe other trick is to make your instructions/questions always successful, so  for example if your only option is to ask about an unknown input bit and once you've asked them all then you can definitely produce an output bit then you know you can't go wrong (even a randomly written program will always work correctly in this language) then the task now reduces to just asking reasonable questions / emitting reasonable instructions and the guide we use is entropy.\n\nThe way we count this is simply the number of combinations of one and zeros added together down both remaining sides.\n\nAnyway it's a brain fuck but it works \ud83d\ude09\n\nPeace", "Reply Score": 0}]}, {"Comment": "[deleted]", "Score": 3, "Replies": [{"Reply": "Umm, many to one mappings,\n\nGroup entropy estimation,\n\nBinary decision Forrests.\n\nI've never seen a nothing quite like it myself.", "Reply Score": 0}]}, {"Comment": "But will it tell me if my code halts?", "Score": 3, "Replies": [{"Reply": "Hehe not as far as I know but for those familiar with kaugnaugh complexity there might be a way for some class of problems \ud83d\ude09", "Reply Score": 2}]}, {"Comment": ">It automatically writes any length program by splitting the problem down the axis (executing the next instruction) that most reduces the problems remaining entropy.\n\nSounds like your friend has rediscovered decision trees", "Score": 1, "Replies": [{"Reply": "Yeah he doesn't seem to think of them as too special.\n\nBut the fact that it beats so many other larger older more specialized systems seems special to me \ud83d\ude09\n\nGeniuses seem to think the rest of us are all geniuses aswell \ud83d\ude02", "Reply Score": 1}]}, {"Comment": "Drivel.", "Score": 0, "Replies": []}, {"Comment": "Theoretical CS has a lot more than P = NP\n\nThere's a lot more work on Proof Assistants, Boolean-SAT and SMT solvers since then. \n\nCategory/Type theory and Graph Theory have a lot of ongoing work. (Other definitely do too given the number of conferences, but I shan't speak of what I'm unfamiliar with)\n\nPolyhedral compilation is another - while the compiler itself is arguably software engineering - the math around polyhedral transformation is strongly within graphs and computational geometry.\n\nThere's the whole thing with program equivalences and all that too, which is also heavily graph based.\n\nAs always, over time a lot of work becomes incremental instead of radical. We're improving small operations on established theory instead of establishing theory - but that's the nature of research. Eventually the horizons are wide, but far away from those that aren't actively pursuing it.", "Score": 7, "Replies": []}, {"Comment": "Mathematics as a whole is still stuck on whether P=NP, but we wouldn\u2019t use this as an indicator to suggest our mathematical understanding is at its infancy.", "Score": 5, "Replies": [{"Reply": "If this is not indicative that our understanding of mathematics is at its infancy it absolutely is indicative that our understanding of mathematics is not even \"pubescent\". In most fields of study we are unable to answer what seem to be absurdly rudimentary questions. Not just, \"we don't know a proof yet,\" but rather the tools available at least seem inadequate.", "Reply Score": 1}]}, {"Comment": "P vs NP is the *newest* Millennium prize problem. It is less than 60 years old (as is the Yang-Mills existence and mass gap problem, but that doesn't sound as good on TV). It is *fantastically* young as far as unsolved problems go. Using it as an example to show that it's \"stuck\" is shockingly ignorant of not just computer science, but basically the entirety of science/mathematics.", "Score": 5, "Replies": [{"Reply": "Cool story, bro.", "Reply Score": -4}]}, {"Comment": "I do not know who told you this, but this is simply not true.\n\nYou do not even have to be an expert on TCS to see the amount of work done. For example, essentially all work on (theoretical) quantum computing (such as Shor's algorithm) are newer than the 80s. And that is just the very surface level stuff, that even reaches a general audience.", "Score": 1, "Replies": []}, {"Comment": "We understand literally almost nothing about computation. I know this might seem bizarre because as you say, you can go buy computers and calculate with them. However, computer science is a field of mathematics meant to answer questions about computation and information, and we are still in the era of not being able to answer extremely, extremely simple questions. Questions you can ask a child, we can't answer. Same situation as say some other fields of mathematics that are hundreds of years old, yet we do not have the understanding required to answer the most rudimentary of questions. Consider the Collatz conjecture, you can ask children the question, yet some of the smartest folks on earth can say confidently \"we probably are not advanced enough to answer these types of questions with mathematics at this time.\" https://www.youtube.com/watch?v=5mFpVDpKX70\n\nI'm not downvoting you btw, I consider your comment a good starting point for further, extremely fun, investigation of CS.", "Score": 9, "Replies": []}, {"Comment": "What do you mean by solved?\n\n>Most architectures have been solved before by someone smart.\n\nwhat do you mean with that?\n\n>We\u2019re about at the peak.\n\nwhat?\n\n>The math to make machine learning work is already mostly there.\n\nYou are getting down voted because either you don't know what you are talking about or you are not explaining yourself very well.", "Score": 8, "Replies": []}, {"Comment": "You\u2019re getting downvoted because you\u2019re only talking about computer hardware and seem to be confusing computer science with computer engineering.\n\nAlso what do you mean by the math to make machine learning work is almost there?", "Score": 7, "Replies": [{"Reply": "[deleted]", "Reply Score": 0}]}, {"Comment": "Maybe?\nMoving from gates to circuits to machine code to assembly to a higher language to a container to a container orchestration isn\u2019t exactly accidental complexity. I feel the abstraction is necessary and not removing accidental complexity. \n\nComplexity is a serious concern though, with Ousterhout\u2019s book on A Philosophy of Software Design a treatise on modern software engineering and the fight against complexity. \n\nI would even argue that eliminating accidental (and intentional) complexity a driving factor in the creation of Go at Google.", "Score": 3, "Replies": [{"Reply": ">I feel the abstraction is necessary \n\nWhat if you could program without abstraction; directly setting switches by the billions? \n\nObviously this is impossible to do manually, but this is exactly how neural networks are trained with optimization.\n\n Optimizers don't care about complexity because they don't need to understand what they are doing - just lower the loss. This lets you do things we have no clue how to do with the traditional software approach of stacking abstractions on top of abstractions.", "Reply Score": 1}, {"Reply": "I really enjoyed Ousterhout's book.  I need to re-read it--lots of good ideas in that book.", "Reply Score": 1}]}, {"Comment": "there are so many that i don\u2019t know which to mention. Just a random one for example is research in ML based solvers for NP hard problems. Humanoid robots are not even close to mimic human hands and coordination. They struggle with foldable things like paper. Quantum computing is the obvious one. DNA hard drives is another one because there has to be an advancement with all the data we get nowadays. iirc a sssp problem that i had in my class was just solved 2 years ago or something. New techniques on Mips/integer programming, explainable and robust ai\u2026 i mean look at the news, ai is just getting started. So much to be done i have no idea how people think we are at the peak when we litteraly just started out", "Score": 14, "Replies": [{"Reply": "[deleted]", "Reply Score": -18}]}, {"Comment": "Where\u2019s the money at on the [Unique Games Conjecture ](https://en.wikipedia.org/wiki/Unique_games_conjecture) these days?", "Score": 1, "Replies": []}, {"Comment": "Any P=PSPACE fans?", "Score": 1, "Replies": []}, {"Comment": "> it would make us happier\n\nIf P=NP I suspect we will go extinct not so long after understanding why. I do not believe human beings are remotely equipped for that eventuality. Instead of a \"force multiplier\" it'd be more appropriate to consider that type of knowledge a \"force exponentiator.\"", "Score": 2, "Replies": [{"Reply": "I think more likely we prove that hard problems are in P but with time in something like O(n^(Graham\u2019s number)) or the constant is similarly astronomical. \u201cClassify all graphs on less than X nodes first then...\u201d for X so absurdly large it might as well be impossible for any scales we expect our universe to exist for.", "Reply Score": 5}]}, {"Comment": "Precisely. And as long as we still have such questions open there will still be plenty to learn", "Score": 1, "Replies": []}, {"Comment": "https://gitlab.mpi-sws.org/iris/refinedc/-/blob/master/examples/mpool.c", "Score": 2, "Replies": []}, {"Comment": "That and more training data, more parameters, more hardware for training - practical advancements, but hardly theoretical changes", "Score": 1, "Replies": []}, {"Comment": "I am not sure if this is really true - it kind of depends on what you call recent advancements. \n\nThe transformer architecture is from is 2017 - 6 years ago. That is absolutely recent. And it would be bold to say that the transformer architecture was not \"disruptive\".", "Score": 1, "Replies": []}, {"Comment": "\u201cReverse SHA\u201d \n*side eye*", "Score": 3, "Replies": []}, {"Comment": "Jaja yeah fair position.\n\nThe trick is to convert the problem to a many to one mappings task, all the possible output bits have their address encoded into the input so you can ask for each one while the whole system remains many to one.\n\nThe other trick is to make your instructions/questions always successful, so  for example if your only option is to ask about an unknown input bit and once you've asked them all then you can definitely produce an output bit then you know you can't go wrong (even a randomly written program will always work correctly in this language) then the task now reduces to just asking reasonable questions / emitting reasonable instructions and the guide we use is entropy.\n\nThe way we count this is simply the number of combinations of one and zeros added together down both remaining sides.\n\nAnyway it's a brain fuck but it works \ud83d\ude09\n\nPeace", "Score": 0, "Replies": [{"Reply": "Do you have any idea how much money a person could make doing bitcoin mining if they could \"reverse sha\"???  Billionaire in no time at all.  Your friend definitely can NOT \"reverse sha\"....", "Reply Score": 1}]}, {"Comment": "Umm, many to one mappings,\n\nGroup entropy estimation,\n\nBinary decision Forrests.\n\nI've never seen a nothing quite like it myself.", "Score": 0, "Replies": []}, {"Comment": "Hehe not as far as I know but for those familiar with kaugnaugh complexity there might be a way for some class of problems \ud83d\ude09", "Score": 2, "Replies": []}, {"Comment": "Yeah he doesn't seem to think of them as too special.\n\nBut the fact that it beats so many other larger older more specialized systems seems special to me \ud83d\ude09\n\nGeniuses seem to think the rest of us are all geniuses aswell \ud83d\ude02", "Score": 1, "Replies": []}, {"Comment": "If this is not indicative that our understanding of mathematics is at its infancy it absolutely is indicative that our understanding of mathematics is not even \"pubescent\". In most fields of study we are unable to answer what seem to be absurdly rudimentary questions. Not just, \"we don't know a proof yet,\" but rather the tools available at least seem inadequate.", "Score": 1, "Replies": [{"Reply": "If you believe that P = NP is so rudimentary as to speak for how little we know about mathematics, then so be it.  \n\n\nI'm of the opinion that it's a problem so absurdly difficult, despite its ubiquity, that our inability to solve it says nothing about whether or not math and computer science are mature fields. We don't say Michael Phelps is a bad swimmer just because he cannot cross the Pacific, we instead acknowledge that some challenges are beyond stratospherically hard.", "Reply Score": 2}]}, {"Comment": "Cool story, bro.", "Score": -4, "Replies": []}, {"Comment": "[deleted]", "Score": 0, "Replies": [{"Reply": "I appreciate you elaborating on that. Computer science is made up of so many different topics with research opportunity. I\u2019m sure you\u2019re right with that point, it just doesn\u2019t meet the scope of what was asked", "Reply Score": 1}]}, {"Comment": ">I feel the abstraction is necessary \n\nWhat if you could program without abstraction; directly setting switches by the billions? \n\nObviously this is impossible to do manually, but this is exactly how neural networks are trained with optimization.\n\n Optimizers don't care about complexity because they don't need to understand what they are doing - just lower the loss. This lets you do things we have no clue how to do with the traditional software approach of stacking abstractions on top of abstractions.", "Score": 1, "Replies": [{"Reply": "Aren\u2019t neural nets the biggest abstraction of all because we can\u2019t even \u201csee\u201d what they are doing on the inside?", "Reply Score": 2}]}, {"Comment": "I really enjoyed Ousterhout's book.  I need to re-read it--lots of good ideas in that book.", "Score": 1, "Replies": []}, {"Comment": "[deleted]", "Score": -18, "Replies": [{"Reply": "I want to strongly contest what you say about Quantum Computing: Firstly, even if it were useless, it is essentially a new way of thinking about what a computation is and how circuits work! That's like saying Quantum Physics has few use cases because it doesn't speed up most production: It's a wondrous mathematical world full of counterintuitive and perplexing results that we will study for decades to come.\n\nAnd secondly, that paragraph was just for emphasis: If we were to apply quantum computing today, it would bring a relevant speedup to many problems that seem like magic from a classical POV (Look up Grover's Search Algorithm: Searching unstructured data in sqrt(n)!), a complete rethinking of cryptography and what constitutes a 'hard enough' problem and more, I can't really imagine how you would read that Wikipedia article and not see how those known algorithms would change CS. \n\nAnd, obviously, one of the most worked on things in QC is finding new quantum algorithms! We know that they can have an advantage over classical ones so there are people trying to solve all kinds of problems super efficiently using QC. It's getting more relevant and pressing month by month.", "Reply Score": 9}, {"Reply": "> Quantum computing sounds fancy, but any time I look into it, its use cases sound very minimal. Maybe we\u2019ll have stronger encryption? It doesn\u2019t sound life changing though: https://en.m.wikipedia.org/wiki/Quantum_algorithm\n\nElectricity also seemed like it had very minimal uses. Burn things, heat things, maybe if you got really fancy you could make purple plasma arcs with it or make someone's hair stick up from their head.\n\nWithout understanding it, we would never even think of using electromagnetic waves to transmit information and make sensors to learn more about the world around us", "Reply Score": 5}, {"Reply": "re:DNA hard drives, just consider generically the differences between our most advanced storage devices and what known polymers are capable of. Polymer based storage could be thousands of times more information dense, as well as thousands of times more durable. DNA might not be that polymer (eg, due to the speed of polymerase), but the fact that polymers are able to do this at all is similar fortune as winning the lottery IMO.", "Reply Score": 3}, {"Reply": "Every problem you mentioned with these is a research gap :)", "Reply Score": 1}]}, {"Comment": "I think more likely we prove that hard problems are in P but with time in something like O(n^(Graham\u2019s number)) or the constant is similarly astronomical. \u201cClassify all graphs on less than X nodes first then...\u201d for X so absurdly large it might as well be impossible for any scales we expect our universe to exist for.", "Score": 5, "Replies": [{"Reply": "You don\u2019t even have to go that far. Even something like O(n^2) grows quick enough to be a serious issue in practice", "Reply Score": 6}, {"Reply": "Really? I'm not a cs theory person, but P=NP would collapse the polynomial hierarchy and cause a whole bunch of weird theory implications in fields I do have more familiarity with. I find it far more likely the Strong Exponential Time hypothesis is true. Are there equally weird implications for a negative result?", "Reply Score": 1}]}, {"Comment": "Do you have any idea how much money a person could make doing bitcoin mining if they could \"reverse sha\"???  Billionaire in no time at all.  Your friend definitely can NOT \"reverse sha\"....", "Score": 1, "Replies": [{"Reply": "yeah we talk about that.\n\nSigning fake blocks would tank the currency but you could certainly use it to 'improve' your mining efficiency arbitrarily.\n\nHe showed me how to do that (it involves walking down the nodes of a sha modeling tree until you hit a certain kind of leaf node) this lets you basically 'walk' towards nonce values which will sha with ur block to produce values starting with the desired number of zeros. (again producing just one 'gold' block with too many zeros at the start would show existence of control over sha exists and tank the currency, so its a delicate game)\n\nAnyways thankfully geniuses like him are too busy because god knows what damage they could do if extortion was their motive.\n\nPeace,", "Reply Score": 1}]}, {"Comment": "If you believe that P = NP is so rudimentary as to speak for how little we know about mathematics, then so be it.  \n\n\nI'm of the opinion that it's a problem so absurdly difficult, despite its ubiquity, that our inability to solve it says nothing about whether or not math and computer science are mature fields. We don't say Michael Phelps is a bad swimmer just because he cannot cross the Pacific, we instead acknowledge that some challenges are beyond stratospherically hard.", "Score": 2, "Replies": []}, {"Comment": "I appreciate you elaborating on that. Computer science is made up of so many different topics with research opportunity. I\u2019m sure you\u2019re right with that point, it just doesn\u2019t meet the scope of what was asked", "Score": 1, "Replies": []}, {"Comment": "Aren\u2019t neural nets the biggest abstraction of all because we can\u2019t even \u201csee\u201d what they are doing on the inside?", "Score": 2, "Replies": [{"Reply": "This is why we can't understand what they're doing on the inside.  Complexity is incomprehensible without abstractions to provide structure.\n\nSo we surround them with another abstraction - a black box - in order to fit them into our existing software.", "Reply Score": 0}, {"Reply": "Obfuscation is not the same as abstraction.\n\nNeural networks excel in generality.", "Reply Score": 0}]}, {"Comment": "I want to strongly contest what you say about Quantum Computing: Firstly, even if it were useless, it is essentially a new way of thinking about what a computation is and how circuits work! That's like saying Quantum Physics has few use cases because it doesn't speed up most production: It's a wondrous mathematical world full of counterintuitive and perplexing results that we will study for decades to come.\n\nAnd secondly, that paragraph was just for emphasis: If we were to apply quantum computing today, it would bring a relevant speedup to many problems that seem like magic from a classical POV (Look up Grover's Search Algorithm: Searching unstructured data in sqrt(n)!), a complete rethinking of cryptography and what constitutes a 'hard enough' problem and more, I can't really imagine how you would read that Wikipedia article and not see how those known algorithms would change CS. \n\nAnd, obviously, one of the most worked on things in QC is finding new quantum algorithms! We know that they can have an advantage over classical ones so there are people trying to solve all kinds of problems super efficiently using QC. It's getting more relevant and pressing month by month.", "Score": 9, "Replies": [{"Reply": "Do you think (general) quantum computers would be a meaningful thing in our lifetime?\n\nFor me, they seems like fusion reactors, they are always 30 years away. Like [the biggest number that have been factored by Shor algorithm is 21](https://en.wikipedia.org/wiki/Integer_factorization_records#Records_for_efforts_by_quantum_computers), and even that was more than 10 years ago.\n\nMaybe quantum key-distributions will become a thing, but that is more quantum physics than CS.", "Reply Score": 1}]}, {"Comment": "> Quantum computing sounds fancy, but any time I look into it, its use cases sound very minimal. Maybe we\u2019ll have stronger encryption? It doesn\u2019t sound life changing though: https://en.m.wikipedia.org/wiki/Quantum_algorithm\n\nElectricity also seemed like it had very minimal uses. Burn things, heat things, maybe if you got really fancy you could make purple plasma arcs with it or make someone's hair stick up from their head.\n\nWithout understanding it, we would never even think of using electromagnetic waves to transmit information and make sensors to learn more about the world around us", "Score": 5, "Replies": []}, {"Comment": "re:DNA hard drives, just consider generically the differences between our most advanced storage devices and what known polymers are capable of. Polymer based storage could be thousands of times more information dense, as well as thousands of times more durable. DNA might not be that polymer (eg, due to the speed of polymerase), but the fact that polymers are able to do this at all is similar fortune as winning the lottery IMO.", "Score": 3, "Replies": [{"Reply": "[deleted]", "Reply Score": 0}]}, {"Comment": "Every problem you mentioned with these is a research gap :)", "Score": 1, "Replies": [{"Reply": "[deleted]", "Reply Score": 1}]}, {"Comment": "You don\u2019t even have to go that far. Even something like O(n^2) grows quick enough to be a serious issue in practice", "Score": 6, "Replies": [{"Reply": "Oh totally, the point is that the amount of computation we can theoretically reach with absurd exponents is practically indistinguishable from \u201cexponential\u201d. 2 is too big then Graham\u2019s number is really really too big.", "Reply Score": 2}]}, {"Comment": "Really? I'm not a cs theory person, but P=NP would collapse the polynomial hierarchy and cause a whole bunch of weird theory implications in fields I do have more familiarity with. I find it far more likely the Strong Exponential Time hypothesis is true. Are there equally weird implications for a negative result?", "Score": 1, "Replies": [{"Reply": "[A general resolution of intractable problems in polynomial time through DNA Computing](https://www.sciencedirect.com/science/article/abs/pii/S0303264716302350) \n\nThe trick is exponential space, so no silver bullet here. With exponential parallelism you can obviously solve NPC problems quick because they have small proofs.", "Reply Score": 1}]}, {"Comment": "yeah we talk about that.\n\nSigning fake blocks would tank the currency but you could certainly use it to 'improve' your mining efficiency arbitrarily.\n\nHe showed me how to do that (it involves walking down the nodes of a sha modeling tree until you hit a certain kind of leaf node) this lets you basically 'walk' towards nonce values which will sha with ur block to produce values starting with the desired number of zeros. (again producing just one 'gold' block with too many zeros at the start would show existence of control over sha exists and tank the currency, so its a delicate game)\n\nAnyways thankfully geniuses like him are too busy because god knows what damage they could do if extortion was their motive.\n\nPeace,", "Score": 1, "Replies": [{"Reply": "I'm not talking about \"fake blocks\" or anything like that. Just pure legit mining, but doing it more efficiently than others.\n\nYour friend cannot reverse sha. They just can't. They are totally dreaming when they say that.", "Reply Score": 1}]}, {"Comment": "This is why we can't understand what they're doing on the inside.  Complexity is incomprehensible without abstractions to provide structure.\n\nSo we surround them with another abstraction - a black box - in order to fit them into our existing software.", "Score": 0, "Replies": []}, {"Comment": "Obfuscation is not the same as abstraction.\n\nNeural networks excel in generality.", "Score": 0, "Replies": []}, {"Comment": "Do you think (general) quantum computers would be a meaningful thing in our lifetime?\n\nFor me, they seems like fusion reactors, they are always 30 years away. Like [the biggest number that have been factored by Shor algorithm is 21](https://en.wikipedia.org/wiki/Integer_factorization_records#Records_for_efforts_by_quantum_computers), and even that was more than 10 years ago.\n\nMaybe quantum key-distributions will become a thing, but that is more quantum physics than CS.", "Score": 1, "Replies": [{"Reply": "I don't know, because I am not an engineer building these things, but a Computer Scientist working with them theoretically. I do hope that we will have a breakthrough at some point, akin to the transistor maybe, that will enable us to build quantum systems reliably and quickly. But I have no clue whether that is realistic.", "Reply Score": 1}]}, {"Comment": "[deleted]", "Score": 0, "Replies": [{"Reply": "Copying DNA is pretty quick (at scale), it would be physically transporting it that might be slow. Maybe we should try carrier pigeons.", "Reply Score": 1}]}, {"Comment": "[deleted]", "Score": 1, "Replies": [{"Reply": "Oh, okay my bad. I got too caught up in the discussion in the comments I forgot the initial point. \n\nAnyways transformers were introduced in 2017 and they changed the game in DL. The field of DL is moving too fast. Half of the entire field was invented within the last decade. It gave birth to many job titles and the technology is now found in every other field out there.", "Reply Score": 2}]}, {"Comment": "Oh totally, the point is that the amount of computation we can theoretically reach with absurd exponents is practically indistinguishable from \u201cexponential\u201d. 2 is too big then Graham\u2019s number is really really too big.", "Score": 2, "Replies": []}, {"Comment": "[A general resolution of intractable problems in polynomial time through DNA Computing](https://www.sciencedirect.com/science/article/abs/pii/S0303264716302350) \n\nThe trick is exponential space, so no silver bullet here. With exponential parallelism you can obviously solve NPC problems quick because they have small proofs.", "Score": 1, "Replies": []}, {"Comment": "I'm not talking about \"fake blocks\" or anything like that. Just pure legit mining, but doing it more efficiently than others.\n\nYour friend cannot reverse sha. They just can't. They are totally dreaming when they say that.", "Score": 1, "Replies": [{"Reply": "It's not fake computation lol it's just that with reverse sha you can break block chains by signing whatever you like.\n\nThe reason people can't reverse sha is because the rolls and xors are hard to visualise and reason about (lots of overlap/mixing/noise etc) but these are not informational issues there are interpretational issues.\n\nThe program which reverses sha is similarly complicated to the program which applies it forward.\n\nI didn't say he could do fast factorisation (or some truely information level difficult task) although you should see his factor list based number system jaja\n\nPeace", "Reply Score": 1}]}, {"Comment": "I don't know, because I am not an engineer building these things, but a Computer Scientist working with them theoretically. I do hope that we will have a breakthrough at some point, akin to the transistor maybe, that will enable us to build quantum systems reliably and quickly. But I have no clue whether that is realistic.", "Score": 1, "Replies": []}, {"Comment": "Copying DNA is pretty quick (at scale), it would be physically transporting it that might be slow. Maybe we should try carrier pigeons.", "Score": 1, "Replies": [{"Reply": "[deleted]", "Reply Score": 1}]}, {"Comment": "Oh, okay my bad. I got too caught up in the discussion in the comments I forgot the initial point. \n\nAnyways transformers were introduced in 2017 and they changed the game in DL. The field of DL is moving too fast. Half of the entire field was invented within the last decade. It gave birth to many job titles and the technology is now found in every other field out there.", "Score": 2, "Replies": []}, {"Comment": "It's not fake computation lol it's just that with reverse sha you can break block chains by signing whatever you like.\n\nThe reason people can't reverse sha is because the rolls and xors are hard to visualise and reason about (lots of overlap/mixing/noise etc) but these are not informational issues there are interpretational issues.\n\nThe program which reverses sha is similarly complicated to the program which applies it forward.\n\nI didn't say he could do fast factorisation (or some truely information level difficult task) although you should see his factor list based number system jaja\n\nPeace", "Score": 1, "Replies": [{"Reply": "Hey, you're the one that used the word \"fake.\"\n\nSorry man, but you literally have no idea what you're talking about. It has zero to do with being \"hard to visualize\" or \"interpretational issues\" - you simply can't do it.\n\nHere's my challenge for you and/or your friend: Here's the SHA256 hash of a file - if you can provide any input that produces this hash value using this magical \"reverse sha\" then I will give you $5000. Simple as that. Here's tha hash:\n\ne0bfaa76ca4426446b8b9191f0f45a5e98a751b777736ba0919ae0ec240d490c\n\nGood luck!", "Reply Score": 1}]}, {"Comment": "[deleted]", "Score": 1, "Replies": [{"Reply": "I mean that we can make exponential copies of DNA. One round of PCR is something like 1 minute and doubles the amount of DNA. At small scales of data it\u2019s obviously much faster to send 1 minutes worth of data over wire, but there is clearly a point that \u201cat scale\u201d that exponential doubling wins.", "Reply Score": 1}, {"Reply": "I assume they mean that they aren't spending a lot of time setting up and managing some repeatable process to achieve volume. Eg, if I have some setup and teardown time for my process it is important that I am able to do a lot of that process each time.\n\nhttps://en.wikipedia.org/wiki/Polymerase_chain_reaction\n\nA google search says DNA replicates at a rate of ~208,000 base-pairs per second, but again that's just DNA, one polymer and however many polymerase. That doesn't speak to the types of polymer and polymerase that may be found.", "Reply Score": 1}]}, {"Comment": "Hey, you're the one that used the word \"fake.\"\n\nSorry man, but you literally have no idea what you're talking about. It has zero to do with being \"hard to visualize\" or \"interpretational issues\" - you simply can't do it.\n\nHere's my challenge for you and/or your friend: Here's the SHA256 hash of a file - if you can provide any input that produces this hash value using this magical \"reverse sha\" then I will give you $5000. Simple as that. Here's tha hash:\n\ne0bfaa76ca4426446b8b9191f0f45a5e98a751b777736ba0919ae0ec240d490c\n\nGood luck!", "Score": 1, "Replies": [{"Reply": "yeah fake BLOCK not fake COMPUTE lol\n\nOfcoarse you can reverse it, there is no evidence than anything in logic cannot be reversed and SHA 1 (with rolls and Xors) is extremely reversible.\n\nThanks for the offer but money is not motivating (certainly not 5k lol) If your legit interested send me a PM im happy to walk you thru it but youll need some basic programming and math skills.\n\nI find this stuff fascinating! the fact that the same algorithm can be used for so many difficult unrelated things its really cool.\n\npeace", "Reply Score": 1}]}, {"Comment": "I mean that we can make exponential copies of DNA. One round of PCR is something like 1 minute and doubles the amount of DNA. At small scales of data it\u2019s obviously much faster to send 1 minutes worth of data over wire, but there is clearly a point that \u201cat scale\u201d that exponential doubling wins.", "Score": 1, "Replies": [{"Reply": "[deleted]", "Reply Score": 1}]}, {"Comment": "I assume they mean that they aren't spending a lot of time setting up and managing some repeatable process to achieve volume. Eg, if I have some setup and teardown time for my process it is important that I am able to do a lot of that process each time.\n\nhttps://en.wikipedia.org/wiki/Polymerase_chain_reaction\n\nA google search says DNA replicates at a rate of ~208,000 base-pairs per second, but again that's just DNA, one polymer and however many polymerase. That doesn't speak to the types of polymer and polymerase that may be found.", "Score": 1, "Replies": []}, {"Comment": "yeah fake BLOCK not fake COMPUTE lol\n\nOfcoarse you can reverse it, there is no evidence than anything in logic cannot be reversed and SHA 1 (with rolls and Xors) is extremely reversible.\n\nThanks for the offer but money is not motivating (certainly not 5k lol) If your legit interested send me a PM im happy to walk you thru it but youll need some basic programming and math skills.\n\nI find this stuff fascinating! the fact that the same algorithm can be used for so many difficult unrelated things its really cool.\n\npeace", "Score": 1, "Replies": [{"Reply": "Again, you just don't know what you're talking about. If you think SHA-1 is \"extremely reversible\" you fundamentally don't understand SHA-1.\n\nLook: SHA-1 was introduced in 1993. It took until 2017 - 24 years - until anyone could find even a single collision. 24 years with some of the greatest minds in cryptography, computer science, and mathematics thinking about this before they found ONE value. Finding that one value took thousands of CPU-years (around 7000 CPU years). And that's for finding an arbitrary collision, which is far, FAR easier than \"reversing SHA1,\" which no one has gotten even remotely close to.\n\nI remember being young and naive and thinking computation was simpler than it actually is. It's not simple. And you can't just reverse things like SHA1.", "Reply Score": 1}]}, {"Comment": "[deleted]", "Score": 1, "Replies": [{"Reply": "Oh definitely you gotta run more rounds and use something like a fountain code to decode for the signal to come through reliably, then you need a sequencing machine on the other end to read it which heavily depends on how the DNA itself is actually structured.   \n\nBut again we can make exponential copies with a doubling time of about a minute and need basically none of it (in terms of grams of material) to read it it again on the other side. There is a scale that it has to work. Just might be we need to be talking about shipping all of humanities data to Jupiter before we get over the hump.\n\nDNA computation systems can probably solve NP-Hard problems too in polynomial time, they just also use exponential space so aren\u2019t practical. But again just gotta get I\u2019ve that hump to be at scale and we\u2019re golden.", "Reply Score": 1}]}, {"Comment": "Again, you just don't know what you're talking about. If you think SHA-1 is \"extremely reversible\" you fundamentally don't understand SHA-1.\n\nLook: SHA-1 was introduced in 1993. It took until 2017 - 24 years - until anyone could find even a single collision. 24 years with some of the greatest minds in cryptography, computer science, and mathematics thinking about this before they found ONE value. Finding that one value took thousands of CPU-years (around 7000 CPU years). And that's for finding an arbitrary collision, which is far, FAR easier than \"reversing SHA1,\" which no one has gotten even remotely close to.\n\nI remember being young and naive and thinking computation was simpler than it actually is. It's not simple. And you can't just reverse things like SHA1.", "Score": 1, "Replies": [{"Reply": "Look I get it, you don't understand how to do it so it sounds like magic to you.\n\nI remember being young and naive and thinking others cant do things just because I cant do them.\n\nThere is no evidence that anything in computation is irreversible (in the sense that we are using here), obviously a shift one way can be undone by a shift the other way (were not interested in information loss based irreversibility, again we want to be able to produce an input which produces a specific output, we don't care WHICH input was used to produce a specific output, as it's not relevant to 'break' a hashing algorithm).\n\nSha is ultimately based on obscurity, data garbling is used to confuse manual reversal and repeated xor veils are used to confuse cryptanalysis.\n\nThese types of difficulties do not affect an entropy guided decision tree synthesizer.\n\nRolls just move data around (no assumption are made in our system about organization or layout so this is fine)\n\nXor also isn't a problem, veils simply cause indecision where picking either side looks just as good and you basically just end up breaking it down on both sides, these can simply be ignored (only cause minor inefficiency at inference time) but if you want you can walk back up the tree and identify the patterns caused by such veils, if you want you could just allow your tree builder to inject XOR operation there.\n\nIf there's one thing I've learned about cutting edge advanced tech, it's that the best stuff comes from peoples basements :D\n\nGralic beats all image compressors handily (and makes PNG etc look like a joke) Cerebro (the name of this algorithm) also beats PNG, and I have no doubt with some massaging it could be Gralic aswell.\n\nFeel free to ask questions I find this stuff fascinating!\n\nBut if you only wanna harp on about how you know I'm wrong then keep it to yourself, your not here and you don't have the tech and it is just not interesting to be told your bike is broken while you are on it riding happily :D\n\nAny task which can be solved by a program can be solved by cerebro (and unlike exhaustive search etc Cerebro can write enormous programs very quickly), if your saying reverse Sha can't be solved by any program then we need to \nhave a talk about the nature of programs :D\n\nAny sequence can be recognized and any sequence can be generated, computers are universal machines after all.\n\nPeace", "Reply Score": 1}]}, {"Comment": "Oh definitely you gotta run more rounds and use something like a fountain code to decode for the signal to come through reliably, then you need a sequencing machine on the other end to read it which heavily depends on how the DNA itself is actually structured.   \n\nBut again we can make exponential copies with a doubling time of about a minute and need basically none of it (in terms of grams of material) to read it it again on the other side. There is a scale that it has to work. Just might be we need to be talking about shipping all of humanities data to Jupiter before we get over the hump.\n\nDNA computation systems can probably solve NP-Hard problems too in polynomial time, they just also use exponential space so aren\u2019t practical. But again just gotta get I\u2019ve that hump to be at scale and we\u2019re golden.", "Score": 1, "Replies": [{"Reply": "[deleted]", "Reply Score": 1}]}, {"Comment": "Look I get it, you don't understand how to do it so it sounds like magic to you.\n\nI remember being young and naive and thinking others cant do things just because I cant do them.\n\nThere is no evidence that anything in computation is irreversible (in the sense that we are using here), obviously a shift one way can be undone by a shift the other way (were not interested in information loss based irreversibility, again we want to be able to produce an input which produces a specific output, we don't care WHICH input was used to produce a specific output, as it's not relevant to 'break' a hashing algorithm).\n\nSha is ultimately based on obscurity, data garbling is used to confuse manual reversal and repeated xor veils are used to confuse cryptanalysis.\n\nThese types of difficulties do not affect an entropy guided decision tree synthesizer.\n\nRolls just move data around (no assumption are made in our system about organization or layout so this is fine)\n\nXor also isn't a problem, veils simply cause indecision where picking either side looks just as good and you basically just end up breaking it down on both sides, these can simply be ignored (only cause minor inefficiency at inference time) but if you want you can walk back up the tree and identify the patterns caused by such veils, if you want you could just allow your tree builder to inject XOR operation there.\n\nIf there's one thing I've learned about cutting edge advanced tech, it's that the best stuff comes from peoples basements :D\n\nGralic beats all image compressors handily (and makes PNG etc look like a joke) Cerebro (the name of this algorithm) also beats PNG, and I have no doubt with some massaging it could be Gralic aswell.\n\nFeel free to ask questions I find this stuff fascinating!\n\nBut if you only wanna harp on about how you know I'm wrong then keep it to yourself, your not here and you don't have the tech and it is just not interesting to be told your bike is broken while you are on it riding happily :D\n\nAny task which can be solved by a program can be solved by cerebro (and unlike exhaustive search etc Cerebro can write enormous programs very quickly), if your saying reverse Sha can't be solved by any program then we need to \nhave a talk about the nature of programs :D\n\nAny sequence can be recognized and any sequence can be generated, computers are universal machines after all.\n\nPeace", "Score": 1, "Replies": [{"Reply": ">There is no evidence that anything in computation is irreversible\n\nIf P != NP then one-way functions exist, which are functions that are irreversible. To date all evidence points to P != NP, and while there's less solid evidence that SHA is a one-way function, evidence suggests that it is. Limiting to programs with sensible running times (say under a year on the fastest computer available today) I'd definitely say that \"reverse Sha can't be solved by any program.\"  I guarantee you that the vast majority of computer scientists would agree with that.\n\n>Sha is ultimately based on obscurity, data garbling is used to confuse manual reversal and repeated xor veils are used to confuse cryptanalysis.\n\nNo, sha is not based on any of that. No crypto is \"based on obscurity\" (in fact, that's one of the guiding rules of cryptography, going back to the 1800s with Kerckhoff's Principle). \n\nI'm not wasting any more of my time on this. If your friend really could do a tiny fraction of the things you claim, they would immediately be the most famous computer scientist on the face of the planet, win the Turing award and every other award in the field, and would be unfathomly wealthy. Human civilization would quite literally be thrown for a loop.\n\nSo knock yourself out spending your time to establish this. If you were one of my students I would gently point out why none of this makes sense, but even for one of my students I wouldn't spend any more time on this, and I have some responsibility toward them. Random cranks on Reddit?  Nah, I'm out.", "Reply Score": 2}]}, {"Comment": "[deleted]", "Score": 1, "Replies": [{"Reply": "A century might be optimistic.", "Reply Score": 1}]}, {"Comment": ">There is no evidence that anything in computation is irreversible\n\nIf P != NP then one-way functions exist, which are functions that are irreversible. To date all evidence points to P != NP, and while there's less solid evidence that SHA is a one-way function, evidence suggests that it is. Limiting to programs with sensible running times (say under a year on the fastest computer available today) I'd definitely say that \"reverse Sha can't be solved by any program.\"  I guarantee you that the vast majority of computer scientists would agree with that.\n\n>Sha is ultimately based on obscurity, data garbling is used to confuse manual reversal and repeated xor veils are used to confuse cryptanalysis.\n\nNo, sha is not based on any of that. No crypto is \"based on obscurity\" (in fact, that's one of the guiding rules of cryptography, going back to the 1800s with Kerckhoff's Principle). \n\nI'm not wasting any more of my time on this. If your friend really could do a tiny fraction of the things you claim, they would immediately be the most famous computer scientist on the face of the planet, win the Turing award and every other award in the field, and would be unfathomly wealthy. Human civilization would quite literally be thrown for a loop.\n\nSo knock yourself out spending your time to establish this. If you were one of my students I would gently point out why none of this makes sense, but even for one of my students I wouldn't spend any more time on this, and I have some responsibility toward them. Random cranks on Reddit?  Nah, I'm out.", "Score": 2, "Replies": [{"Reply": "If P != NP ... Big assumption my dude! (personally I lean strongly to P==NP)\n\nGood chat, don't feel bad this is legitimately frustrating stuff!\n\nI know exactly how you feel about tech being wasted :D here's a quick related story about this friend..\n\nHe used to work with Hexagon at a large shared office and everyday he would sit next to some guys who were involved with lossless compression related to GIF and PNG, they were obsessed with performance (both image decode and file at rest size) but in all the years he worked there he never taught them about Cerebro (despite my many suggestions!)\n\nI've met many geniuses in my time, more than one of which as solved some well known problem but has no interest in taking the next steps.\n\nGreat to hear your a teacher! a strong sense of skepticism is super important, just make sure you don't crush their ideas (even if they seem a little wild)\n\nI can tell your off, but just for readers (and future you) i want to end by saying that im happy to explain and walk you thru, if your a coder you can prove whats possible with this stuff to yourself) any who, have a lovely day!\n\npeace yall", "Reply Score": 1}]}, {"Comment": "A century might be optimistic.", "Score": 1, "Replies": []}, {"Comment": "If P != NP ... Big assumption my dude! (personally I lean strongly to P==NP)\n\nGood chat, don't feel bad this is legitimately frustrating stuff!\n\nI know exactly how you feel about tech being wasted :D here's a quick related story about this friend..\n\nHe used to work with Hexagon at a large shared office and everyday he would sit next to some guys who were involved with lossless compression related to GIF and PNG, they were obsessed with performance (both image decode and file at rest size) but in all the years he worked there he never taught them about Cerebro (despite my many suggestions!)\n\nI've met many geniuses in my time, more than one of which as solved some well known problem but has no interest in taking the next steps.\n\nGreat to hear your a teacher! a strong sense of skepticism is super important, just make sure you don't crush their ideas (even if they seem a little wild)\n\nI can tell your off, but just for readers (and future you) i want to end by saying that im happy to explain and walk you thru, if your a coder you can prove whats possible with this stuff to yourself) any who, have a lovely day!\n\npeace yall", "Score": 1, "Replies": []}]},{"Title": "How to start with Kernel Development", "Score": 23, "URL": "https://www.reddit.com/r/compsci/comments/1823aip/how_to_start_with_kernel_development/", "CreatedAt": 1700752397.0, "Full Content": "I have good experience of C/C++/Go and am experienced in Systems Programming. Done academic projects on Concurrent Data structures, Concurrent Runtime, Garbage Collection, Network programming etc.\n\nBut never touched Kernel Programming.\nI have the following questions.\n\n1. How to start with Kernel programming?\n2. I will be graduating in 2024. Any project that can be done within 2/3 months to show kernel  programming experience?\n\nThanks.", "CntComments": 5, "Comments": [{"Comment": "I\u2019d start with implementing a device driver for a well documented hardware device for Linux. It will at least get you started coding and debugging in kernel space. Perhaps driver for a graphics controller to enable it to be used as a compute subsystem. Two to three months is not a long time for a craft that can take many years to understand how much of a modern operating system kernel works. I would start out with understanding the device driver interface and then ensure a good understanding of how multi processor code is implemented with locking critical regions and interrupt masking and handling. Much of the kernel for most modern operating systems are well defined and don\u2019t change much so job opportunities won\u2019t be hugely plentiful. However adding new peripheral types and hardware support, along with optimization may well be. From there, understanding the rest of the kernel is a gradual learning curve. If you are doing it for a career, the list of employers is small, Microsoft, Google, Apple, Oracle and a few other Linux houses. I suppose there are a few real time and communications equipment companies but it is a list of a small number of companies nevertheless. As an ex operating systems engineer a combination of a few commodity operating systems such as Linux and Windows NT killed the every company had their own Unix industry. It\u2019s a fun job but I suspect the job opportunities are filled by word of mouth or perhaps from candidates which have relevant Masters or PhD projects for alternative operating system architectures.", "Score": 6, "Replies": [{"Reply": "Thanks for the reply.\nMay be totally offtopic but I have the following questions, will be very grateful if you answer it.\n\n1. Could you please share you industry experience as an OS developer.\n2. Have you seen acacdemic papers/ideas been integrating to the mainstream OS?\n3. What is your view of the future of OS in the current cloud computing world with advancement of different technologies like Kernel Bypass, specialized HW/accelerators, languages like rust?", "Reply Score": 1}]}, {"Comment": "Build an RTOS.  https://www.freertos.org/Creating-a-new-FreeRTOS-project.html", "Score": 2, "Replies": []}, {"Comment": "Just a heads up, \"Kernel Programming\" could be creating a kernel for an OS, or creating Kernels for the GPU ie: [https://openai.com/careers/kernel-engineer](https://openai.com/careers/kernel-engineer) which is parallel programming.\n\nBut, anyway one fun way I did this is by picking up an OS book / reading this wiki: [https://wiki.osdev.org/Expanded\\_Main\\_Page](https://wiki.osdev.org/Expanded_Main_Page)\n\nAnd then playing around with an older console, either through the SDK (N64, Sega Saturn) or via making an emulator (NES) and implementing it through that.", "Score": 4, "Replies": []}, {"Comment": "Thanks for the reply.\nMay be totally offtopic but I have the following questions, will be very grateful if you answer it.\n\n1. Could you please share you industry experience as an OS developer.\n2. Have you seen acacdemic papers/ideas been integrating to the mainstream OS?\n3. What is your view of the future of OS in the current cloud computing world with advancement of different technologies like Kernel Bypass, specialized HW/accelerators, languages like rust?", "Score": 1, "Replies": [{"Reply": "I worked on UNIX(TM) System V Release 4 MP, the first AT&T SVR4 UNIX port to symmetrical multiprocessing systems. Of course these days such skills are needed for every OS as multiple core or thread processors are ubiquitous. Back then SMP systems used discrete single core CPUs. Our targets were i486 and then Pentium CPUs.\n\nBack then there was a thought that shifting monolithic kernels onto micro kernels with message passing might be the way forward and the shift of moving a lot of kernel code into user space. That was the subject of a lot of research. Mac OSX of course ended up on a micro kernel, NT had a hardware abstraction layer but most kernels ended up staying monolithic kernel space items, which is as far as I can see describes Linux.\n\nSo, my prediction would be more of the same with internal driver type interfaces to specialist processors. As today most OS types like Windows, Mac OSX and Linux have the same overall design, I don\u2019t think a radical change will occur in any of them unless there is an overwhelming business for the change and that all majors would support that change.\n\nSo from my perspective I don\u2019t see a rapid need to train OS kernel engineers other than to replace those older folks retiring. I haven\u2019t yet seen a return to the good old days where everyone had their own UNIX on very varied hardware.\n\nAs for Rust, obviously it\u2019s a more modern language but like C you can write an OS kernel efficiently in it. There is a good reason kernels are still written in C as you have a good idea of what efficiencies you are doing or not doing from the nature of the language itself. Whether this situation lasts for another 40 years or I have no idea but I suppose it probably will as CPU technology seems to have the same nature of low level instructions as they always have and building things that look more like VM interpreters with microcode, like the Java CPU, don\u2019t seem to be very successful.\n\nObviously the big move forward has been the capability to build virtual machines, thanks to enough hardware support instructions to make it feasible.\n\nBack then I thought CPUs would end up being a higher and higher level abstraction with their instruction sets but the reverse happened with RISC CPUs. I also thought Intel or others would have dropped 8086 and 32 bit support to reduce chip count as nobody would use that stuff and more at the application level, but that never happened. I thought the i860-II would replace x86, or even Itanium but I was wrong there too.\n\nIn reality despite all the research on single system images across interconnected machines, micro kernels, distributed operating systems and modular message passing user space kernels, we are still mostly using monolithic kernels with pretty much the same designs written in C. Remember OS technology is now largely a commodity product, not a value add profit center.\n\nBut, my opinions are worth what you paid for them and I am probably going to be wrong, again.", "Reply Score": 3}]}, {"Comment": "I worked on UNIX(TM) System V Release 4 MP, the first AT&T SVR4 UNIX port to symmetrical multiprocessing systems. Of course these days such skills are needed for every OS as multiple core or thread processors are ubiquitous. Back then SMP systems used discrete single core CPUs. Our targets were i486 and then Pentium CPUs.\n\nBack then there was a thought that shifting monolithic kernels onto micro kernels with message passing might be the way forward and the shift of moving a lot of kernel code into user space. That was the subject of a lot of research. Mac OSX of course ended up on a micro kernel, NT had a hardware abstraction layer but most kernels ended up staying monolithic kernel space items, which is as far as I can see describes Linux.\n\nSo, my prediction would be more of the same with internal driver type interfaces to specialist processors. As today most OS types like Windows, Mac OSX and Linux have the same overall design, I don\u2019t think a radical change will occur in any of them unless there is an overwhelming business for the change and that all majors would support that change.\n\nSo from my perspective I don\u2019t see a rapid need to train OS kernel engineers other than to replace those older folks retiring. I haven\u2019t yet seen a return to the good old days where everyone had their own UNIX on very varied hardware.\n\nAs for Rust, obviously it\u2019s a more modern language but like C you can write an OS kernel efficiently in it. There is a good reason kernels are still written in C as you have a good idea of what efficiencies you are doing or not doing from the nature of the language itself. Whether this situation lasts for another 40 years or I have no idea but I suppose it probably will as CPU technology seems to have the same nature of low level instructions as they always have and building things that look more like VM interpreters with microcode, like the Java CPU, don\u2019t seem to be very successful.\n\nObviously the big move forward has been the capability to build virtual machines, thanks to enough hardware support instructions to make it feasible.\n\nBack then I thought CPUs would end up being a higher and higher level abstraction with their instruction sets but the reverse happened with RISC CPUs. I also thought Intel or others would have dropped 8086 and 32 bit support to reduce chip count as nobody would use that stuff and more at the application level, but that never happened. I thought the i860-II would replace x86, or even Itanium but I was wrong there too.\n\nIn reality despite all the research on single system images across interconnected machines, micro kernels, distributed operating systems and modular message passing user space kernels, we are still mostly using monolithic kernels with pretty much the same designs written in C. Remember OS technology is now largely a commodity product, not a value add profit center.\n\nBut, my opinions are worth what you paid for them and I am probably going to be wrong, again.", "Score": 3, "Replies": []}]},{"Title": "Confused: Is data transfered quicker when using Programmed I/O compared to Interrupt-Driven?", "Score": 8, "URL": "https://www.reddit.com/r/compsci/comments/181a3ay/confused_is_data_transfered_quicker_when_using/", "CreatedAt": 1700662500.0, "Full Content": "TLDR: Correct me if I'm wrong, but this is what I understand: There is a trade-off between the drawbacks of programmed I/O and interrupt-driven. Either waste processor time but transfer data at a higher rate with programmed I/O, or let the processor be busy with other tasks until the I/O is done but the transfer rate from I/O to memory is slower. Thus, data can be transfered quicker when programmed I/O is quicker.\n\n&#x200B;\n\nOk, so I am reading William Stalling's book: \"Computer Organization and Architecture\", and right now I am reading about DMA. To introduce the concept, he was describing the drawbacks of programmed I/O and interrupt-driven.\n\nI am going to quote the passage and tell you what I interpret and why so that maybe you can help clarify some things? Here is a the passage:\n\n`\"Interrupt-driven I/O, though more efficeint than simple programmed I/O, still requires active intervention on the processor to transfer data between memory and an I/O module, and any data transfer mist traverse a path through the processor. Thus, both these forms of I/O suffer from two inherent drawbacks:`\n\n`- the I/O transfer rate is limited by the speed with which the processor can test and service a device.`\n\n`- the processor is tied up in managing an I/O transfer; a number of intructions must be executed for each I/O transfer`\n\n`There is somewhat of a trade-off between these two drawbacks. Consider the transfer of a block of data. Using simple programmed I/O, the processor is dedicated to the task of I/O and can move data at a rather high rate, at the cost of doing nothing else. Interrupt I/O frees up the processor to some extent at the expense of the I/O transfer rate.\"` \n\nI'm sure I must have misunderstood something but I just can't see it. This is what I interepret:\n\nThere is a trade-off. Either waste processor time but transfer data at a higher rate with programmed I/O, or let the processor be busy with other tasks until the I/O is done but the transfer rate from I/O to memory is slower. Thus, data can be transfered quicker when programmed I/O is quicker.\n\nI'm confused. I thought that is what the I/O data buffer is for: so that data can be transfered at different speeds (ie. transfer in Ghz when the processor is transferring data even if the I/O device was transfering the same data in Mhz).\n\nThank you for reading until the end, and thank you for your help in advance :)", "CntComments": 9, "Comments": [{"Comment": "A problem with any discussion of these techniques is that what\u2019s important varies from one application to another, and it can be difficult to write something that is correct, crisp, and helpful. With that said, here\u2019s an attempt\u2026\n\nFirst; \u201cprogrammed I/O\u201d and \u201cinterrupt-driven\u201d are describing flavors of two different things; PIO is a specific method of data transfer, whilst interrupt-driven refers to a waiting technique. \n\nCommon alternatives to PIO are DMA and bus-mastering. Common alternatives to interrupt-driven are busy-waiting and polling.\n\nSecond; there are several things that commonly get lumped together when talking about I/O \u201cperformance\u201d; latency, throughput, and efficiency for example.\n\nLatency can be thought of as the time between an I/O operation being possible and it actually being performed, and throughput the rate at which I/O operations are performed. From this you can see that the way that the CPU waits for an operation to be possible will affect latency, and how the transfer is performed will affect throughput.\n\nEfficiency is a bit more vague as it depends on what is considered important in an application. Typically though it\u2019ll refer to how much of a burden the activity is on the rest of the system; how much CPU time or energy it consumes are common metrics.\n\nThere are more subtleties here; e.g. for DMA and bus-mastering transfer modes the CPU is typically only made aware of the transfer when some part of it completes, so the latency definition needs some tweaking to properly account for that\u2026 \n\nYour questions suggest that you haven\u2019t been confronted with enough different peripherals yet for this to really sink in - that\u2019s OK, the best take-away for you right now might just be that there are quite a few different ways to handle I/O and that you need to think carefully about what options you have and what is important for your specific design.", "Score": 12, "Replies": [{"Reply": "> First; \u201cprogrammed I/O\u201d and \u201cinterrupt-driven\u201d are describing flavors of two different things; PIO is a specific method of data transfer, whilst interrupt-driven refers to a waiting technique.\n\nI agree.  OPs question is ill posed.  It's as if you asked \u201cis it colder at night or during winter?\u201d\n\nThe other answers just try to make sense of this mess of a question.\n\nTo OP: to clear up your confusion, learn about what each of these things mean exactly and how they are implemented.  Then it's immediately obvious what the advantages and drawbacks are.", "Reply Score": 1}]}, {"Comment": "cpu can do I/O in two ways\n\n1. do active wait using a loop until the device becomes ready for the next data word\n2. process user code and let the device invoke hardware interruptr when it's ready for the next word\n\nthe second approach is less efficient because cpu needs extra cycles to process each interrupt, so the first approach allows to reach higher speeds.\n\nneither one is DMA, though. **Direct** memory access, by definition, means direct access to RAM from **device** (note that any CPU has direct access to memory anyway, so talking about CPU's direct access to RAM is like talking about wet water).\n\nwith DMA, CPU isn't involved in transferring data words at all. Instead, CPU just sends to a device a single command describing the entire memory block to transfer.\n\nSo, DMA imposes very small usage of CPU time, while the maximum speed of DMA transfer depends on the device's controller. It may be lower than CPU's speed of RAM access, although in these cases probably CPU-controlled I/O from the same device probably will be even slower.", "Score": 4, "Replies": [{"Reply": "Ok, so interrupt I/O takes longer to transfer data compared to programmed I/O because it has addition steps (ie. saving state in stack). That makes sense now, thank you.", "Reply Score": -1}]}, {"Comment": "I'm going to toss out a wacky, not-fully-thought-through analogy.\n\nYou're at home and you have to do your laundry and some other chores.\n\nProgrammed Laundry:\n\nYou bring the laundry to the laundry room. You put it in the washer. You start the washer. You sit in front of it, waiting for it to finish. As soon as it finishes, you take the laundry from the washer and put in the dryer. Now you sit in front of the dryer, waiting for it to finish. As soon it is it finishes, you take the laundry from the dryer and bring it to your room.\n\nYou've done your laundry in the shortest possible amount of time, but accomplished nothing else.\n\nInterrupt Driven Laundry:\n\nYou bring the laundry to the laundry room. You put it in the washer. You start the washer and walk away and start on other chores. Some time later, the end of cycle alarm for the washer goes off. When you have a spare cycle, you pause what you're doing, go back to the laundry room and switch it to the dryer. You go back to doing other chores. Some time later, the end of cycle alarm for the dryer goes off. When you have a spare cycle, you pause what you're doing, go back to the laundry room and retrieve the laundry from the dryer and bring it to your room.\n\nThe laundry is done, but it took a little longer, since you had to stop what you were doing and go back to the laundry room, but you got other stuff done, which is pretty great, too.\n\nDirect Laundry Access:\n\nYou have someone you can just boss around. You tell them to do your laundry, put it back in your room, and let you know when it's done. That someone does what you'd have done in the first option, but without you having to think about it at all. You're not notified until it's done.\n\nThe laundry is done at (or pretty close to) optimal speed and you had maximum time to do other useful work.", "Score": 5, "Replies": []}, {"Comment": "All this is very, very architecture-dependent. Modern CPUs more often than not are actually SoCs (systems-on-chip) with external devices (including main RAM) connected by (often serial) point-to-point buses geared towards exchange of long packages. Meaning that programmatic IO is an illusion kept for the sake of backward compatibility.\n\nOld computer architectures had what is called [system bus](https://en.wikipedia.org/wiki/System_bus) controlled by CPU, with devices appearing as special addresses in main memory. So the only way for IO to happen was for CPU to demand for it. Devices themselves were primitive and simply exposed some registers to be read or written via system bus into special address space. Thus to perform IO a program had to check if a device is present by reading from special address, write a command and required data to another address(es), possibly write some more to force execution of command, and than repeatedly check yet another memory location for a special value that signaled that the command had finished. Than the code could read data from yet another set of addresses. Since code was located at same memory, for reading one byte from a devices, several (tens or even more) cycles of system bus were required. It was damn slow and inefficient, but allowed use of brain-dead-simple components.\n\nHere come [interrupts](https://en.wikipedia.org/wiki/Interrupt). An interrupt is a signal that something changed. So, instead of continuous polling of the devices, the code could wait for an event, an interrupt generated by an external devices and THEN read the data. This is an improvement, and in some cases it might be dramatic. But the cost is more complicated hardware and latency, so even today poll-based IO might see use in very special systems.\n\nAnyway, time moved on, computer components became more and more complicated and at some time memory moved on to volatile dynamic RAM which required a separate controller (OK, some processors lived with manual refreshing of RAM, but it didn't last). At the same time devices were becoming smarter and smarter and didn't need so much babysitting anymore. Furthermore, system bus was slowly becoming a bottleneck. All-in-all, there was a need to change the model of interaction of devices.\n\nHere comes DMA or [Direct Memory Access](https://en.wikipedia.org/wiki/Direct_memory_access). Instead of babysitting by CPU, devices were allowed to send (read) bursts of data directly to (from) memory controller, bypassing CPU. This allowed for much better utilization of the system bus, making things dramatically faster. Obviously, while the system bus is occupied, CPU can't execute code since instructions cannot be read from memory. Theoretically speaking, DMA may be babysit, but to my knowledge it usually interacts with CPU using interrupts.\n\nAnyway, modern desktop (and mobile) CPUs are actually SoCs. This means that the system bus (as much as it still exists) is located on the same die as CPU and doesn't extend beyond the CPU die. Instead a set of external bus controllers are located on the CPU die. Those controllers connect to external devices by separate, non-system point-to-point buses.", "Score": 2, "Replies": []}, {"Comment": "Well it all depends upon the hardware and the type of system you are building.\n\nOriginally PC type device often used inb() and outb() to control I/o hardware which then was interrupt driven when given area of memory to transfer and an interrupt when complete. The movement to DMA driven memory transfer tookthe CPU out of the memory transfer itself, just the creation of buffers to transfer in and out of. Interrupts handling mechanisms have the CPU involved, then there potential user space to system space and back transitions depending where the buffers creating in context switch and transferring in and out of kernel space depending on", "Score": 1, "Replies": []}, {"Comment": "> First; \u201cprogrammed I/O\u201d and \u201cinterrupt-driven\u201d are describing flavors of two different things; PIO is a specific method of data transfer, whilst interrupt-driven refers to a waiting technique.\n\nI agree.  OPs question is ill posed.  It's as if you asked \u201cis it colder at night or during winter?\u201d\n\nThe other answers just try to make sense of this mess of a question.\n\nTo OP: to clear up your confusion, learn about what each of these things mean exactly and how they are implemented.  Then it's immediately obvious what the advantages and drawbacks are.", "Score": 1, "Replies": []}, {"Comment": "Ok, so interrupt I/O takes longer to transfer data compared to programmed I/O because it has addition steps (ie. saving state in stack). That makes sense now, thank you.", "Score": -1, "Replies": [{"Reply": "afaik, Interrupt-driven I/O is also considered as programmed I/O (bc it's also performed by CPU). Note that your text mentions interrupt-less I/O as \"simple programmed I/O\"", "Reply Score": 2}, {"Reply": "> takes longer to transfer data\n\nIf you're bit-banging, I guess. Assuming your peripheral is more complicated than on/off, there's presumably going to be a buffer of at least a word, and likely the time required will be the same.\n\nProcessing time dedicated to the task of pushing data to/from it will be less, of course.\n\nDMA just takes that one step further, by removing the processor from the loop entirely, once a \"move this from here to here on each data request\" command is set up.", "Reply Score": 1}]}, {"Comment": "afaik, Interrupt-driven I/O is also considered as programmed I/O (bc it's also performed by CPU). Note that your text mentions interrupt-less I/O as \"simple programmed I/O\"", "Score": 2, "Replies": []}, {"Comment": "> takes longer to transfer data\n\nIf you're bit-banging, I guess. Assuming your peripheral is more complicated than on/off, there's presumably going to be a buffer of at least a word, and likely the time required will be the same.\n\nProcessing time dedicated to the task of pushing data to/from it will be less, of course.\n\nDMA just takes that one step further, by removing the processor from the loop entirely, once a \"move this from here to here on each data request\" command is set up.", "Score": 1, "Replies": []}]},{"Title": "How do you people stay updated?", "Score": 11, "URL": "https://www.reddit.com/r/compsci/comments/1811o1q/how_do_you_people_stay_updated/", "CreatedAt": 1700630541.0, "Full Content": "Do y'all follow any newsletters or use any apps that focus on tech news?", "CntComments": 12, "Comments": [{"Comment": "Reddit mostly. If you join subreddits for a particular technology, people usually post updates for it. \n\nFor things I'm interested in like java, flutter/dart, go, compilers - I randomly remember about them and go down a rabbit hole, testing all the new features/new discoveries since my last go down the rabbit hole.", "Score": 20, "Replies": [{"Reply": "Ohh alright. Thx!", "Reply Score": 0}]}, {"Comment": "hackernews, fireship, some blogs I added to my feedly...", "Score": 7, "Replies": [{"Reply": "Thanks!", "Reply Score": 0}]}, {"Comment": "I use Feedly an RSS aggregator to quickly check all my tech news websites I wish RSS was a little more common outside of news hounds and researchers.", "Score": 5, "Replies": []}, {"Comment": "lobsters and hackernews", "Score": 3, "Replies": [{"Reply": "Thx!", "Reply Score": 1}]}, {"Comment": "I follow @_akhaliq on twitter to get my ML paper news.\n\nSince he got hired by Huggingface it's been kinda meh though, he still posts papers but also a lot of Huggingface/Gradio news that I don't really care about.", "Score": 2, "Replies": []}, {"Comment": "I use newsreadeck app. I can follow as many source I want, for free, and get the articles. The app has a \\`technology\\` section with a bunch of sources and also, every time I follow a new source, the show me similar sources", "Score": 2, "Replies": []}, {"Comment": "Ohh alright. Thx!", "Score": 0, "Replies": []}, {"Comment": "Thanks!", "Score": 0, "Replies": []}, {"Comment": "Thx!", "Score": 1, "Replies": []}]},{"Title": "Publishing a paper on a package", "Score": 12, "URL": "https://www.reddit.com/r/compsci/comments/180qzs9/publishing_a_paper_on_a_package/", "CreatedAt": 1700599594.0, "Full Content": "\nTo clarify I am a mechatronics engineering researcher, I didn't study software engineering or computer science so I am not very familiar with the conventions.\n\nAnyways, I made a python package that has to do with analysing neural networks. My supervisor suggested publishing a paper. He doesn't want me to make the repo public in fear of having the idea be plagiarised, and to make sure we don't lose the citations in case the package is used by other researchers. (this might seem awful but the reality of the matter is that these things do happen and the citations count basically helps us keep our jobs).\n\nDoes anyone have any experience with writing a paper about a software or a package? Dol only make the repo public after publish it? Is there a way I can get some feedback on the work before publishing or is the peer review process enough?", "CntComments": 23, "Comments": [{"Comment": "I don\u2019t think having your repo out in the open is going to make people plagiarise you or make less people cite you. If you want citations make it easy for other researchers to see and build upon your work and a public repo would definitely help. It\u2019s common practice to publish the paper\u2019s code and include the citation in the README .", "Score": 14, "Replies": [{"Reply": "He meant that if I made it public before publishing, the authors might end up citing the repo itself. The university however only acknowledge papers published at indexed journals or presented at conferences. \n\nI wanted to make it public to get feedback first. But he is against the idea.", "Reply Score": 2}, {"Reply": "But should it be done before or after the paper has been published?", "Reply Score": 1}]}, {"Comment": "You should at least make the repo public (or viewable with some sort of url) at the time of publication or conference presentation. Don't forget to include a software license. \n\nIn terms of citations,  I can't speak to any overall trend, but I will say that I have and will cite papers presenting a software package if I use or build upon it in my own research, and those packages are usually open source.\n\nThe rest of it is pretty much writing any other sort of academic paper. Follow the style the conference/journal's style, have a structure like intro+related work, approach, results, conclusion, struggle to compress it all into an abstract, etc.", "Score": 7, "Replies": [{"Reply": "Yeah, I think he is afraid that authors might cite the repository or the software itself if they used it before the paper is made available. The university does not acknowledge these citations.\n\nI honestly wanted to open it to the public first to get some feedback, before writing the paper. But he doesn\u2019t think it\u2019s a good idea.", "Reply Score": 1}]}, {"Comment": "Read up on non-commercial software licenses. Recommend to make the software public at the time of conference presentation or publication date.\n\nUsually universities have policies about what you can do with your novel research done on their dime + facilities, e.g. commercial use on your own time.\n\nThey also usually have places where they keep software for the purposes of saving research.\n\nIt's a good learning experience.", "Score": 3, "Replies": [{"Reply": "Oh, I wasn\u2019t aware of this. This is a very good point. Thank you so much, I will do so.", "Reply Score": 2}]}, {"Comment": ">this might seem awful but the reality of the matter is that these things do happen and the citations count basically helps us keep our jobs\n\n  \nI get it, lots of people don't want to work for free. We protect our knowledge and sell it for profit or credentials we can convert into profit.\n\nSometimes we protect bits and pieces of what we know and give back to the community where it makes sense - as others have mentioned in this thread, open source software is one such way to do that.\n\nMy personal reaction, however, is if you want me to review your stuff and keep it secret, you are going to pay me. That payment might be in credit form maybe.", "Score": 3, "Replies": [{"Reply": "Yeah, that\u2019s fair enough. I think at this point paying for a private review from an expert sounds like the best option. \n\nI don\u2019t really want to keep it a secret. I made the initial version while working on a paper earlier this year. My intention was to make the package available to researchers since nothing similar exists. It will make their jobs a lot easier.", "Reply Score": 2}]}, {"Comment": "You can publish papers for package?", "Score": 1, "Replies": [{"Reply": "systems research is basically a cottage industry of publishing python packages, complete with stupid/cutesy/hypebeast names. there are literally thousands of such papers (and correspondingly, thousands of crappy github repos with dead/abandoned code).", "Reply Score": 3}, {"Reply": "Yes. I think any open source scientific package or tool has a paper published somewhere. You can cite that paper if you used it in your research.", "Reply Score": 2}]}, {"Comment": "Many conferences will ask for artifact submission after acceptance, especially if it's one where people have built tools demonstrating the paper's work.", "Score": 1, "Replies": []}, {"Comment": "He meant that if I made it public before publishing, the authors might end up citing the repo itself. The university however only acknowledge papers published at indexed journals or presented at conferences. \n\nI wanted to make it public to get feedback first. But he is against the idea.", "Score": 2, "Replies": [{"Reply": "It is somewhat common for software to be published prior to publication, though it's related to how specific the software is. Like I know of one particular case where a tool was developed, published, used widely, and iterated on for at least five years prior to the first publication about the tool being submitted, but there was essentially zero chance of another group choosing to develop a similar tool because of its specific use. I've had PIs who didn't want to publish anything in advance (which I hated), and I've had PIs who like to publish often and freely.\n\nThat said, in the repository's README you can at the top specifically request that any academic uses of the software cite the paper (probably with a link to the DOI or the full citation information). I think there are some software licenses that specifically handle this sort of thing, though I've not looked into that.", "Reply Score": 3}]}, {"Comment": "But should it be done before or after the paper has been published?", "Score": 1, "Replies": []}, {"Comment": "Yeah, I think he is afraid that authors might cite the repository or the software itself if they used it before the paper is made available. The university does not acknowledge these citations.\n\nI honestly wanted to open it to the public first to get some feedback, before writing the paper. But he doesn\u2019t think it\u2019s a good idea.", "Score": 1, "Replies": []}, {"Comment": "Oh, I wasn\u2019t aware of this. This is a very good point. Thank you so much, I will do so.", "Score": 2, "Replies": []}, {"Comment": "Yeah, that\u2019s fair enough. I think at this point paying for a private review from an expert sounds like the best option. \n\nI don\u2019t really want to keep it a secret. I made the initial version while working on a paper earlier this year. My intention was to make the package available to researchers since nothing similar exists. It will make their jobs a lot easier.", "Score": 2, "Replies": []}, {"Comment": "systems research is basically a cottage industry of publishing python packages, complete with stupid/cutesy/hypebeast names. there are literally thousands of such papers (and correspondingly, thousands of crappy github repos with dead/abandoned code).", "Score": 3, "Replies": [{"Reply": "Research papers are a method of communication between researchers and professionals. You can't expect all research to be very useful. Sometimes an idea is good on paper but fails to be efficient in practice. It can still serve as a great steping stone for further research, though.", "Reply Score": 2}, {"Reply": "Which journals are usually popular for accepting such research papers?", "Reply Score": 1}]}, {"Comment": "Yes. I think any open source scientific package or tool has a paper published somewhere. You can cite that paper if you used it in your research.", "Score": 2, "Replies": [{"Reply": "I would like to know more can you give any good example.", "Reply Score": 2}]}, {"Comment": "It is somewhat common for software to be published prior to publication, though it's related to how specific the software is. Like I know of one particular case where a tool was developed, published, used widely, and iterated on for at least five years prior to the first publication about the tool being submitted, but there was essentially zero chance of another group choosing to develop a similar tool because of its specific use. I've had PIs who didn't want to publish anything in advance (which I hated), and I've had PIs who like to publish often and freely.\n\nThat said, in the repository's README you can at the top specifically request that any academic uses of the software cite the paper (probably with a link to the DOI or the full citation information). I think there are some software licenses that specifically handle this sort of thing, though I've not looked into that.", "Score": 3, "Replies": []}, {"Comment": "Research papers are a method of communication between researchers and professionals. You can't expect all research to be very useful. Sometimes an idea is good on paper but fails to be efficient in practice. It can still serve as a great steping stone for further research, though.", "Score": 2, "Replies": [{"Reply": "lol bruh you don't have much experience publishing do you? use value has nothing to do with it because it's actually very hard to publish something that's just useful and not hype - i've had a paper rejected 4 times because older (very very dead) projects covered some of the same ideas.\n\nhype and jargon and framing and narrative and latest greatest buzzwords are all that matters (both for getting shitty work published and getting good work rejected).", "Reply Score": 1}]}, {"Comment": "Which journals are usually popular for accepting such research papers?", "Score": 1, "Replies": [{"Reply": "All of them but you're misunderstanding me - I'm saying the majority of systems research is basically at the level of Python package + a whole bunch of jargon, hype, a healthy dose of exaggeration and meaningless eval.", "Reply Score": 1}]}, {"Comment": "I would like to know more can you give any good example.", "Score": 2, "Replies": [{"Reply": "Pretty much all the major libraries have papers attached in the ReadMe or the doc. But here are some examples: \nDeap: https://www.jmlr.org/papers/volume13/fortin12a/fortin12a.pdf\n\nIt doesn't have to be huge in scale, though: \nPyhessian: https://arxiv.org/abs/1912.07145", "Reply Score": 2}]}, {"Comment": "lol bruh you don't have much experience publishing do you? use value has nothing to do with it because it's actually very hard to publish something that's just useful and not hype - i've had a paper rejected 4 times because older (very very dead) projects covered some of the same ideas.\n\nhype and jargon and framing and narrative and latest greatest buzzwords are all that matters (both for getting shitty work published and getting good work rejected).", "Score": 1, "Replies": [{"Reply": "Yeah, I just began my career with only few papers published. I am still optimistic about science, tho :') \n\nIt might be the case that my optimism is misguided, but till now my experience with academia isn't that bad. \n\nWhat is your field of study if you don't mind me asking? Is it machine learning? Because I think that's the most hyped field nowadays", "Reply Score": 1}]}, {"Comment": "All of them but you're misunderstanding me - I'm saying the majority of systems research is basically at the level of Python package + a whole bunch of jargon, hype, a healthy dose of exaggeration and meaningless eval.", "Score": 1, "Replies": []}, {"Comment": "Pretty much all the major libraries have papers attached in the ReadMe or the doc. But here are some examples: \nDeap: https://www.jmlr.org/papers/volume13/fortin12a/fortin12a.pdf\n\nIt doesn't have to be huge in scale, though: \nPyhessian: https://arxiv.org/abs/1912.07145", "Score": 2, "Replies": []}, {"Comment": "Yeah, I just began my career with only few papers published. I am still optimistic about science, tho :') \n\nIt might be the case that my optimism is misguided, but till now my experience with academia isn't that bad. \n\nWhat is your field of study if you don't mind me asking? Is it machine learning? Because I think that's the most hyped field nowadays", "Score": 1, "Replies": [{"Reply": "I'm machine learning adjacent - compilers for ML/DL (think the interior guts/architecture of PyTorch/Tensorflow). The paper I've had rejected is on FPGA synthesis for DNNs.\n\nI have zero optimism/faith/belief in academia/Science. It's a MLM scam with tenured profs at the top.", "Reply Score": 2}]}, {"Comment": "I'm machine learning adjacent - compilers for ML/DL (think the interior guts/architecture of PyTorch/Tensorflow). The paper I've had rejected is on FPGA synthesis for DNNs.\n\nI have zero optimism/faith/belief in academia/Science. It's a MLM scam with tenured profs at the top.", "Score": 2, "Replies": [{"Reply": "This actually sounds like a really cool field of study. \n\nI heard academia is pretty tough in America and the west. It's quite forgiving here in Malaysia.", "Reply Score": 1}]}, {"Comment": "This actually sounds like a really cool field of study. \n\nI heard academia is pretty tough in America and the west. It's quite forgiving here in Malaysia.", "Score": 1, "Replies": []}]},{"Title": "What started the generative AI frenzy?", "Score": 59, "URL": "https://www.reddit.com/r/compsci/comments/1803qzg/what_started_the_generative_ai_frenzy/", "CreatedAt": 1700526362.0, "Full Content": "I understand there have been genAI products before 2022 (like DALL-E in 2021), but nothing gained traction. Then came chatGPT, the biggest hit in genAI on 30 Nov 2022. After which, within a few months, several other genAI products like MidJourney, Dall-E, Stable Diffusion were popularised and more LLM-applications like Bard were created.\n\nWhat kickstarted the genAI frenzy? Did people think for the longest time genAI was too farfetched, until they saw it done with chatGPT and saw it was possible, then got to work? Was it a breakthrough in research for genAI technology or a release of a whitepaper? Was it a particular class of hardware that revolutionalised compute? I'm really curious, but nobody talks about this stuff.", "CntComments": 76, "Comments": [{"Comment": "All you need is attention", "Score": 125, "Replies": [{"Reply": "Very underrated answer. Without this paper from Google Brain, none of it would be possible.", "Reply Score": 23}, {"Reply": "I wouldn't say the transformer paper kickstarted the frenzy, the paper is from 2018 and no one outside of research cared about it until recently. Not to mention that transformers are for sequence modelling and have nothing to do with the whole AI art aspect of genAI.\n\nWhat kickstarted the frenzy was that it got good enough to compete with or outperform humans in several ways.", "Reply Score": 12}, {"Reply": "Achtuchually its attention is all you need", "Reply Score": 8}]}, {"Comment": "Releasing ChatGPT to the public turned out to be a fun toy for people. That's about it imo.", "Score": 80, "Replies": [{"Reply": "ChatGPT actually is a pretty useful tool if you think about it as more like a search engine. But anyway, it really drew hype because it was the first of its kind", "Reply Score": 16}, {"Reply": "I use it as a more helpful rubber duck - 90% of unusable answers but often leads me in the right direction", "Reply Score": 2}, {"Reply": "This is correct and I think maybe just a touch more significant? I do mostly boring crap in image ml and have for years now, but hadn't been paying all that much attention to the LLMs even though in theory I work in the field. Seeing the emergent properties in ChatGPT directly was a Holy Shit moment for me. And that happened because I didn't have a significant time cost tooling up to play with it.", "Reply Score": 4}, {"Reply": "With all the shenanigans going on with OpenAI right now I wonder if they would have made ChatGPT open for the public if it wasn't a nonprofit company", "Reply Score": -2}, {"Reply": "ChatGPT is much more powerful than a toy imo", "Reply Score": 1}]}, {"Comment": "There's a point I didn't see mentioned here yet: availability of powerful GPUs and corresponding deep learning frameworks. There was actually a global shortage of GPUs for a little while due to cryptocurrency mining, from around 2013 even all the way up to 2021. This no doubt also filled NVidia's pockets, allowing them to increase production, develop better cards, etc. If you look at their stock price it has been growing exponentially since about 2016, that's not a coincidence. So while the GPU shortage may have held back ML research for a bit, it also in some way caused the technology leaps that allowed ML research to flourish.", "Score": 14, "Replies": [{"Reply": "GPUs are definitely a major enabling factor. In a sense, they gave us a new version of Dijkstra's [\"software crisis\"](https://en.wikipedia.org/wiki/Software_crisis) from the 70s:\n\n>The major cause of the software crisis is that the machines have become several orders of magnitude more powerful! To put it quite bluntly: as long as there were no machines, programming was no problem at all; when we had a few weak computers, programming became a mild problem, and now we have gigantic computers, programming has become an equally gigantic problem.\n\nModern GPUs provide *ludicrous* amounts of raw FLOPS - 100x-1000x more than your CPU. But writing algorithms that take advantage of this is hard, especially since it's only available in parallel. \n\nNeural networks are great in this situation because they are easy to parallelize, can take advantage of as much compute as you can throw at them, and can be trained to do many tasks.", "Reply Score": 6}]}, {"Comment": "Transformers enabled efficient training over very large datasets, however that is not enough for the performance modern models achieve. Just try to get any base model with no instruction finetuning to do anything useful, it's certainly possible, but you usually need to give it a lot more information about the task in the context (including many samples of similar tasks being correctly solved).\n\nThe other significant advance is instruction finetuning, which was first published somewhere around the beginning of 2022 afaik. This enabled us to easily align huge LLMs to solve tasks for us with relatively few training samples (compared to pretraining requirements). \"Align\" here refers to the idea that LLMs already have the \"knowledge\" required to solve these tasks, it's just that we're teaching them how to answer queries with simple templates. There's also stuff like RLHF for chat, but I'd argue this isn't as important as instruction tuning, it's more of a way to further align a model with human preferences.\n\nAnd of course the importance of computing advances cannot be overstated. Training even just a 1B model was significantly more costly and unefficient just 10 years ago.\n\nAdvances in vision genAI took place somewhat concurrently, but not necessarily for the same reasons, besides computing advances. It was also arguably more gradual than with text, as good image generators existed pre-diffusion models. What made them very popular was the effectiveness of text-to-image diffusion-based models, probably since it's one of the most intuitive usecases of image generation models.", "Score": 6, "Replies": []}, {"Comment": "it's been 20 years in the making bit by bit till it finally became usable. \n\nThe big ones though are:\n\n1. Google created transformers.\n2. Then GANs became better, and could make images.\n3. Diffusion models were created for upscaling\n4. Dalle-e combined all of those and wrote a paper about it, biggest development was adding the CLIP language model that combined text with image ideas.\n5. Stable diffusion made an open source version\n6. Midjourney made a version based on the same research, then made it better using stable diffusion\n7. and then they kept getting better till here we are\n\nChatGPT followed almost the same pattern just different companies/papers\n\n1. Google created transformers\n2. GANs became better, they used it for language models\n3. ChatGPT-2 came out and released research papers\n4. every other company used that research and made theirs better\n5. ChatGPT got better and we got GPT3, it more or less was finally useable, and halfway decent at code\n6. Meta released theirs as open source, a lot of other's followed\n7. Github copilot got in the running, microsoft made deals with openAI, other companies like Clyde formed\n8. GPT got better, we got GPT-4, and now you can make agents.\n9. and here we are. \n\nBut this stuff and research is built on stuff going back to the 50's, it just finally became good enough is really all that happened.", "Score": 10, "Replies": [{"Reply": "MidJourney has existed since before Stable Diffusion. Your timeline also doesn\u2019t include \u201cDiffusion models beat GANs on image synthesis\u201d or \u201cLatent Diffusion Models for high fidelity image synthesis\u201d which is what Stable Diffusion is based off of.", "Reply Score": 3}]}, {"Comment": "The core technology, transformers, has existed since 2017. Researchers/Engineers knew they were useful back then, but it just for contextual prediction. It wasn't until OpenAI trained transformers on instruction following and chat that things really took off.\n\nTransformers --> [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762) \n\nInstruction Tuning --> [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)", "Score": 23, "Replies": [{"Reply": "Those two links are the same", "Reply Score": 21}, {"Reply": "I think you mean to link InstructGPT in the second paper? Anyway, that's certainly one the most significant results. Generative Transformers were pretty weak at chatting and following instructions prior to these results.", "Reply Score": 2}, {"Reply": "NLP chat bots have existed so much longer than that.  [Watson](https://en.wikipedia.org/wiki/IBM_Watson) has existed since at least 2006, [A.L.I.C.E.](https://en.wikipedia.org/wiki/Artificial_Linguistic_Internet_Computer_Entity) since 1995, and [ELIZA](https://en.wikipedia.org/wiki/ELIZA) since the 1960s.\n\nAI and neural nets are nothing new.  Natural language processing and cross referencing with a set of learned data isn't new technology either.  GPT is just the latest iteration, built on decades of research and other successful projects.  chatGPT just brought attention to the masses of this generation, like Watson did a decade ago when it beat Ken Jennings on Jeopardy, or Deep Blue did a decade before that when it beat Gary Kasparov in chess.  Sure, this latest iteration has new features, and can process larger datasets faster, and performs in slightly different methods than past programs, but the same can be said for every generation before it.", "Reply Score": -4}]}, {"Comment": "GAN models spawned a heavy interest in the field. Things like thispersondoesnotexist.com and other early sites.", "Score": 7, "Replies": []}, {"Comment": "It became accessible and society is now more in touch with tech.", "Score": 3, "Replies": []}, {"Comment": "I think Midjourney and Stable Diffusion started gaining traction in summer 2022, before ChatGPT 3.5 in November 2022.", "Score": 3, "Replies": []}, {"Comment": "Its a strange time. When I was in college Alexnet was the big thing. Im not even 30. A lot has changed", "Score": 3, "Replies": []}, {"Comment": "Your timeline post-DALLE release is a little off. Last year we initially saw DALLE2 around spring time, then shortly after stable diffusion and midjourney popped up in the summer. ChatGPT didnt come until a few months after those, in November like you said.", "Score": 2, "Replies": [{"Reply": "yeah you're right, and I'm fully aware of that. I'm saying that MJ and SD were popularised after chatGPT was released. there wasn't much traction for MJ & SD before Nov 2022", "Reply Score": 0}]}, {"Comment": "Before ChatGPT there were GPT-3.5, GPT-3, GPT-2 and GPT-1. It's not that it came out of nowhere.", "Score": 2, "Replies": []}, {"Comment": "Stable diffusion + webui  was a pivot point, because many otaku's around the world could make a shitton of 2d hentai porn images by that model.", "Score": 2, "Replies": []}, {"Comment": "GPT-3  \nOpenAI tried throwing almost a terrabyte of VRAM and half the internet at a model, and it made something that could actually generate believable content. Being able to generate language that makes sense and code that compiles directly from a prompt is actually sorta useful sometimes unlike all previous generative AI.", "Score": 2, "Replies": []}, {"Comment": "ChatGPT turned out to be more than an order of magnitude more capable than what the public thought was possible. I know my shit, keep up with the tech, know exactly how it works (by that I mean the architecture in general, not their secret sauce if there is any) and even I was gobsmacked. It does not just represent a fun toy, it is the harbinger of the transformation the world is about to have.\n\nGenAI has been a thing for quite a while, even before ChatGPT. GenAI that can do language? Can understand you, the context, and generate answers? That was quite new for the public.", "Score": 6, "Replies": []}, {"Comment": "They finally stole enough copyright material that it could spit out something coherent.", "Score": 6, "Replies": []}, {"Comment": "End-product-wise, ChatGPT is the biggest thing, but Stable Diffusion and NovelAI were before it, and AI dungeon before those.", "Score": 3, "Replies": []}, {"Comment": "[The generative AI revolution has begun - how did we get here?](https://arstechnica.com/gadgets/2023/01/the-generative-ai-revolution-has-begun-how-did-we-get-here/)", "Score": 1, "Replies": []}, {"Comment": "That one stupid fucker who couldn't bother to draw.", "Score": 0, "Replies": []}, {"Comment": "A mix of Quality Improvement and Manufactured Hype (aka. good PR)", "Score": 0, "Replies": []}, {"Comment": "Building generative AI tools became a lot easier because companies like OpenAI built tools that make it dead simple to fine-tune an LLM to behave exactly how you want and ship it to customers.\n\nSo everyone started building them, naturally.", "Score": 1, "Replies": []}, {"Comment": "It's cool and gives great results. Unlike a few years ago where AI generation is just some faces or style transfers that get old after a while, now the fun never ends.", "Score": 1, "Replies": []}, {"Comment": "I think mostly tooling tbh. A lot of the math and science needed had already been done. Neural networks and other function appropriators aren't new tech by any means.\n\nI think the ease of use of ML/AI frameworks allowed people from a lot of different disciplines to jump into it without having to worry about the lower level components that they abstract.", "Score": 1, "Replies": []}, {"Comment": "> What kickstarted the genAI frenzy?\n\nThe actual answer is 2015 DeepDream and its successors, early \"weird AI art\".\nhttps://en.wikipedia.org/wiki/DeepDream", "Score": 1, "Replies": []}, {"Comment": "It seems like the pivotal moment was indeed the success of chatGPT, marking a turning point in the perception and realization of genAI possibilities. The domino effect of innovations like [generative ai for digital art](https://www.bombaysoftwares.com/blog/generative-ai-elevating-digital-art-and-design-creativity) that followed suggests a collective realization that genAI was not just a concept but a tangible frontier worth exploring.", "Score": 1, "Replies": []}, {"Comment": "Very underrated answer. Without this paper from Google Brain, none of it would be possible.", "Score": 23, "Replies": [{"Reply": "[deleted]", "Reply Score": -16}]}, {"Comment": "I wouldn't say the transformer paper kickstarted the frenzy, the paper is from 2018 and no one outside of research cared about it until recently. Not to mention that transformers are for sequence modelling and have nothing to do with the whole AI art aspect of genAI.\n\nWhat kickstarted the frenzy was that it got good enough to compete with or outperform humans in several ways.", "Score": 12, "Replies": [{"Reply": ">Not to mention that transformers are for sequence modelling and have nothing to do with the whole AI art aspect of genAI. \n\nImage generators are half image model, half language model - the language model is what allows them to understand prompts, and it's a transformer. \n\nThe U-Nets commonly used for the image diffusion model also have attention layers and are a sort of CNN/Transformer hybrid.", "Reply Score": 14}]}, {"Comment": "Achtuchually its attention is all you need", "Score": 8, "Replies": []}, {"Comment": "ChatGPT actually is a pretty useful tool if you think about it as more like a search engine. But anyway, it really drew hype because it was the first of its kind", "Score": 16, "Replies": [{"Reply": "Given that ChatGPT regularly returns completely fabricated paper titles (using real researchers' names), no it's nothing like a search engine.", "Reply Score": 24}]}, {"Comment": "I use it as a more helpful rubber duck - 90% of unusable answers but often leads me in the right direction", "Score": 2, "Replies": []}, {"Comment": "This is correct and I think maybe just a touch more significant? I do mostly boring crap in image ml and have for years now, but hadn't been paying all that much attention to the LLMs even though in theory I work in the field. Seeing the emergent properties in ChatGPT directly was a Holy Shit moment for me. And that happened because I didn't have a significant time cost tooling up to play with it.", "Score": 4, "Replies": [{"Reply": "MidJourney in particular fundamentally rocked my ideas of what computers are capable of. \n\nI think there is a large category of problems that are only solvable with huge amounts of prior information. For example, if you want to generate pictures of objects, you're going to need a lot of information about what objects look like. There's just no way around it.\n\nThese large neural networks are very good at generalizing from prior information, in very abstract ways that traditional algorithms have never been flexible enough to manage.", "Reply Score": 7}, {"Reply": "I'm not LLM but have a MS in Comp Sci and when it came out, I showed it to everyone in my family.  So I'm guilty.  Then after about a month I realized it was just wrong probably more than half the time about any one topic and lost interest.  I do use Github co-pilot for development which is useful and targeted but that was out long before ChatGPT mania started. It's not really right a lot of the time either but is a handy quick code variable substitute/paster intellisense.", "Reply Score": 1}]}, {"Comment": "With all the shenanigans going on with OpenAI right now I wonder if they would have made ChatGPT open for the public if it wasn't a nonprofit company", "Score": -2, "Replies": [{"Reply": "I'm thinking they decided to release it because they couldn't improve it anymore.  BUT what a toy to show off!  Google's LLM was in the same boat and didn't show their version for a reason.  That reason being that it is not really that good either.  But Google was forced to push out Bard when OpenAI made ChatGPT public.  It's a money game.  OpenAI stole the show with their toy and were the upstarts.  It was definitely a cool toy.  But likely they hit a wall scientifically and didn't know what to do from there. Altman knew it would be a good show.  He was right.  He made a lot of money from that show.", "Reply Score": -2}]}, {"Comment": "ChatGPT is much more powerful than a toy imo", "Score": 1, "Replies": []}, {"Comment": "GPUs are definitely a major enabling factor. In a sense, they gave us a new version of Dijkstra's [\"software crisis\"](https://en.wikipedia.org/wiki/Software_crisis) from the 70s:\n\n>The major cause of the software crisis is that the machines have become several orders of magnitude more powerful! To put it quite bluntly: as long as there were no machines, programming was no problem at all; when we had a few weak computers, programming became a mild problem, and now we have gigantic computers, programming has become an equally gigantic problem.\n\nModern GPUs provide *ludicrous* amounts of raw FLOPS - 100x-1000x more than your CPU. But writing algorithms that take advantage of this is hard, especially since it's only available in parallel. \n\nNeural networks are great in this situation because they are easy to parallelize, can take advantage of as much compute as you can throw at them, and can be trained to do many tasks.", "Score": 6, "Replies": []}, {"Comment": "MidJourney has existed since before Stable Diffusion. Your timeline also doesn\u2019t include \u201cDiffusion models beat GANs on image synthesis\u201d or \u201cLatent Diffusion Models for high fidelity image synthesis\u201d which is what Stable Diffusion is based off of.", "Score": 3, "Replies": []}, {"Comment": "Those two links are the same", "Score": 21, "Replies": []}, {"Comment": "I think you mean to link InstructGPT in the second paper? Anyway, that's certainly one the most significant results. Generative Transformers were pretty weak at chatting and following instructions prior to these results.", "Score": 2, "Replies": []}, {"Comment": "NLP chat bots have existed so much longer than that.  [Watson](https://en.wikipedia.org/wiki/IBM_Watson) has existed since at least 2006, [A.L.I.C.E.](https://en.wikipedia.org/wiki/Artificial_Linguistic_Internet_Computer_Entity) since 1995, and [ELIZA](https://en.wikipedia.org/wiki/ELIZA) since the 1960s.\n\nAI and neural nets are nothing new.  Natural language processing and cross referencing with a set of learned data isn't new technology either.  GPT is just the latest iteration, built on decades of research and other successful projects.  chatGPT just brought attention to the masses of this generation, like Watson did a decade ago when it beat Ken Jennings on Jeopardy, or Deep Blue did a decade before that when it beat Gary Kasparov in chess.  Sure, this latest iteration has new features, and can process larger datasets faster, and performs in slightly different methods than past programs, but the same can be said for every generation before it.", "Score": -4, "Replies": [{"Reply": "This comparison feels a little disingenuous. It's like looking at a car and saying it's nothing new, we've had carts with wheels since antiquity. \n\nWhat's new in the last few years is the ability to create very large programs through optimization rather than construction. Training a neural network is not just a slightly different method, it's a *method for searching for methods.* This is much broader than chatbots; it allows us to tackle all sorts of problems we don't know how to write programs to solve. \n\nIn the last few years, the same kind of neural networks have become state of the art on a bunch of other \"impossible\" problems like image superresolution, audio-to-audio translation, protein folding, open-world robot control, etc. A single general-purpose algorithm beats all the handcrafted specialized algorithms we've ever found for these tasks.", "Reply Score": 24}]}, {"Comment": "yeah you're right, and I'm fully aware of that. I'm saying that MJ and SD were popularised after chatGPT was released. there wasn't much traction for MJ & SD before Nov 2022", "Score": 0, "Replies": []}, {"Comment": "[deleted]", "Score": -16, "Replies": [{"Reply": ">nobody cared before SD1.5, and nobody cared about SD1.5 before Midjourney, and nobody cared about midjourney before GPT3.x, and nobody cared about GPT3.x before Playground, and nobody cared about Playground before Markov Chains.\n\nYou look like GPT-2 here lol", "Reply Score": 13}]}, {"Comment": ">Not to mention that transformers are for sequence modelling and have nothing to do with the whole AI art aspect of genAI. \n\nImage generators are half image model, half language model - the language model is what allows them to understand prompts, and it's a transformer. \n\nThe U-Nets commonly used for the image diffusion model also have attention layers and are a sort of CNN/Transformer hybrid.", "Score": 14, "Replies": [{"Reply": ">Image generators are half image model, half language model - the language model is what allows them to understand prompts, and it's a transformer.\n\nDepends on the embedding model, but the commonly used CLIP does use a transformer, true - my bad.", "Reply Score": 1}]}, {"Comment": "Given that ChatGPT regularly returns completely fabricated paper titles (using real researchers' names), no it's nothing like a search engine.", "Score": 24, "Replies": [{"Reply": "It\u2019s not \u201cnothing like a search engine\u201d lol. It answers probably 90% of the questions I historically asked Google. \n\nYes, they don\u2019t share 100% functionality or use cases, but the stuff they overlap in, ChatGPT wins. Outside is ChatGPT, Bard literally does do search. As it gets better, you should expect to see ChatGPT-like answers with citations\u2026probably from both apps.", "Reply Score": 11}, {"Reply": "I've been using ChatGPT for 3 different programming projects where I didn't know the language or some framework/library and it's been extremely useful. It's completely replaced Google and StackOverflow for me, since you can ask for precisely what you want (or follow up with corrections if it \"misunderstood\" what you asked for), whether it's explanations (\"how does X work?\", \"why is X like that?\") or requests for examples (\"show me how to do X in Y\").\n\nIt occasionally comes up with something bogus like a function that doesn't exist -- that's not a problem. You just tell it \"I ran your example and it gave me this error\" and it will apologize and say \"you're right, X doesn't exist, let me give you another example\".\n\nIt's like having an assistant who can look up exactly what you need on the spot, in a much more refined way than Google or StackOverflow (or even API documentation).", "Reply Score": 12}, {"Reply": "Don't be too hard-headed. It's good as a search engine that needs verification - it gave me sensible template code and links to 3 relevant (real) libraries for a pipeline project that I am working on. It won't do your job for you, but in some areas it really can cut down the initial info-gathering searches.", "Reply Score": 4}, {"Reply": "Its very useful as a search tool, i use it nearly every day. But like wikipedia, its not a trusted source. its even less trustworthy than wikipedia. But its still incredibly useful if you clearly understand its limitations, and verify anything it puts out if you need to know if its true.\n\nFor instance, you can go \"I am researching the antebellum period in southern united states. Please give me a list of 40 non-fiction books on this era\" \n\nAnd you can have a very useful conversation like this: https://chat.openai.com/share/b303cda6-ba9d-4eb1-8c63-b602b0882c32\n\nI just had that, I didnt check that books because this is an example, but what i would guess is a few of them are imagined, a few more are innaccurately titled but real, and most are real.\n\nAs you can see, i asked further questions and got more information.\n\nI would not ever trust this information unverified by me, but I can use this information within a process of research.\n\nTHe same is true of writing an article or a million other uses. For an article, I suggest building it up first with brainstorming ideas, then asking for an outline, then asking for a modified outline, then editing yourself, then building up from there.", "Reply Score": 2}, {"Reply": "You would've hated Google in 1997.", "Reply Score": 2}, {"Reply": "Sure, but I can ask it \"what are some good starting points to learn about X\". It will give me a list of 10-20 things, which I can then refine myself", "Reply Score": 1}]}, {"Comment": "MidJourney in particular fundamentally rocked my ideas of what computers are capable of. \n\nI think there is a large category of problems that are only solvable with huge amounts of prior information. For example, if you want to generate pictures of objects, you're going to need a lot of information about what objects look like. There's just no way around it.\n\nThese large neural networks are very good at generalizing from prior information, in very abstract ways that traditional algorithms have never been flexible enough to manage.", "Score": 7, "Replies": []}, {"Comment": "I'm not LLM but have a MS in Comp Sci and when it came out, I showed it to everyone in my family.  So I'm guilty.  Then after about a month I realized it was just wrong probably more than half the time about any one topic and lost interest.  I do use Github co-pilot for development which is useful and targeted but that was out long before ChatGPT mania started. It's not really right a lot of the time either but is a handy quick code variable substitute/paster intellisense.", "Score": 1, "Replies": [{"Reply": "I'm more excited about what could be done with this form of data-driven computation, rather than specifically ChatGPT as a product. \n\nSometimes wrong or not, these large unsupervised models are achieving breakthroughs across a bunch of problems that have traditionally been very hard for computers.", "Reply Score": -2}]}, {"Comment": "I'm thinking they decided to release it because they couldn't improve it anymore.  BUT what a toy to show off!  Google's LLM was in the same boat and didn't show their version for a reason.  That reason being that it is not really that good either.  But Google was forced to push out Bard when OpenAI made ChatGPT public.  It's a money game.  OpenAI stole the show with their toy and were the upstarts.  It was definitely a cool toy.  But likely they hit a wall scientifically and didn't know what to do from there. Altman knew it would be a good show.  He was right.  He made a lot of money from that show.", "Score": -2, "Replies": []}, {"Comment": "This comparison feels a little disingenuous. It's like looking at a car and saying it's nothing new, we've had carts with wheels since antiquity. \n\nWhat's new in the last few years is the ability to create very large programs through optimization rather than construction. Training a neural network is not just a slightly different method, it's a *method for searching for methods.* This is much broader than chatbots; it allows us to tackle all sorts of problems we don't know how to write programs to solve. \n\nIn the last few years, the same kind of neural networks have become state of the art on a bunch of other \"impossible\" problems like image superresolution, audio-to-audio translation, protein folding, open-world robot control, etc. A single general-purpose algorithm beats all the handcrafted specialized algorithms we've ever found for these tasks.", "Score": 24, "Replies": [{"Reply": "> Training a neural network is not just a slightly different method, it's a method for searching for methods. This is much broader than chatbots; \n\nNo, my point is neural nets aren't anything new, they date back to at least the 1940s.  And chatGPT is really nothing more than a chatbot with a larger dataset.  The only thing that's changed over the years is the size of storage and the speed of processors.  More storage = more knowledge, and more processing speed = more time to search through responses to find something more correct.\n\n> it allows us to tackle all sorts of problems we don't know how to write programs to solve. \n\nSo did Watson, when it was applied to the [healthcare field](https://en.wikipedia.org/wiki/IBM_Watson#Healthcare) in 2011, confirming and even suggesting diagnoses doctors could only guess.  Look into [folding@home](https://en.wikipedia.org/wiki/Folding@home), which wasn't too different from neural nets in its use of distributed computing, and its ability to find *methods for searching for methods* for combating disease via protein folding, and has existed since 2000.\n\nYes, the technology has gotten better, but more importantly, it's gotten larger and faster, made possible by cheaper storage and processing power.  The technology isn't all that new, though some methods are certainly becoming more refined.", "Reply Score": -7}]}, {"Comment": ">nobody cared before SD1.5, and nobody cared about SD1.5 before Midjourney, and nobody cared about midjourney before GPT3.x, and nobody cared about GPT3.x before Playground, and nobody cared about Playground before Markov Chains.\n\nYou look like GPT-2 here lol", "Score": 13, "Replies": [{"Reply": "[deleted]", "Reply Score": -3}]}, {"Comment": ">Image generators are half image model, half language model - the language model is what allows them to understand prompts, and it's a transformer.\n\nDepends on the embedding model, but the commonly used CLIP does use a transformer, true - my bad.", "Score": 1, "Replies": [{"Reply": "Models like stable diffusion also utilize cross attention.", "Reply Score": 2}]}, {"Comment": "It\u2019s not \u201cnothing like a search engine\u201d lol. It answers probably 90% of the questions I historically asked Google. \n\nYes, they don\u2019t share 100% functionality or use cases, but the stuff they overlap in, ChatGPT wins. Outside is ChatGPT, Bard literally does do search. As it gets better, you should expect to see ChatGPT-like answers with citations\u2026probably from both apps.", "Score": 11, "Replies": []}, {"Comment": "I've been using ChatGPT for 3 different programming projects where I didn't know the language or some framework/library and it's been extremely useful. It's completely replaced Google and StackOverflow for me, since you can ask for precisely what you want (or follow up with corrections if it \"misunderstood\" what you asked for), whether it's explanations (\"how does X work?\", \"why is X like that?\") or requests for examples (\"show me how to do X in Y\").\n\nIt occasionally comes up with something bogus like a function that doesn't exist -- that's not a problem. You just tell it \"I ran your example and it gave me this error\" and it will apologize and say \"you're right, X doesn't exist, let me give you another example\".\n\nIt's like having an assistant who can look up exactly what you need on the spot, in a much more refined way than Google or StackOverflow (or even API documentation).", "Score": 12, "Replies": [{"Reply": "I've tried to use it for help and it literally just makes up functions or command syntax that sounds plausible.", "Reply Score": 7}, {"Reply": "Are you using Javascript/Typescript, Python, or something else? I've found it helpful but it's gotten consistently worse over time, especially for writing Scala.", "Reply Score": 4}]}, {"Comment": "Don't be too hard-headed. It's good as a search engine that needs verification - it gave me sensible template code and links to 3 relevant (real) libraries for a pipeline project that I am working on. It won't do your job for you, but in some areas it really can cut down the initial info-gathering searches.", "Score": 4, "Replies": []}, {"Comment": "Its very useful as a search tool, i use it nearly every day. But like wikipedia, its not a trusted source. its even less trustworthy than wikipedia. But its still incredibly useful if you clearly understand its limitations, and verify anything it puts out if you need to know if its true.\n\nFor instance, you can go \"I am researching the antebellum period in southern united states. Please give me a list of 40 non-fiction books on this era\" \n\nAnd you can have a very useful conversation like this: https://chat.openai.com/share/b303cda6-ba9d-4eb1-8c63-b602b0882c32\n\nI just had that, I didnt check that books because this is an example, but what i would guess is a few of them are imagined, a few more are innaccurately titled but real, and most are real.\n\nAs you can see, i asked further questions and got more information.\n\nI would not ever trust this information unverified by me, but I can use this information within a process of research.\n\nTHe same is true of writing an article or a million other uses. For an article, I suggest building it up first with brainstorming ideas, then asking for an outline, then asking for a modified outline, then editing yourself, then building up from there.", "Score": 2, "Replies": []}, {"Comment": "You would've hated Google in 1997.", "Score": 2, "Replies": [{"Reply": "Why the fuck are we regressing to 1997.", "Reply Score": 4}]}, {"Comment": "Sure, but I can ask it \"what are some good starting points to learn about X\". It will give me a list of 10-20 things, which I can then refine myself", "Score": 1, "Replies": []}, {"Comment": "I'm more excited about what could be done with this form of data-driven computation, rather than specifically ChatGPT as a product. \n\nSometimes wrong or not, these large unsupervised models are achieving breakthroughs across a bunch of problems that have traditionally been very hard for computers.", "Score": -2, "Replies": [{"Reply": "downvoted, but you are right. chatgpt is specifically designed to be a general tool. its forcibly trained to give the user what it wants, and answer the question even if it has to make stuff up. Its semi kneecapped with a bunch of restraints, and it isnt hypertrained for specific tasks, which other ai tools are beginning to be.\n\nFor a visual example of this for anyone curious, you can go to civit.ai and look at the various loras. notice there can be a lora which is like a \"mod\" that real help make a picture of like john wick or whatever. or a lora that will generate moss really well.  these are hyper-focused to do an intended task well.\n\nthe same can be done with text models. and you can have multiple autonomous ai agents working together. so there was the research paper where they had a bunch of separate ais building a game, one was an \"Artist\" one a \"coder\" etc etc, and they would share the information they generated between each other and alter improve it and then give it back and repeat this to iterate over time improving the game.\n\nthe same can be done with writing a people, having a fact checker bot, a grammar bot, a creative bot etc. each bot can suggest a change and another can override it if necessary.", "Reply Score": 1}]}, {"Comment": "> Training a neural network is not just a slightly different method, it's a method for searching for methods. This is much broader than chatbots; \n\nNo, my point is neural nets aren't anything new, they date back to at least the 1940s.  And chatGPT is really nothing more than a chatbot with a larger dataset.  The only thing that's changed over the years is the size of storage and the speed of processors.  More storage = more knowledge, and more processing speed = more time to search through responses to find something more correct.\n\n> it allows us to tackle all sorts of problems we don't know how to write programs to solve. \n\nSo did Watson, when it was applied to the [healthcare field](https://en.wikipedia.org/wiki/IBM_Watson#Healthcare) in 2011, confirming and even suggesting diagnoses doctors could only guess.  Look into [folding@home](https://en.wikipedia.org/wiki/Folding@home), which wasn't too different from neural nets in its use of distributed computing, and its ability to find *methods for searching for methods* for combating disease via protein folding, and has existed since 2000.\n\nYes, the technology has gotten better, but more importantly, it's gotten larger and faster, made possible by cheaper storage and processing power.  The technology isn't all that new, though some methods are certainly becoming more refined.", "Score": -7, "Replies": [{"Reply": "Neural nets are not new, but they didn't really work back then. They were tiny, nobody knew how to train them, and computers weren't fast enough anyway. I'm not going to downplay the importance of faster computers, but new ideas have also been important - attention, dropout, normalization, generative training objectives, etc.\n\nIt's like [the steam engine that the greeks had](https://en.wikipedia.org/wiki/Aeolipile), compared to the steam engine of the 1800s. They had the idea, but they couldn't really build a useful one. \n\n>more processing speed = more time to search through responses to find something more correct.\n\nI think you have an incorrect idea about how unsupervised learning works. It is not a program that searches through the data to find the right answer; it's more like a program created from the inverse of the data. The dataset is discarded after training, all the information is encoded into the weights in a very abstract way.", "Reply Score": 12}, {"Reply": ">And chatGPT is really nothing more than a chatbot with a larger dataset. The only thing that's changed over the years is the size of storage and the speed of processors.\n\nIt's a transformer, an architecture that wasn't publicized until 2017. It's not just \"bigger and faster,\" it's literally using techniques that were only recently discovered and implemented. You can't cite perceptrons from the 40's and compare those to what GPT is.\n\n>Look into folding@home, which wasn't too different from neural nets in its use of distributed computing,\n\nIt's completely different. It just runs extremely intensive simulations.\n\n>its ability to find methods for searching for methods\n\nThat's not what it does. All folding@home does is simulate protein folds. It literally doesn't search for or discover anything on its own, it just simulates and collects the result of folds for use in actual laboratory studies.", "Reply Score": 5}]}, {"Comment": "[deleted]", "Score": -3, "Replies": [{"Reply": "nobody cared about Markov Chains before GPT3, and nobody cared about GPT3 before GPT2, and nobody cared about GPT2 before GPT1, and nobody cared about GPT1 before GPT0, and nobody cared about GPT0 before GPT-1, and nobody cared about GPT-1 before GPT-2, and nobody cared about GPT-2 before GPT-3, and nobody cared about GPT-3 before GPT-4, and nobody cared about GPT-4 before GPT-5, and nobody cared about GPT-5 before GPT-6, and nobody cared about GPT-6 before GPT-7, and nobody cared about GPT-7 before GPT-8, and nobody cared about GPT-8 before GPT-9, and nobody cared about GPT-9 before GPT-10, and nobody cared about GPT-10 before GPT-11, and nobody cared about GPT-11 before GPT-12, and nobody cared about GPT-12 before GPT-13, and nobody cared about GPT-13 before GPT-14, and nobody cared about GPT-14 before GPT-15, and nobody cared about GPT-15 before GPT-16, and nobody cared about GPT-16 before GPT-17, and nobody cared about GPT-17 before GPT-18, and nobody cared about GPT-18 before GPT-19, and nobody cared about GPT-19 before GPT-20, and nobody cared about GPT-20 before GPT-21, and nobody cared about GPT-21 before GPT-22, and nobody cared about GPT-22 before GPT-23, and nobody cared about GPT-23 before GPT-24, and nobody cared about GPT-24 before GPT-25, and nobody cared about GPT-25 before GPT-26, and nobody cared about GPT-26 before GPT-27, and nobody cared about GPT-27 before GPT-28, and nobody cared about GPT-28 before GPT-29, and nobody cared about GPT-29 before GPT-30, and nobody cared about GPT-30 before GPT-31, and nobody cared about GPT-31 before GPT-32", "Reply Score": 11}]}, {"Comment": "Models like stable diffusion also utilize cross attention.", "Score": 2, "Replies": []}, {"Comment": "I've tried to use it for help and it literally just makes up functions or command syntax that sounds plausible.", "Score": 7, "Replies": []}, {"Comment": "Are you using Javascript/Typescript, Python, or something else? I've found it helpful but it's gotten consistently worse over time, especially for writing Scala.", "Score": 4, "Replies": []}, {"Comment": "Why the fuck are we regressing to 1997.", "Score": 4, "Replies": [{"Reply": "We're not, ChatGPT is *considerably* better than Google was in 1997. My point is that new technology is often janky at first.\n\nIn my opinion, if you have tried GPT4 with an open mind and you walk away thinking it's useless, then you're really, *thoroughly* stupid and/or unimaginative.", "Reply Score": 1}]}, {"Comment": "downvoted, but you are right. chatgpt is specifically designed to be a general tool. its forcibly trained to give the user what it wants, and answer the question even if it has to make stuff up. Its semi kneecapped with a bunch of restraints, and it isnt hypertrained for specific tasks, which other ai tools are beginning to be.\n\nFor a visual example of this for anyone curious, you can go to civit.ai and look at the various loras. notice there can be a lora which is like a \"mod\" that real help make a picture of like john wick or whatever. or a lora that will generate moss really well.  these are hyper-focused to do an intended task well.\n\nthe same can be done with text models. and you can have multiple autonomous ai agents working together. so there was the research paper where they had a bunch of separate ais building a game, one was an \"Artist\" one a \"coder\" etc etc, and they would share the information they generated between each other and alter improve it and then give it back and repeat this to iterate over time improving the game.\n\nthe same can be done with writing a people, having a fact checker bot, a grammar bot, a creative bot etc. each bot can suggest a change and another can override it if necessary.", "Score": 1, "Replies": []}, {"Comment": "Neural nets are not new, but they didn't really work back then. They were tiny, nobody knew how to train them, and computers weren't fast enough anyway. I'm not going to downplay the importance of faster computers, but new ideas have also been important - attention, dropout, normalization, generative training objectives, etc.\n\nIt's like [the steam engine that the greeks had](https://en.wikipedia.org/wiki/Aeolipile), compared to the steam engine of the 1800s. They had the idea, but they couldn't really build a useful one. \n\n>more processing speed = more time to search through responses to find something more correct.\n\nI think you have an incorrect idea about how unsupervised learning works. It is not a program that searches through the data to find the right answer; it's more like a program created from the inverse of the data. The dataset is discarded after training, all the information is encoded into the weights in a very abstract way.", "Score": 12, "Replies": [{"Reply": "> I think you have an incorrect idea about how unsupervised learning works. It is not a program that searches through the data to find the right answer; it's more like a program created from the inverse of the data. The dataset is discarded after training, all the information is encoded into the weights in a very abstract way.\n\nYou're right, but more processing power means a better understanding before it discards the data, given the same amount of time.", "Reply Score": 0}]}, {"Comment": ">And chatGPT is really nothing more than a chatbot with a larger dataset. The only thing that's changed over the years is the size of storage and the speed of processors.\n\nIt's a transformer, an architecture that wasn't publicized until 2017. It's not just \"bigger and faster,\" it's literally using techniques that were only recently discovered and implemented. You can't cite perceptrons from the 40's and compare those to what GPT is.\n\n>Look into folding@home, which wasn't too different from neural nets in its use of distributed computing,\n\nIt's completely different. It just runs extremely intensive simulations.\n\n>its ability to find methods for searching for methods\n\nThat's not what it does. All folding@home does is simulate protein folds. It literally doesn't search for or discover anything on its own, it just simulates and collects the result of folds for use in actual laboratory studies.", "Score": 5, "Replies": [{"Reply": "> It's a transformer, an architecture that wasn't publicized until 2017. It's not just \"bigger and faster,\" it's literally using techniques that were only recently discovered and implemented.\n\nSee\n\n>  Sure, this latest iteration has new features, and can process larger datasets faster, and performs in slightly different methods than past programs, but the same can be said for every generation before it.\n\nProtein folding was mentioned by the comment above mine.\n\nLook at OP's question, then take my comments in that context.  Generative AI is nothing new.", "Reply Score": -4}]}, {"Comment": "nobody cared about Markov Chains before GPT3, and nobody cared about GPT3 before GPT2, and nobody cared about GPT2 before GPT1, and nobody cared about GPT1 before GPT0, and nobody cared about GPT0 before GPT-1, and nobody cared about GPT-1 before GPT-2, and nobody cared about GPT-2 before GPT-3, and nobody cared about GPT-3 before GPT-4, and nobody cared about GPT-4 before GPT-5, and nobody cared about GPT-5 before GPT-6, and nobody cared about GPT-6 before GPT-7, and nobody cared about GPT-7 before GPT-8, and nobody cared about GPT-8 before GPT-9, and nobody cared about GPT-9 before GPT-10, and nobody cared about GPT-10 before GPT-11, and nobody cared about GPT-11 before GPT-12, and nobody cared about GPT-12 before GPT-13, and nobody cared about GPT-13 before GPT-14, and nobody cared about GPT-14 before GPT-15, and nobody cared about GPT-15 before GPT-16, and nobody cared about GPT-16 before GPT-17, and nobody cared about GPT-17 before GPT-18, and nobody cared about GPT-18 before GPT-19, and nobody cared about GPT-19 before GPT-20, and nobody cared about GPT-20 before GPT-21, and nobody cared about GPT-21 before GPT-22, and nobody cared about GPT-22 before GPT-23, and nobody cared about GPT-23 before GPT-24, and nobody cared about GPT-24 before GPT-25, and nobody cared about GPT-25 before GPT-26, and nobody cared about GPT-26 before GPT-27, and nobody cared about GPT-27 before GPT-28, and nobody cared about GPT-28 before GPT-29, and nobody cared about GPT-29 before GPT-30, and nobody cared about GPT-30 before GPT-31, and nobody cared about GPT-31 before GPT-32", "Score": 11, "Replies": []}, {"Comment": "We're not, ChatGPT is *considerably* better than Google was in 1997. My point is that new technology is often janky at first.\n\nIn my opinion, if you have tried GPT4 with an open mind and you walk away thinking it's useless, then you're really, *thoroughly* stupid and/or unimaginative.", "Score": 1, "Replies": [{"Reply": "Lets just say it's mostly useless for any work that requires actual thought or innovation.", "Reply Score": 3}]}, {"Comment": "> I think you have an incorrect idea about how unsupervised learning works. It is not a program that searches through the data to find the right answer; it's more like a program created from the inverse of the data. The dataset is discarded after training, all the information is encoded into the weights in a very abstract way.\n\nYou're right, but more processing power means a better understanding before it discards the data, given the same amount of time.", "Score": 0, "Replies": []}, {"Comment": "> It's a transformer, an architecture that wasn't publicized until 2017. It's not just \"bigger and faster,\" it's literally using techniques that were only recently discovered and implemented.\n\nSee\n\n>  Sure, this latest iteration has new features, and can process larger datasets faster, and performs in slightly different methods than past programs, but the same can be said for every generation before it.\n\nProtein folding was mentioned by the comment above mine.\n\nLook at OP's question, then take my comments in that context.  Generative AI is nothing new.", "Score": -4, "Replies": [{"Reply": ">See\n\n>>Sure, this latest iteration has new features, and can process larger datasets faster, and performs in slightly different methods than past programs, but the same can be said for every generation before it.\n\nYeah that doesn't answer anything. The same can't be said for the tiny incremental steps that have come before, which is precisely why generative AI has been completely stagnant since the last AI winter.\n\n>Protein folding was mentioned by the comment above mine\n\nThat doesn't make comparing folding@home to modern AI correct. That poster was referencing [DeepMind's nearly perfect performance in CASP using AlphaFold](https://www.science.org/doi/10.1126/science.370.6521.1144), which even helped resolve a 10 year study on an unsolved protein structure.\n\n>Look at OP's question, then take my comments in that context.\n\nOP was asking *why now*, what you're saying doesn't add anything in that context beyond trying to downplay the interest.", "Reply Score": 4}]}, {"Comment": "Lets just say it's mostly useless for any work that requires actual thought or innovation.", "Score": 3, "Replies": [{"Reply": "[deleted]", "Reply Score": 1}]}, {"Comment": ">See\n\n>>Sure, this latest iteration has new features, and can process larger datasets faster, and performs in slightly different methods than past programs, but the same can be said for every generation before it.\n\nYeah that doesn't answer anything. The same can't be said for the tiny incremental steps that have come before, which is precisely why generative AI has been completely stagnant since the last AI winter.\n\n>Protein folding was mentioned by the comment above mine\n\nThat doesn't make comparing folding@home to modern AI correct. That poster was referencing [DeepMind's nearly perfect performance in CASP using AlphaFold](https://www.science.org/doi/10.1126/science.370.6521.1144), which even helped resolve a 10 year study on an unsolved protein structure.\n\n>Look at OP's question, then take my comments in that context.\n\nOP was asking *why now*, what you're saying doesn't add anything in that context beyond trying to downplay the interest.", "Score": 4, "Replies": [{"Reply": "\"Why now\" is that chatGPT [helped a bunch of middle schoolers write papers for class](https://www.youtube.com/watch?v=G2SuBGH8ilQ), and [a bunch of poor programmers submit half-decent code](https://tab.al/posts/adventures-with-chatgpt/) before getting the attention of [CNN](https://www.cnn.com/2022/12/05/tech/chatgpt-trnd/index.html) and [Fox News](https://www.foxnews.com/tech/ai-bot-schoolwork-blow-up-us-education-system-youngest-risk-former-teacher).  But every group of under-30s think they've invented the latest hot shit, completely ignoring the people who paved the way.\n\nHere's some generative AI for you that came before chatGPT:\n\nhttps://this-person-does-not-exist.com/en\n\nhttps://www.science.org/doi/10.1126/science.370.6521.1144\n\nhttps://spectrum.ieee.org/what-is-deepfake\n\nhttps://en.wikipedia.org/wiki/Autocomplete\n\nhttps://en.wikipedia.org/wiki/AARON", "Reply Score": -8}]}, {"Comment": "[deleted]", "Score": 1, "Replies": [{"Reply": "calculators are mostly useless to mathematicians; most of it ends up being symbolic and you don't often need to calculate out for discrete numbers. \n\nBut please do continue with your horrible analogies.", "Reply Score": 1}]}, {"Comment": "\"Why now\" is that chatGPT [helped a bunch of middle schoolers write papers for class](https://www.youtube.com/watch?v=G2SuBGH8ilQ), and [a bunch of poor programmers submit half-decent code](https://tab.al/posts/adventures-with-chatgpt/) before getting the attention of [CNN](https://www.cnn.com/2022/12/05/tech/chatgpt-trnd/index.html) and [Fox News](https://www.foxnews.com/tech/ai-bot-schoolwork-blow-up-us-education-system-youngest-risk-former-teacher).  But every group of under-30s think they've invented the latest hot shit, completely ignoring the people who paved the way.\n\nHere's some generative AI for you that came before chatGPT:\n\nhttps://this-person-does-not-exist.com/en\n\nhttps://www.science.org/doi/10.1126/science.370.6521.1144\n\nhttps://spectrum.ieee.org/what-is-deepfake\n\nhttps://en.wikipedia.org/wiki/Autocomplete\n\nhttps://en.wikipedia.org/wiki/AARON", "Score": -8, "Replies": [{"Reply": "You're comparing totally different things here, the discussion ended like 4 comments ago. The methods used now are completely different, computer science is mostly the study of algorithms and you're basically saying we already had all this stuff just because they share a name in the English language, meanwhile they are totally different algorithmically, and what we have now is something that would have been inconceivable even a decade ago, no matter what processing power you threw at it.", "Reply Score": 5}]}, {"Comment": "calculators are mostly useless to mathematicians; most of it ends up being symbolic and you don't often need to calculate out for discrete numbers. \n\nBut please do continue with your horrible analogies.", "Score": 1, "Replies": []}, {"Comment": "You're comparing totally different things here, the discussion ended like 4 comments ago. The methods used now are completely different, computer science is mostly the study of algorithms and you're basically saying we already had all this stuff just because they share a name in the English language, meanwhile they are totally different algorithmically, and what we have now is something that would have been inconceivable even a decade ago, no matter what processing power you threw at it.", "Score": 5, "Replies": [{"Reply": "Real discussion ended 9 comments ago:\n\n> Sure, this latest iteration has new features, and can process larger datasets faster, and performs in slightly different methods than past programs, but the same can be said for every generation before it.", "Reply Score": -3}]}, {"Comment": "Real discussion ended 9 comments ago:\n\n> Sure, this latest iteration has new features, and can process larger datasets faster, and performs in slightly different methods than past programs, but the same can be said for every generation before it.", "Score": -3, "Replies": [{"Reply": "I hate AI-revisionist historians so god damn much\n\n...already.", "Reply Score": 4}]}, {"Comment": "I hate AI-revisionist historians so god damn much\n\n...already.", "Score": 4, "Replies": [{"Reply": "\"We just invented AI!  Why did nobody before us think of this?!\"", "Reply Score": -2}]}, {"Comment": "\"We just invented AI!  Why did nobody before us think of this?!\"", "Score": -2, "Replies": [{"Reply": "I rest my case.", "Reply Score": 5}]}, {"Comment": "I rest my case.", "Score": 5, "Replies": []}]},{"Title": "Would \"there exists a pair x,y in the subarray arr[left:right+1] that sums up to k.\" be a good loop invariant for the code below which determines given a sorted array of integers if there is a a pair of integers, x and y, that sum to k.", "Score": 0, "URL": "https://www.reddit.com/r/compsci/comments/180vjue/would_there_exists_a_pair_xy_in_the_subarray/", "CreatedAt": 1700611245.0, "Full Content": "The code: \n\nhttps://preview.redd.it/mvhp2recis1c1.png?width=317&format=png&auto=webp&s=f8f8b1cbb2b70ef1ced4aafd35a7d781ec5ec45a", "CntComments": 5, "Comments": [{"Comment": "No, the loop invariant is true at the beginning and end of every loop iteration.  That statement is not always true, there might be no interesting pairs in the whole list, let alone in any subarray.\n\nI think the invariant is actually, \"there is no interesting pair in the list arr\\[:left\\] + arr\\[right:\\] (the outsides of the list that have already been checked by the loop).\"  Then when you exit the loop, the invariant being true and right == left means that there is no interesting pair in the entire list.", "Score": 5, "Replies": [{"Reply": "What about the invariant being left<=right?", "Reply Score": 1}]}, {"Comment": "What about the invariant being left<=right?", "Score": 1, "Replies": [{"Reply": "That is true as well, but it is not a very useful invariant on its own.", "Reply Score": 2}]}, {"Comment": "That is true as well, but it is not a very useful invariant on its own.", "Score": 2, "Replies": [{"Reply": "If I wanna prove iterative correctness what would be the best choice", "Reply Score": 1}]}, {"Comment": "If I wanna prove iterative correctness what would be the best choice", "Score": 1, "Replies": [{"Reply": "What I said before.", "Reply Score": 2}]}, {"Comment": "What I said before.", "Score": 2, "Replies": []}]},{"Title": "Ready Worker One? High-Res VR for the Home Office | Proceedings of the 29th ACM Symposium on Virtual Reality Software and Technology", "Score": 0, "URL": "https://dl.acm.org/doi/abs/10.1145/3611659.3615680", "CreatedAt": 1700596217.0, "Full Content": "", "CntComments": 0, "Comments": []},{"Title": "Introduction to Code Coverage Testing - Guide", "Score": 0, "URL": "https://www.reddit.com/r/compsci/comments/180eiz6/introduction_to_code_coverage_testing_guide/", "CreatedAt": 1700564796.0, "Full Content": "Here is a guide on how code coverage testing helps to improve the quality and reliability of software. It helps to identify and resolve bugs before they become problems in production: [Introduction to Code Coverage Testing](https://www.codium.ai/blog/introduction-to-code-coverage-testing/)", "CntComments": 0, "Comments": []},{"Title": "Recommend me some fields in CS that you find interesting!", "Score": 36, "URL": "https://www.reddit.com/r/compsci/comments/17zmslh/recommend_me_some_fields_in_cs_that_you_find/", "CreatedAt": 1700480976.0, "Full Content": "I just wanna know what sub fields in computer science are you guys learning, or doing research. What fields are worth learning about currently?", "CntComments": 38, "Comments": [{"Comment": "Work\n\n* Topological analysis of multidimensional data.\n* Computational tensor algebra and programming tensor contraction \n\nFun\n\n* Image processing and analysis:  image feature extraction and classification, computational photography, models of human vision.\n* Advanced topics in type systems and related abstractions in programming languages.\n* Optimization of complex queries over large data sets", "Score": 30, "Replies": [{"Reply": "I thought there would be real world application for image processing and analysis", "Reply Score": 2}]}, {"Comment": "._. Some might define this as math but it has many applications in computer science, Discrete and Computational Geometry, and Category Theory. \n\nI guess these are more topics and less fields but they\u2019re still interesting in their own right.", "Score": 11, "Replies": [{"Reply": "anything you recommend reading for becoming more acquainted with these topics? I've started watching Bartosz Milewsky's category theory on youtube", "Reply Score": 3}]}, {"Comment": "Automata theory is cool, circuit complexity as well and their interaction is beautiful.", "Score": 7, "Replies": [{"Reply": "Wait! What would be the intersection of automata theory and circuit complexity? You got me hooked.", "Reply Score": 1}]}, {"Comment": "SAT solvers are really cool, though they are kind of an esoteric field.", "Score": 5, "Replies": []}, {"Comment": "I do research in computational complexity with a really algebraic flavor.", "Score": 5, "Replies": [{"Reply": "Care to elaborate", "Reply Score": 2}, {"Reply": "Computational complexity hits me like a truck when I learn the foundation of Zero-Knowledge proof.\n\nNeed a lot of patience to learn this alone.", "Reply Score": 2}]}, {"Comment": "Crdt. Probabilistic data structure. Quantum algorithm. Formal methods. Databases and data analysis. Software and hardware security. Programming language theory.like Unison :-)", "Score": 5, "Replies": []}, {"Comment": "Graph theory and matrices. Ever take the laplacian of a graph? What is the potential energy of a vertex with respect to another? Linear algebra is so rich, but shut up and calculate linear algebra courses lead you to believe that matrices are boring and only used for solving systems of equations.", "Score": 3, "Replies": []}, {"Comment": "Currently learning Formal Verification.\n\nIt can be used almost in every field. Traditionally, mainly used fot verifying electronic circuits behaviour but now in a lot of field like program correctness, Neural Net Verification.\n\nI am learning to apply formal verification to check program correctness.", "Score": 3, "Replies": []}, {"Comment": "Theoretical computer science. (Actually this term refers to a HUGE literature ranging from complexity theory to type theory and so on.)\n\n&#x200B;\n\nIt is entirely math. We don't run experiments, we prove! The fun is that we solve computer science problems with techniques and tools from mathematics. It has also applications in the real world, if you care about that. \n\n&#x200B;\n\nThere are lots of fascinating ideas in this field. It is simply beautiful and get messy sometimes (if you know, you know).", "Score": 3, "Replies": [{"Reply": "I really like math,  proving anything mathematical / computational is also my favourite. I'm still in uni, what texts/ books do you recommend.", "Reply Score": 1}]}, {"Comment": "DATABASES. Databases in general and specifically relational databases can go surprisingly deep. All of the big database solutions are built on top of super cool data structures and you are essentially learning how to harness all that power left by the past engineers. Think of Instagram, Facebook, etc., a large portion of the \"backend\" work for those platforms is just database work. And then some distributed systems work, and the majority of said systems are databases lol.", "Score": 4, "Replies": []}, {"Comment": "Shaders, lots of cool math and programming \ud83d\ude0d\ud83d\ude0d", "Score": 2, "Replies": []}, {"Comment": "Group theory", "Score": 2, "Replies": []}, {"Comment": "Guys all these topics seem really interesting but how to get started with one as a complete beginner? I am in my 3rd year of college and I have not done any sort of research.", "Score": 2, "Replies": []}, {"Comment": "Parallel programming, parallel programming, parallel programming. It's extremely interesting to dive into and witness how many algorithms can be greatly improved with it. Imagine traversing a tree as fast as possible where each leaf is possibly a single thread, it's awesome.", "Score": 2, "Replies": []}, {"Comment": "Cryptography.  Not the math side, which is mind bending and beyond my math skills, but the engineering, optimization, and usage side is fascinating.\n\nReally any code or systems optimization is super fun.\n\nGraph theory stuff.  I avoided it like the plague until I had to do it on a huge scale and I couldn't avoid it anymore, then I found it challenging in a brain-bending painful kind of way.  Now I want more (someday, not today).\n\nProgramming languages themselves are fascinating.  The intersection of psychology, reliability, engineering, and low-level code challenges.\n\nAnd echoing what /u/permeakra said: Optimization of complex queries over large data sets", "Score": 3, "Replies": []}, {"Comment": "Machine Learning seems to be the big one these days.", "Score": 3, "Replies": []}, {"Comment": "Deep learning", "Score": 0, "Replies": []}, {"Comment": "relational/logic programming: like https://github.com/pythological/kanren \n\ndan friedman and will byrd even made a partially reversible lambda calculus interpreter... \n\n\n    (evalo expr 6)\n    ;;; not in order\n    ;; -> 6\n    ;; -> (+ 1 5)\n    ;; -> (+ 0 6)\n    ;; -> (+ (+ 0 6) 0)\n    ;; -> (int-to-str \"6\")\n    ;; so on and so forth\n\nand while we're on evaluators\n\n- infinite intepretation towers: brian smith 3-lisp\n- reflective multi level interpreter: http://www.is.ocha.ac.jp/~asai/Black/", "Score": 1, "Replies": []}, {"Comment": "The other recommendations seem really heavy duty here.  I've been out of the field for awhile now, so my opinions may be dated, but I think Database Analysis would still be a very lucrative field.\n\nPersonally, I liked being close to the metal, so embedded systems is good for that.  I suspect communications, particularly if you're working at the bottom layers of the stack, would be both lucrative and close to the metal.\n\nAvoid the glamorous stuff where you'll have a lot of competition, and even if you get a job, the pay will probably be low-balled.", "Score": 1, "Replies": []}, {"Comment": "A couple topics I haven\u2019t seen mentioned yet, communication complexity, streaming models/online algorithms, and dimensionality reduction/locality sensitive hashing are all really fun. \n\nPersonally though I\u2019m more into approximation algorithms, combinatorial optimization, and algorithmic game theory.", "Score": 1, "Replies": []}, {"Comment": "There's a field outside of my house with an owl in it I enjoy this field at night time when I see it flying around.\nI recommend this field", "Score": 1, "Replies": []}, {"Comment": "Cellular Automaton", "Score": 1, "Replies": []}, {"Comment": "Currently learning functional programming. If you have trouble understanding what a monad is,  remember it's just a monoid in the category of endofunctors.", "Score": 1, "Replies": []}, {"Comment": "I'm currently taking a discrete mathematics course online for fun and to add something to my resume.", "Score": 1, "Replies": []}, {"Comment": "- Polynomial Factorization (Algebraic Complexity Theory)\n- Randomized Algorithms\n\nThese topics i find very fascinating. Randomized is like you get the most easiest algorithms with very good complexity for very complex problems", "Score": 1, "Replies": []}, {"Comment": "I thought there would be real world application for image processing and analysis", "Score": 2, "Replies": []}, {"Comment": "anything you recommend reading for becoming more acquainted with these topics? I've started watching Bartosz Milewsky's category theory on youtube", "Score": 3, "Replies": [{"Reply": "^-^ Oh Bartosz\u2019s lectures are great so I hope you enjoy them but for computational geometry I\u2019ve been mainly reading \u201cDiscrete and Computational Geometry \u201c by Satyan L. Devadoss and Joseph O\u2019Rourke. I\u2019ve been watching Philipp Kindermann videos on Computational Geometry as well but I do believe some of his videos are in German so just a heads up ( but I also believe that the Computational Geometry playlist is fully in English) . I also recommend looking into graph theory and combinatorics since most computer science majors make use of graphing algorithms and combinatoric models so it\u2019s cool and very interesting to dive deeper into the theory of it. I\u2019ve only ever studied this topic using books and i currently don\u2019t have my books with and can\u2019t remember the names so I cant recommend any materials but there are definitely some YouTuber that has a series on it. \n\n._./I hope this helps and good luck with your studies", "Reply Score": 2}]}, {"Comment": "Wait! What would be the intersection of automata theory and circuit complexity? You got me hooked.", "Score": 1, "Replies": [{"Reply": "So Barrington theorem for instance, probably the most surprising positive result of circuit complexity states that some automata are NC1-complete.\n\nBelows NC1, a lot of circuit complexity classes matches with regular languages classes in a rather deep sense.\n\nFor instance, regular language of AC0 are mostly all regular languages definable in first order logic/ with a starfree expression. In turn, all fixed-depth sub class of AC0 admits a complete regular languages.\n\nA very beautiful interplay between sequential and parallel computation. \n\n(Disclaimer, it is my research field, so probably over enthusiast with it)", "Reply Score": 2}]}, {"Comment": "Care to elaborate", "Score": 2, "Replies": [{"Reply": "Sure! \n\nI am doing work in implicit computational complexity (ICC). The goal is a machine independent characterization of complexity classes. The approach is really functional programming inspired. One takes the functions that a machine computes as the primitive thing we work with (in the case of turing machines one can declare the function it computes is defined on inputs where it halts and outputs the tape contents), a classification in terms of these would apply to any other machine that computes the same kind of functions.\n\nThe insight is that by introducing typed variables and restricting what functions can do with these variables, you can restrict functions from computable, to a complexity class of interest. You can see this with ex: simply typed lambda calculus which is strictly weaker than untyped lambda calculus. The wikipedia page for ICC details a nice example of a type theory for FP (functions computable in polynomial time), it\u2019s essentially done by introducing a new type called a safe variable and requiring that recursion is a safe operation, and that is all one needs.\n\nSo where\u2019s the algebra? One can realize almost any logic we care about as the internal logic of some category (this is a process of associating objects with types and morphisms with functions). A choice of such a category is called a model. The logics that pop out in ICC are generally linear logics. These correspond to symmetric monoidal categories. I\u2019m interested in studying this duality and applying ideas from category theory to come up with new complexity theory results.", "Reply Score": 4}]}, {"Comment": "Computational complexity hits me like a truck when I learn the foundation of Zero-Knowledge proof.\n\nNeed a lot of patience to learn this alone.", "Score": 2, "Replies": []}, {"Comment": "I really like math,  proving anything mathematical / computational is also my favourite. I'm still in uni, what texts/ books do you recommend.", "Score": 1, "Replies": [{"Reply": "For an introduction Sipser's book \"Introduction to the theory of computation\" is very readable and enjoyable too! And it doesn't need too much math background. If you have more mathy background I highly suggest \"Analysis of Boolean Functions\" by Ryan O'Donnell to get a feel for more advanced theory results. This book is simply amazing.", "Reply Score": 1}]}, {"Comment": "^-^ Oh Bartosz\u2019s lectures are great so I hope you enjoy them but for computational geometry I\u2019ve been mainly reading \u201cDiscrete and Computational Geometry \u201c by Satyan L. Devadoss and Joseph O\u2019Rourke. I\u2019ve been watching Philipp Kindermann videos on Computational Geometry as well but I do believe some of his videos are in German so just a heads up ( but I also believe that the Computational Geometry playlist is fully in English) . I also recommend looking into graph theory and combinatorics since most computer science majors make use of graphing algorithms and combinatoric models so it\u2019s cool and very interesting to dive deeper into the theory of it. I\u2019ve only ever studied this topic using books and i currently don\u2019t have my books with and can\u2019t remember the names so I cant recommend any materials but there are definitely some YouTuber that has a series on it. \n\n._./I hope this helps and good luck with your studies", "Score": 2, "Replies": []}, {"Comment": "So Barrington theorem for instance, probably the most surprising positive result of circuit complexity states that some automata are NC1-complete.\n\nBelows NC1, a lot of circuit complexity classes matches with regular languages classes in a rather deep sense.\n\nFor instance, regular language of AC0 are mostly all regular languages definable in first order logic/ with a starfree expression. In turn, all fixed-depth sub class of AC0 admits a complete regular languages.\n\nA very beautiful interplay between sequential and parallel computation. \n\n(Disclaimer, it is my research field, so probably over enthusiast with it)", "Score": 2, "Replies": []}, {"Comment": "Sure! \n\nI am doing work in implicit computational complexity (ICC). The goal is a machine independent characterization of complexity classes. The approach is really functional programming inspired. One takes the functions that a machine computes as the primitive thing we work with (in the case of turing machines one can declare the function it computes is defined on inputs where it halts and outputs the tape contents), a classification in terms of these would apply to any other machine that computes the same kind of functions.\n\nThe insight is that by introducing typed variables and restricting what functions can do with these variables, you can restrict functions from computable, to a complexity class of interest. You can see this with ex: simply typed lambda calculus which is strictly weaker than untyped lambda calculus. The wikipedia page for ICC details a nice example of a type theory for FP (functions computable in polynomial time), it\u2019s essentially done by introducing a new type called a safe variable and requiring that recursion is a safe operation, and that is all one needs.\n\nSo where\u2019s the algebra? One can realize almost any logic we care about as the internal logic of some category (this is a process of associating objects with types and morphisms with functions). A choice of such a category is called a model. The logics that pop out in ICC are generally linear logics. These correspond to symmetric monoidal categories. I\u2019m interested in studying this duality and applying ideas from category theory to come up with new complexity theory results.", "Score": 4, "Replies": [{"Reply": "This is super fascinating, thanks for sharing. Its a side of complexity theory I haven\u2019t seen before. When you say you\u2019re working to be machine agnostic are you using \u201cmachine\u201d to mean \u201cmodel of computation\u201d or something else?", "Reply Score": 1}]}, {"Comment": "For an introduction Sipser's book \"Introduction to the theory of computation\" is very readable and enjoyable too! And it doesn't need too much math background. If you have more mathy background I highly suggest \"Analysis of Boolean Functions\" by Ryan O'Donnell to get a feel for more advanced theory results. This book is simply amazing.", "Score": 1, "Replies": []}, {"Comment": "This is super fascinating, thanks for sharing. Its a side of complexity theory I haven\u2019t seen before. When you say you\u2019re working to be machine agnostic are you using \u201cmachine\u201d to mean \u201cmodel of computation\u201d or something else?", "Score": 1, "Replies": [{"Reply": "> are you using \u201cmachine\u201d to mean \u201cmodel of computation\u201d or something else?\n\nThat\u2019s exactly right. I mean a model of computation. Complexity classes are generally defined in terms of machines such as turing machines or circuits. But when everything is turing equivalent in the end (ok for circuits you do need to restrict to uniform constructible families) you can use any machine you want to do computation, this idea is captured by the Church-Turing thesis and that is why a machine independent characterization is so desirable.", "Reply Score": 2}]}, {"Comment": "> are you using \u201cmachine\u201d to mean \u201cmodel of computation\u201d or something else?\n\nThat\u2019s exactly right. I mean a model of computation. Complexity classes are generally defined in terms of machines such as turing machines or circuits. But when everything is turing equivalent in the end (ok for circuits you do need to restrict to uniform constructible families) you can use any machine you want to do computation, this idea is captured by the Church-Turing thesis and that is why a machine independent characterization is so desirable.", "Score": 2, "Replies": []}]},{"Title": "how does an algorithm that tries to match as many people with their preferred choice work?", "Score": 6, "URL": "https://www.reddit.com/r/compsci/comments/17zq03j/how_does_an_algorithm_that_tries_to_match_as_many/", "CreatedAt": 1700491266.0, "Full Content": "hi everyone! I have no computer science background at all but I am applying to my clinical for nursing. for our last clinical, we are supposed rank our top 20 choices and then a computer tries to allocate as many people towards their preferred location as possible. any idea how this algorithm works or if there's a way I can kinda cheat my way into getting some of my top choices? ", "CntComments": 10, "Comments": [{"Comment": "It is likely to use the Hungarian algorithm, or some variant of it: [https://en.wikipedia.org/wiki/Hungarian\\_algorithm](https://en.wikipedia.org/wiki/Hungarian_algorithm)\n\n>any idea how this algorithm works or if there's a way I can kinda cheat my way into getting some of my top choices?\n\nWell, that is the idea - if the algorithm is well-constructed, then you lying should never help you! This is a basic idea in the field of algorithmic game theory, specifically mechanism design: lying should never be incentivized. \n\nFor better seeing this idea consider two auction systems: the standard auction procedure vs a [vickrey auction](https://en.wikipedia.org/wiki/Vickrey_auction). Do you see why it might be worth it for someone to lie in the first case, but never in the second?", "Score": 9, "Replies": [{"Reply": "by cheating I mean ranking my choices in a certain way that maximizes my chances of getting the location I want! nothing goes into the algorithm other than the choices of students.", "Reply Score": 1}]}, {"Comment": "Great question; it is a textbook matching algorithm at its core. Basically, it is a bipartite graph with one side being students and the other being the clinics (location?) with an edge connecting students to the clinics. \n\nIf the goal of the algorithm as you state it is to find the maximum matching, then the algorithm is likely going to use Berge's Theorem which, of the choices you and other selected, will mix and match options until the maximum is returned. If all students are guaranteed to be matched, then it would seem to make sense to put the minimal choices possible since the algorithm will be forced to assign you to one of those choice assuming other students put more choices down on their list. If there is no guarantee of matching, then you are rolling the dice with the minimal choices approach.\n\nHowever, I assume there is some sort of ranking system in which some vertices (the students) will have a higher weight based on grades, etc. In that case, there is not much you can do as the algorithm will prioritize them, based on some optimization of matching edges that prioritizes them. In that case, you have less control.\n\nThat is my thinking from reading this, curious to hear other thoughts if they are along the same line.", "Score": 13, "Replies": [{"Reply": "If you want to have some sort of weighting my approach would be something like Simulated Annealing (or any other optimization algorithm) with a score function based on the value of all your matchings.", "Reply Score": 3}, {"Reply": "There's no ranking system! they dont take into account any other student information other than how you have ranked your preferred locations.", "Reply Score": 1}]}, {"Comment": "It depends on the specific problem. Is only one side ranking or are both sides?\n\nIf only one side is ranking then this can be modelled as a minimum/maximum weight matching where between two groups A and B we have weighted connections and we want to match elements from A to elements in B such that every element is matched exactly once while maximizing the weight of the selected connections (do note that this will happily give one person an extremely bad thing if it allows everyone else to get better results). The algorithm for the optimal solution is fairly advanced (and relies on a bunch of stuff from various other areas that Reddit comments are not suited for explaining). If anyone with knowledge of algorithmics knowledge is interested search for Edmunds algorithm for a minimum weight perfect matching.\n\n\n\nIf both sides are ranking it's usually modelled as a stable marriage problem: https://en.m.wikipedia.org/wiki/Stable_marriage_problem\n\nThe Wikipedia link also has a section on the algorithmic solution with a pretty good explanation.", "Score": 6, "Replies": []}, {"Comment": "Gale-Shapely algorithm:[https://en.wikipedia.org/wiki/Gale%E2%80%93Shapley\\_algorithm](https://en.wikipedia.org/wiki/Gale%E2%80%93Shapley_algorithm)But here both teams (students and institutions) need to have a preference list.Students' preference list contain choices.Institutions prefer better students (higher the score or lower the ranking better it is).Based on this, stable matching is obtained (best for proposing and worst for accepting)  \n\n\nEdit: the solution is not best for both parties.", "Score": 4, "Replies": [{"Reply": "Gale-Shapley does not produce a matching which is the \"best possible for both parties\", quite the opposite: The \"proposing\" party in Gale-Shapley gets to have an \"optimal\" solution whereas the \"accepting\" party gets a pessimal solution.", "Reply Score": 2}]}, {"Comment": "by cheating I mean ranking my choices in a certain way that maximizes my chances of getting the location I want! nothing goes into the algorithm other than the choices of students.", "Score": 1, "Replies": [{"Reply": "I know. But if the algorithm is well-designed (and it can be well-designed) then you should simply give the genuine ranking - the ranking that you truly agree with!\n\nThis is the point of mechanism design: cheating/lying/strategizing is difficult and expensive; the goal is to create algorithms where this does not help you, ever.", "Reply Score": 6}]}, {"Comment": "If you want to have some sort of weighting my approach would be something like Simulated Annealing (or any other optimization algorithm) with a score function based on the value of all your matchings.", "Score": 3, "Replies": []}, {"Comment": "There's no ranking system! they dont take into account any other student information other than how you have ranked your preferred locations.", "Score": 1, "Replies": []}, {"Comment": "Gale-Shapley does not produce a matching which is the \"best possible for both parties\", quite the opposite: The \"proposing\" party in Gale-Shapley gets to have an \"optimal\" solution whereas the \"accepting\" party gets a pessimal solution.", "Score": 2, "Replies": [{"Reply": "Sorry.  \nMade changes accordingly.  \nThanks for the comment.", "Reply Score": 2}]}, {"Comment": "I know. But if the algorithm is well-designed (and it can be well-designed) then you should simply give the genuine ranking - the ranking that you truly agree with!\n\nThis is the point of mechanism design: cheating/lying/strategizing is difficult and expensive; the goal is to create algorithms where this does not help you, ever.", "Score": 6, "Replies": []}, {"Comment": "Sorry.  \nMade changes accordingly.  \nThanks for the comment.", "Score": 2, "Replies": []}]}]